Python 3.8.2 (default, Jul 16 2020, 14:00:26) 
[GCC 9.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> python.el: native completion setup loaded
>>> train_modelList:
>>> ----------------------feature-knock-out---------------------------
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 118, in <module>
    kpi = tK.train(epochs=200)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 182, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 156, in splitSet
    X = X1[shuffleL]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1640, in _validate_read_indexer
    raise KeyError(f"None of [{key}] are in the [{axis_name}]")
KeyError: "None of [Int64Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,\n                9,\n            ...\n            29536, 29537, 29538, 29539, 29540, 29541, 29542, 29543, 29544,\n            29545],\n           dtype='int64', length=29546)] are in the [columns]"
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 109, in <module>
    y1 = difference(y, 1).values[:201]
NameError: name 'difference' is not defined
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 123, in <module>
    X1 = y1.reshape(-1,1)
AttributeError: 'numpy.ndarray' object has no attribute 'shift'
>>> y1
0             NaN
1        0.012606
2       -0.006924
3        0.011584
4       -0.000379
           ...   
29541   -0.007380
29542    0.014436
29543   -0.006398
29544    0.010769
29545   -0.004202
Length: 29546, dtype: float64
>>> y1
array([ 1.26057723e-02, -6.92393888e-03,  1.15835907e-02, -3.79051111e-04,
        8.84229193e-04,  1.00503358e-02,  2.49968753e-04, -1.25756184e-02,
       -2.28050268e-03, -1.59819293e-02,  5.78296281e-03, -1.04335331e-02,
       -1.63178347e-02, -1.03189167e-02, -1.17711364e-02,  1.74766461e-03,
       -9.98928385e-03, -1.69657920e-02,  5.09257170e-03,  1.05156311e-02,
        2.44233498e-03,  1.62648923e-02, -9.33769153e-04,  6.78329390e-03,
        1.98636096e-03, -6.90298767e-03, -4.13802897e-03,  1.01145392e-02,
        9.26477460e-04,  1.27507477e-02, -1.96116947e-03,  1.82848898e-02,
        1.15584678e-03,  3.08377776e-02, -1.61850342e-02, -8.12806095e-03,
       -1.20591582e-02,  1.14213624e-02, -4.73237434e-03,  1.86707651e-02,
       -3.23536944e-02,  2.51553303e-02,  2.52797843e-02, -4.08643993e-03,
       -1.60103481e-02, -5.04477243e-04, -1.66638721e-02, -1.28279135e-04,
       -4.37131061e-03, -2.45219016e-02,  5.79330122e-03,  1.65357585e-02,
        3.73784025e-03, -2.96334686e-03,  1.80482193e-03, -2.42483525e-02,
        1.31952233e-04,  4.21330442e-03, -7.38497351e-03,  3.96301705e-03,
        3.10524008e-01, -5.80046417e-04,  3.66795776e-03,  1.62465346e-02,
        2.55621440e-03,  1.27773252e-02, -7.96667596e-03, -3.00466941e-01,
       -7.78214039e-03,  1.28065569e-04, -1.60065376e-02, -5.48018069e-03,
       -2.10227877e-02,  2.66880327e-03, -3.60456966e-03,  4.93630510e-03,
       -4.93630510e-03, -3.08084094e-03,  1.20017442e-02,  1.39531024e-02,
       -9.85229637e-03,  1.23328773e-02, -5.21716455e-04,  1.79719103e-02,
        7.91325119e-03,  1.48898649e-02,  6.26056486e-04,  3.49869154e-03,
       -6.00527260e-03,  3.13224585e-03, -4.76489926e-03, -1.58361451e-02,
       -3.93279846e-02, -1.08166908e-02,  4.95547786e-03,  1.48524145e-02,
        9.20992098e-04, -3.42511200e-03,  8.14829052e-03,  2.96604590e-02,
        2.28455485e-03, -7.50782932e-03, -4.48058098e-03, -5.01641025e-03,
        6.55487208e-03,  1.50034600e-02, -9.76483562e-03,  4.83092724e-03,
        2.02711331e-03, -8.77142845e-03,  3.95030775e-03, -1.68006515e-02,
       -7.52956947e-03, -8.90058225e-03, -8.84786221e-03,  9.76776502e-03,
        1.16227230e-02, -8.73709666e-03,  4.83439888e-03,  2.21339845e-03,
       -3.12622370e-03, -2.48187703e-03, -1.09146983e-02,  1.71741900e-03,
        3.68012503e-02, -5.22993726e-03, -1.31309816e-02,  1.13388664e-02,
        2.30355872e-03,  3.83411083e-04, -1.54522775e-02, -4.28191858e-02,
        2.29994012e-03, -3.51970037e-03, -8.58023644e-03,  6.13624338e-03,
        2.71517949e-03, -5.43775126e-03,  1.87703842e-02,  1.22325683e-02,
        1.26068642e-02,  3.63909467e-02, -6.43819678e-03,  1.96883905e-02,
        1.41793880e-02, -7.37285222e-03, -1.60464145e-03, -8.18659987e-03,
       -4.87465728e-02, -1.10454771e-02, -2.42233321e-02, -1.15812997e-02,
       -3.84457425e-03, -1.21800813e-02,  6.93869001e-03,  3.25176597e-02,
        1.59365921e-02,  5.78036287e-03,  9.90623318e-03,  6.48298235e-04,
        8.90385900e-03,  6.40289031e-03, -2.23307115e-02, -7.99427618e-03,
       -2.23934756e-03, -2.24437349e-03,  1.36520891e-02,  2.21397497e-03,
       -3.77973728e-03,  1.00046309e-02, -1.42311945e-03,  7.99488456e-03,
        1.15525331e-03, -7.72701843e-03,  1.16286594e-03, -1.62732224e-02,
       -7.37719056e-03,  1.32135325e-03, -9.15307836e-03,  8.22831445e-03,
       -2.11696293e-03, -1.31993116e-02,  1.87717941e-03,  8.40397123e-03,
        1.85568264e-02, -1.17424502e-03,  2.09282924e-02,  2.93610983e-03,
        6.60655428e-03,  5.05242956e-03, -1.92081208e-02, -2.19446576e-02,
        3.14589325e-03, -3.02932688e-02, -5.27491582e-03,  3.24939357e-03,
        1.35080392e-03,  2.46662727e-02,  2.89321609e-03,  1.20090204e-02,
       -8.33881047e-03])
>>> 
>>> 
>>> 2020-08-10 09:28:05.798131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-08-10 09:28:06.370400: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-08-10 09:28:06.370443: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (malindro): /proc/driver/nvidia/version does not exist
2020-08-10 09:28:06.371389: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-08-10 09:28:06.406549: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2899885000 Hz
2020-08-10 09:28:06.406808: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7efe98000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-10 09:28:06.406823: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
>>> 


>>> int([x]) -> integer
>>> y
array([4.9340242 , 4.94662997, 4.93970603, ..., 4.87694517, 4.88771409,
       4.88351191])
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 182, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 156, in splitSet
    X = X1[shuffleL]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: '[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249, 6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289, 6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339, 6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369, 6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379, 6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489, 6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509, 6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539, 6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549, 6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559, 6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599, 6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619, 6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629, 6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649, 6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669, 6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709, 6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749, 6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759, 6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769, 6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809, 6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829, 6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839, 6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859, 6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869, 6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879, 6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899, 6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909, 6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919, 6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929, 6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939, 6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949, 6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989, 6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999, 7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059, 7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069, 7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079, 7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089, 7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099, 7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109, 7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129, 7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139, 7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149, 7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179, 7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209, 7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219, 7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239, 7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259, 7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269, 7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319, 7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349, 7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359, 7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369, 7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379, 7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399, 7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409, 7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419, 7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429, 7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439, 7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449, 7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479, 7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489, 7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509, 7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519, 7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529, 7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7539, 7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549, 7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569, 7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579, 7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609, 7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619, 7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629, 7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639, 7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649, 7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659, 7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669, 7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679, 7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 7700, 7701, 7702, 7703, 7704, 7705, 7706, 7707, 7708, 7709, 7710, 7711, 7712, 7713, 7714, 7715, 7716, 7717, 7718, 7719, 7720, 7721, 7722, 7723, 7724, 7725, 7726, 7727, 7728, 7729, 7730, 7731, 7732, 7733, 7734, 7735, 7736, 7737, 7738, 7739, 7740, 7741, 7742, 7743, 7744, 7745, 7746, 7747, 7748, 7749, 7750, 7751, 7752, 7753, 7754, 7755, 7756, 7757, 7758, 7759, 7760, 7761, 7762, 7763, 7764, 7765, 7766, 7767, 7768, 7769, 7770, 7771, 7772, 7773, 7774, 7775, 7776, 7777, 7778, 7779, 7780, 7781, 7782, 7783, 7784, 7785, 7786, 7787, 7788, 7789, 7790, 7791, 7792, 7793, 7794, 7795, 7796, 7797, 7798, 7799, 7800, 7801, 7802, 7803, 7804, 7805, 7806, 7807, 7808, 7809, 7810, 7811, 7812, 7813, 7814, 7815, 7816, 7817, 7818, 7819, 7820, 7821, 7822, 7823, 7824, 7825, 7826, 7827, 7828, 7829, 7830, 7831, 7832, 7833, 7834, 7835, 7836, 7837, 7838, 7839, 7840, 7841, 7842, 7843, 7844, 7845, 7846, 7847, 7848, 7849, 7850, 7851, 7852, 7853, 7854, 7855, 7856, 7857, 7858, 7859, 7860, 7861, 7862, 7863, 7864, 7865, 7866, 7867, 7868, 7869, 7870, 7871, 7872, 7873, 7874, 7875, 7876, 7877, 7878, 7879, 7880, 7881, 7882, 7883, 7884, 7885, 7886, 7887, 7888, 7889, 7890, 7891, 7892, 7893, 7894, 7895, 7896, 7897, 7898, 7899, 7900, 7901, 7902, 7903, 7904, 7905, 7906, 7907, 7908, 7909, 7910, 7911, 7912, 7913, 7914, 7915, 7916, 7917, 7918, 7919, 7920, 7921, 7922, 7923, 7924, 7925, 7926, 7927, 7928, 7929, 7930, 7931, 7932, 7933, 7934, 7935, 7936, 7937, 7938, 7939, 7940, 7941, 7942, 7943, 7944, 7945, 7946, 7947, 7948, 7949, 7950, 7951, 7952, 7953, 7954, 7955, 7956, 7957, 7958, 7959, 7960, 7961, 7962, 7963, 7964, 7965, 7966, 7967, 7968, 7969, 7970, 7971, 7972, 7973, 7974, 7975, 7976, 7977, 7978, 7979, 7980, 7981, 7982, 7983, 7984, 7985, 7986, 7987, 7988, 7989, 7990, 7991, 7992, 7993, 7994, 7995, 7996, 7997, 7998, 7999, 8000, 8001, 8002, 8003, 8004, 8005, 8006, 8007, 8008, 8009, 8010, 8011, 8012, 8013, 8014, 8015, 8016, 8017, 8018, 8019, 8020, 8021, 8022, 8023, 8024, 8025, 8026, 8027, 8028, 8029, 8030, 8031, 8032, 8033, 8034, 8035, 8036, 8037, 8038, 8039, 8040, 8041, 8042, 8043, 8044, 8045, 8046, 8047, 8048, 8049, 8050, 8051, 8052, 8053, 8054, 8055, 8056, 8057, 8058, 8059, 8060, 8061, 8062, 8063, 8064, 8065, 8066, 8067, 8068, 8069, 8070, 8071, 8072, 8073, 8074, 8075, 8076, 8077, 8078, 8079, 8080, 8081, 8082, 8083, 8084, 8085, 8086, 8087, 8088, 8089, 8090, 8091, 8092, 8093, 8094, 8095, 8096, 8097, 8098, 8099, 8100, 8101, 8102, 8103, 8104, 8105, 8106, 8107, 8108, 8109, 8110, 8111, 8112, 8113, 8114, 8115, 8116, 8117, 8118, 8119, 8120, 8121, 8122, 8123, 8124, 8125, 8126, 8127, 8128, 8129, 8130, 8131, 8132, 8133, 8134, 8135, 8136, 8137, 8138, 8139, 8140, 8141, 8142, 8143, 8144, 8145, 8146, 8147, 8148, 8149, 8150, 8151, 8152, 8153, 8154, 8155, 8156, 8157, 8158, 8159, 8160, 8161, 8162, 8163, 8164, 8165, 8166, 8167, 8168, 8169, 8170, 8171, 8172, 8173, 8174, 8175, 8176, 8177, 8178, 8179, 8180, 8181, 8182, 8183, 8184, 8185, 8186, 8187, 8188, 8189, 8190, 8191, 8192, 8193, 8194, 8195, 8196, 8197, 8198, 8199, 8200, 8201, 8202, 8203, 8204, 8205, 8206, 8207, 8208, 8209, 8210, 8211, 8212, 8213, 8214, 8215, 8216, 8217, 8218, 8219, 8220, 8221, 8222, 8223, 8224, 8225, 8226, 8227, 8228, 8229, 8230, 8231, 8232, 8233, 8234, 8235, 8236, 8237, 8238, 8239, 8240, 8241, 8242, 8243, 8244, 8245, 8246, 8247, 8248, 8249, 8250, 8251, 8252, 8253, 8254, 8255, 8256, 8257, 8258, 8259, 8260, 8261, 8262, 8263, 8264, 8265, 8266, 8267, 8268, 8269, 8270, 8271, 8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281, 8282, 8283, 8284, 8285, 8286, 8287, 8288, 8289, 8290, 8291, 8292, 8293, 8294, 8295, 8296, 8297, 8298, 8299, 8300, 8301, 8302, 8303, 8304, 8305, 8306, 8307, 8308, 8309, 8310, 8311, 8312, 8313, 8314, 8315, 8316, 8317, 8318, 8319, 8320, 8321, 8322, 8323, 8324, 8325, 8326, 8327, 8328, 8329, 8330, 8331, 8332, 8333, 8334, 8335, 8336, 8337, 8338, 8339, 8340, 8341, 8342, 8343, 8344, 8345, 8346, 8347, 8348, 8349, 8350, 8351, 8352, 8353, 8354, 8355, 8356, 8357, 8358, 8359, 8360, 8361, 8362, 8363, 8364, 8365, 8366, 8367, 8368, 8369, 8370, 8371, 8372, 8373, 8374, 8375, 8376, 8377, 8378, 8379, 8380, 8381, 8382, 8383, 8384, 8385, 8386, 8387, 8388, 8389, 8390, 8391, 8392, 8393, 8394, 8395, 8396, 8397, 8398, 8399, 8400, 8401, 8402, 8403, 8404, 8405, 8406, 8407, 8408, 8409, 8410, 8411, 8412, 8413, 8414, 8415, 8416, 8417, 8418, 8419, 8420, 8421, 8422, 8423, 8424, 8425, 8426, 8427, 8428, 8429, 8430, 8431, 8432, 8433, 8434, 8435, 8436, 8437, 8438, 8439, 8440, 8441, 8442, 8443, 8444, 8445, 8446, 8447, 8448, 8449, 8450, 8451, 8452, 8453, 8454, 8455, 8456, 8457, 8458, 8459, 8460, 8461, 8462, 8463, 8464, 8465, 8466, 8467, 8468, 8469, 8470, 8471, 8472, 8473, 8474, 8475, 8476, 8477, 8478, 8479, 8480, 8481, 8482, 8483, 8484, 8485, 8486, 8487, 8488, 8489, 8490, 8491, 8492, 8493, 8494, 8495, 8496, 8497, 8498, 8499, 8500, 8501, 8502, 8503, 8504, 8505, 8506, 8507, 8508, 8509, 8510, 8511, 8512, 8513, 8514, 8515, 8516, 8517, 8518, 8519, 8520, 8521, 8522, 8523, 8524, 8525, 8526, 8527, 8528, 8529, 8530, 8531, 8532, 8533, 8534, 8535, 8536, 8537, 8538, 8539, 8540, 8541, 8542, 8543, 8544, 8545, 8546, 8547, 8548, 8549, 8550, 8551, 8552, 8553, 8554, 8555, 8556, 8557, 8558, 8559, 8560, 8561, 8562, 8563, 8564, 8565, 8566, 8567, 8568, 8569, 8570, 8571, 8572, 8573, 8574, 8575, 8576, 8577, 8578, 8579, 8580, 8581, 8582, 8583, 8584, 8585, 8586, 8587, 8588, 8589, 8590, 8591, 8592, 8593, 8594, 8595, 8596, 8597, 8598, 8599, 8600, 8601, 8602, 8603, 8604, 8605, 8606, 8607, 8608, 8609, 8610, 8611, 8612, 8613, 8614, 8615, 8616, 8617, 8618, 8619, 8620, 8621, 8622, 8623, 8624, 8625, 8626, 8627, 8628, 8629, 8630, 8631, 8632, 8633, 8634, 8635, 8636, 8637, 8638, 8639, 8640, 8641, 8642, 8643, 8644, 8645, 8646, 8647, 8648, 8649, 8650, 8651, 8652, 8653, 8654, 8655, 8656, 8657, 8658, 8659, 8660, 8661, 8662, 8663, 8664, 8665, 8666, 8667, 8668, 8669, 8670, 8671, 8672, 8673, 8674, 8675, 8676, 8677, 8678, 8679, 8680, 8681, 8682, 8683, 8684, 8685, 8686, 8687, 8688, 8689, 8690, 8691, 8692, 8693, 8694, 8695, 8696, 8697, 8698, 8699, 8700, 8701, 8702, 8703, 8704, 8705, 8706, 8707, 8708, 8709, 8710, 8711, 8712, 8713, 8714, 8715, 8716, 8717, 8718, 8719, 8720, 8721, 8722, 8723, 8724, 8725, 8726, 8727, 8728, 8729, 8730, 8731, 8732, 8733, 8734, 8735, 8736, 8737, 8738, 8739, 8740, 8741, 8742, 8743, 8744, 8745, 8746, 8747, 8748, 8749, 8750, 8751, 8752, 8753, 8754, 8755, 8756, 8757, 8758, 8759, 8760, 8761, 8762, 8763, 8764, 8765, 8766, 8767, 8768, 8769, 8770, 8771, 8772, 8773, 8774, 8775, 8776, 8777, 8778, 8779, 8780, 8781, 8782, 8783, 8784, 8785, 8786, 8787, 8788, 8789, 8790, 8791, 8792, 8793, 8794, 8795, 8796, 8797, 8798, 8799, 8800, 8801, 8802, 8803, 8804, 8805, 8806, 8807, 8808, 8809, 8810, 8811, 8812, 8813, 8814, 8815, 8816, 8817, 8818, 8819, 8820, 8821, 8822, 8823, 8824, 8825, 8826, 8827, 8828, 8829, 8830, 8831, 8832, 8833, 8834, 8835, 8836, 8837, 8838, 8839, 8840, 8841, 8842, 8843, 8844, 8845, 8846, 8847, 8848, 8849, 8850, 8851, 8852, 8853, 8854, 8855, 8856, 8857, 8858, 8859, 8860, 8861, 8862, 8863, 8864, 8865, 8866, 8867, 8868, 8869, 8870, 8871, 8872, 8873, 8874, 8875, 8876, 8877, 8878, 8879, 8880, 8881, 8882, 8883, 8884, 8885, 8886, 8887, 8888, 8889, 8890, 8891, 8892, 8893, 8894, 8895, 8896, 8897, 8898, 8899, 8900, 8901, 8902, 8903, 8904, 8905, 8906, 8907, 8908, 8909, 8910, 8911, 8912, 8913, 8914, 8915, 8916, 8917, 8918, 8919, 8920, 8921, 8922, 8923, 8924, 8925, 8926, 8927, 8928, 8929, 8930, 8931, 8932, 8933, 8934, 8935, 8936, 8937, 8938, 8939, 8940, 8941, 8942, 8943, 8944, 8945, 8946, 8947, 8948, 8949, 8950, 8951, 8952, 8953, 8954, 8955, 8956, 8957, 8958, 8959, 8960, 8961, 8962, 8963, 8964, 8965, 8966, 8967, 8968, 8969, 8970, 8971, 8972, 8973, 8974, 8975, 8976, 8977, 8978, 8979, 8980, 8981, 8982, 8983, 8984, 8985, 8986, 8987, 8988, 8989, 8990, 8991, 8992, 8993, 8994, 8995, 8996, 8997, 8998, 8999, 9000, 9001, 9002, 9003, 9004, 9005, 9006, 9007, 9008, 9009, 9010, 9011, 9012, 9013, 9014, 9015, 9016, 9017, 9018, 9019, 9020, 9021, 9022, 9023, 9024, 9025, 9026, 9027, 9028, 9029, 9030, 9031, 9032, 9033, 9034, 9035, 9036, 9037, 9038, 9039, 9040, 9041, 9042, 9043, 9044, 9045, 9046, 9047, 9048, 9049, 9050, 9051, 9052, 9053, 9054, 9055, 9056, 9057, 9058, 9059, 9060, 9061, 9062, 9063, 9064, 9065, 9066, 9067, 9068, 9069, 9070, 9071, 9072, 9073, 9074, 9075, 9076, 9077, 9078, 9079, 9080, 9081, 9082, 9083, 9084, 9085, 9086, 9087, 9088, 9089, 9090, 9091, 9092, 9093, 9094, 9095, 9096, 9097, 9098, 9099, 9100, 9101, 9102, 9103, 9104, 9105, 9106, 9107, 9108, 9109, 9110, 9111, 9112, 9113, 9114, 9115, 9116, 9117, 9118, 9119, 9120, 9121, 9122, 9123, 9124, 9125, 9126, 9127, 9128, 9129, 9130, 9131, 9132, 9133, 9134, 9135, 9136, 9137, 9138, 9139, 9140, 9141, 9142, 9143, 9144, 9145, 9146, 9147, 9148, 9149, 9150, 9151, 9152, 9153, 9154, 9155, 9156, 9157, 9158, 9159, 9160, 9161, 9162, 9163, 9164, 9165, 9166, 9167, 9168, 9169, 9170, 9171, 9172, 9173, 9174, 9175, 9176, 9177, 9178, 9179, 9180, 9181, 9182, 9183, 9184, 9185, 9186, 9187, 9188, 9189, 9190, 9191, 9192, 9193, 9194, 9195, 9196, 9197, 9198, 9199, 9200, 9201, 9202, 9203, 9204, 9205, 9206, 9207, 9208, 9209, 9210, 9211, 9212, 9213, 9214, 9215, 9216, 9217, 9218, 9219, 9220, 9221, 9222, 9223, 9224, 9225, 9226, 9227, 9228, 9229, 9230, 9231, 9232, 9233, 9234, 9235, 9236, 9237, 9238, 9239, 9240, 9241, 9242, 9243, 9244, 9245, 9246, 9247, 9248, 9249, 9250, 9251, 9252, 9253, 9254, 9255, 9256, 9257, 9258, 9259, 9260, 9261, 9262, 9263, 9264, 9265, 9266, 9267, 9268, 9269, 9270, 9271, 9272, 9273, 9274, 9275, 9276, 9277, 9278, 9279, 9280, 9281, 9282, 9283, 9284, 9285, 9286, 9287, 9288, 9289, 9290, 9291, 9292, 9293, 9294, 9295, 9296, 9297, 9298, 9299, 9300, 9301, 9302, 9303, 9304, 9305, 9306, 9307, 9308, 9309, 9310, 9311, 9312, 9313, 9314, 9315, 9316, 9317, 9318, 9319, 9320, 9321, 9322, 9323, 9324, 9325, 9326, 9327, 9328, 9329, 9330, 9331, 9332, 9333, 9334, 9335, 9336, 9337, 9338, 9339, 9340, 9341, 9342, 9343, 9344, 9345, 9346, 9347, 9348, 9349, 9350, 9351, 9352, 9353, 9354, 9355, 9356, 9357, 9358, 9359, 9360, 9361, 9362, 9363, 9364, 9365, 9366, 9367, 9368, 9369, 9370, 9371, 9372, 9373, 9374, 9375, 9376, 9377, 9378, 9379, 9380, 9381, 9382, 9383, 9384, 9385, 9386, 9387, 9388, 9389, 9390, 9391, 9392, 9393, 9394, 9395, 9396, 9397, 9398, 9399, 9400, 9401, 9402, 9403, 9404, 9405, 9406, 9407, 9408, 9409, 9410, 9411, 9412, 9413, 9414, 9415, 9416, 9417, 9418, 9419, 9420, 9421, 9422, 9423, 9424, 9425, 9426, 9427, 9428, 9429, 9430, 9431, 9432, 9433, 9434, 9435, 9436, 9437, 9438, 9439, 9440, 9441, 9442, 9443, 9444, 9445, 9446, 9447, 9448, 9449, 9450, 9451, 9452, 9453, 9454, 9455, 9456, 9457, 9458, 9459, 9460, 9461, 9462, 9463, 9464, 9465, 9466, 9467, 9468, 9469, 9470, 9471, 9472, 9473, 9474, 9475, 9476, 9477, 9478, 9479, 9480, 9481, 9482, 9483, 9484, 9485, 9486, 9487, 9488, 9489, 9490, 9491, 9492, 9493, 9494, 9495, 9496, 9497, 9498, 9499, 9500, 9501, 9502, 9503, 9504, 9505, 9506, 9507, 9508, 9509, 9510, 9511, 9512, 9513, 9514, 9515, 9516, 9517, 9518, 9519, 9520, 9521, 9522, 9523, 9524, 9525, 9526, 9527, 9528, 9529, 9530, 9531, 9532, 9533, 9534, 9535, 9536, 9537, 9538, 9539, 9540, 9541, 9542, 9543, 9544, 9545, 9546, 9547, 9548, 9549, 9550, 9551, 9552, 9553, 9554, 9555, 9556, 9557, 9558, 9559, 9560, 9561, 9562, 9563, 9564, 9565, 9566, 9567, 9568, 9569, 9570, 9571, 9572, 9573, 9574, 9575, 9576, 9577, 9578, 9579, 9580, 9581, 9582, 9583, 9584, 9585, 9586, 9587, 9588, 9589, 9590, 9591, 9592, 9593, 9594, 9595, 9596, 9597, 9598, 9599, 9600, 9601, 9602, 9603, 9604, 9605, 9606, 9607, 9608, 9609, 9610, 9611, 9612, 9613, 9614, 9615, 9616, 9617, 9618, 9619, 9620, 9621, 9622, 9623, 9624, 9625, 9626, 9627, 9628, 9629, 9630, 9631, 9632, 9633, 9634, 9635, 9636, 9637, 9638, 9639, 9640, 9641, 9642, 9643, 9644, 9645, 9646, 9647, 9648, 9649, 9650, 9651, 9652, 9653, 9654, 9655, 9656, 9657, 9658, 9659, 9660, 9661, 9662, 9663, 9664, 9665, 9666, 9667, 9668, 9669, 9670, 9671, 9672, 9673, 9674, 9675, 9676, 9677, 9678, 9679, 9680, 9681, 9682, 9683, 9684, 9685, 9686, 9687, 9688, 9689, 9690, 9691, 9692, 9693, 9694, 9695, 9696, 9697, 9698, 9699, 9700, 9701, 9702, 9703, 9704, 9705, 9706, 9707, 9708, 9709, 9710, 9711, 9712, 9713, 9714, 9715, 9716, 9717, 9718, 9719, 9720, 9721, 9722, 9723, 9724, 9725, 9726, 9727, 9728, 9729, 9730, 9731, 9732, 9733, 9734, 9735, 9736, 9737, 9738, 9739, 9740, 9741, 9742, 9743, 9744, 9745, 9746, 9747, 9748, 9749, 9750, 9751, 9752, 9753, 9754, 9755, 9756, 9757, 9758, 9759, 9760, 9761, 9762, 9763, 9764, 9765, 9766, 9767, 9768, 9769, 9770, 9771, 9772, 9773, 9774, 9775, 9776, 9777, 9778, 9779, 9780, 9781, 9782, 9783, 9784, 9785, 9786, 9787, 9788, 9789, 9790, 9791, 9792, 9793, 9794, 9795, 9796, 9797, 9798, 9799, 9800, 9801, 9802, 9803, 9804, 9805, 9806, 9807, 9808, 9809, 9810, 9811, 9812, 9813, 9814, 9815, 9816, 9817, 9818, 9819, 9820, 9821, 9822, 9823, 9824, 9825, 9826, 9827, 9828, 9829, 9830, 9831, 9832, 9833, 9834, 9835, 9836, 9837, 9838, 9839, 9840, 9841, 9842, 9843, 9844, 9845, 9846, 9847, 9848, 9849, 9850, 9851, 9852, 9853, 9854, 9855, 9856, 9857, 9858, 9859, 9860, 9861, 9862, 9863, 9864, 9865, 9866, 9867, 9868, 9869, 9870, 9871, 9872, 9873, 9874, 9875, 9876, 9877, 9878, 9879, 9880, 9881, 9882, 9883, 9884, 9885, 9886, 9887, 9888, 9889, 9890, 9891, 9892, 9893, 9894, 9895, 9896, 9897, 9898, 9899, 9900, 9901, 9902, 9903, 9904, 9905, 9906, 9907, 9908, 9909, 9910, 9911, 9912, 9913, 9914, 9915, 9916, 9917, 9918, 9919, 9920, 9921, 9922, 9923, 9924, 9925, 9926, 9927, 9928, 9929, 9930, 9931, 9932, 9933, 9934, 9935, 9936, 9937, 9938, 9939, 9940, 9941, 9942, 9943, 9944, 9945, 9946, 9947, 9948, 9949, 9950, 9951, 9952, 9953, 9954, 9955, 9956, 9957, 9958, 9959, 9960, 9961, 9962, 9963, 9964, 9965, 9966, 9967, 9968, 9969, 9970, 9971, 9972, 9973, 9974, 9975, 9976, 9977, 9978, 9979, 9980, 9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989, 9990, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999, 10000, 10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009, 10010, 10011, 10012, 10013, 10014, 10015, 10016, 10017, 10018, 10019, 10020, 10021, 10022, 10023, 10024, 10025, 10026, 10027, 10028, 10029, 10030, 10031, 10032, 10033, 10034, 10035, 10036, 10037, 10038, 10039, 10040, 10041, 10042, 10043, 10044, 10045, 10046, 10047, 10048, 10049, 10050, 10051, 10052, 10053, 10054, 10055, 10056, 10057, 10058, 10059, 10060, 10061, 10062, 10063, 10064, 10065, 10066, 10067, 10068, 10069, 10070, 10071, 10072, 10073, 10074, 10075, 10076, 10077, 10078, 10079, 10080, 10081, 10082, 10083, 10084, 10085, 10086, 10087, 10088, 10089, 10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099, 10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109, 10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119, 10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129, 10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139, 10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149, 10150, 10151, 10152, 10153, 10154, 10155, 10156, 10157, 10158, 10159, 10160, 10161, 10162, 10163, 10164, 10165, 10166, 10167, 10168, 10169, 10170, 10171, 10172, 10173, 10174, 10175, 10176, 10177, 10178, 10179, 10180, 10181, 10182, 10183, 10184, 10185, 10186, 10187, 10188, 10189, 10190, 10191, 10192, 10193, 10194, 10195, 10196, 10197, 10198, 10199, 10200, 10201, 10202, 10203, 10204, 10205, 10206, 10207, 10208, 10209, 10210, 10211, 10212, 10213, 10214, 10215, 10216, 10217, 10218, 10219, 10220, 10221, 10222, 10223, 10224, 10225, 10226, 10227, 10228, 10229, 10230, 10231, 10232, 10233, 10234, 10235, 10236, 10237, 10238, 10239, 10240, 10241, 10242, 10243, 10244, 10245, 10246, 10247, 10248, 10249, 10250, 10251, 10252, 10253, 10254, 10255, 10256, 10257, 10258, 10259, 10260, 10261, 10262, 10263, 10264, 10265, 10266, 10267, 10268, 10269, 10270, 10271, 10272, 10273, 10274, 10275, 10276, 10277, 10278, 10279, 10280, 10281, 10282, 10283, 10284, 10285, 10286, 10287, 10288, 10289, 10290, 10291, 10292, 10293, 10294, 10295, 10296, 10297, 10298, 10299, 10300, 10301, 10302, 10303, 10304, 10305, 10306, 10307, 10308, 10309, 10310, 10311, 10312, 10313, 10314, 10315, 10316, 10317, 10318, 10319, 10320, 10321, 10322, 10323, 10324, 10325, 10326, 10327, 10328, 10329, 10330, 10331, 10332, 10333, 10334, 10335, 10336, 10337, 10338, 10339, 10340, 10341, 10342, 10343, 10344, 10345, 10346, 10347, 10348, 10349, 10350, 10351, 10352, 10353, 10354, 10355, 10356, 10357, 10358, 10359, 10360, 10361, 10362, 10363, 10364, 10365, 10366, 10367, 10368, 10369, 10370, 10371, 10372, 10373, 10374, 10375, 10376, 10377, 10378, 10379, 10380, 10381, 10382, 10383, 10384, 10385, 10386, 10387, 10388, 10389, 10390, 10391, 10392, 10393, 10394, 10395, 10396, 10397, 10398, 10399, 10400, 10401, 10402, 10403, 10404, 10405, 10406, 10407, 10408, 10409, 10410, 10411, 10412, 10413, 10414, 10415, 10416, 10417, 10418, 10419, 10420, 10421, 10422, 10423, 10424, 10425, 10426, 10427, 10428, 10429, 10430, 10431, 10432, 10433, 10434, 10435, 10436, 10437, 10438, 10439, 10440, 10441, 10442, 10443, 10444, 10445, 10446, 10447, 10448, 10449, 10450, 10451, 10452, 10453, 10454, 10455, 10456, 10457, 10458, 10459, 10460, 10461, 10462, 10463, 10464, 10465, 10466, 10467, 10468, 10469, 10470, 10471, 10472, 10473, 10474, 10475, 10476, 10477, 10478, 10479, 10480, 10481, 10482, 10483, 10484, 10485, 10486, 10487, 10488, 10489, 10490, 10491, 10492, 10493, 10494, 10495, 10496, 10497, 10498, 10499, 10500, 10501, 10502, 10503, 10504, 10505, 10506, 10507, 10508, 10509, 10510, 10511, 10512, 10513, 10514, 10515, 10516, 10517, 10518, 10519, 10520, 10521, 10522, 10523, 10524, 10525, 10526, 10527, 10528, 10529, 10530, 10531, 10532, 10533, 10534, 10535, 10536, 10537, 10538, 10539, 10540, 10541, 10542, 10543, 10544, 10545, 10546, 10547, 10548, 10549, 10550, 10551, 10552, 10553, 10554, 10555, 10556, 10557, 10558, 10559, 10560, 10561, 10562, 10563, 10564, 10565, 10566, 10567, 10568, 10569, 10570, 10571, 10572, 10573, 10574, 10575, 10576, 10577, 10578, 10579, 10580, 10581, 10582, 10583, 10584, 10585, 10586, 10587, 10588, 10589, 10590, 10591, 10592, 10593, 10594, 10595, 10596, 10597, 10598, 10599, 10600, 10601, 10602, 10603, 10604, 10605, 10606, 10607, 10608, 10609, 10610, 10611, 10612, 10613, 10614, 10615, 10616, 10617, 10618, 10619, 10620, 10621, 10622, 10623, 10624, 10625, 10626, 10627, 10628, 10629, 10630, 10631, 10632, 10633, 10634, 10635, 10636, 10637, 10638, 10639, 10640, 10641, 10642, 10643, 10644, 10645, 10646, 10647, 10648, 10649, 10650, 10651, 10652, 10653, 10654, 10655, 10656, 10657, 10658, 10659, 10660, 10661, 10662, 10663, 10664, 10665, 10666, 10667, 10668, 10669, 10670, 10671, 10672, 10673, 10674, 10675, 10676, 10677, 10678, 10679, 10680, 10681, 10682, 10683, 10684, 10685, 10686, 10687, 10688, 10689, 10690, 10691, 10692, 10693, 10694, 10695, 10696, 10697, 10698, 10699, 10700, 10701, 10702, 10703, 10704, 10705, 10706, 10707, 10708, 10709, 10710, 10711, 10712, 10713, 10714, 10715, 10716, 10717, 10718, 10719, 10720, 10721, 10722, 10723, 10724, 10725, 10726, 10727, 10728, 10729, 10730, 10731, 10732, 10733, 10734, 10735, 10736, 10737, 10738, 10739, 10740, 10741, 10742, 10743, 10744, 10745, 10746, 10747, 10748, 10749, 10750, 10751, 10752, 10753, 10754, 10755, 10756, 10757, 10758, 10759, 10760, 10761, 10762, 10763, 10764, 10765, 10766, 10767, 10768, 10769, 10770, 10771, 10772, 10773, 10774, 10775, 10776, 10777, 10778, 10779, 10780, 10781, 10782, 10783, 10784, 10785, 10786, 10787, 10788, 10789, 10790, 10791, 10792, 10793, 10794, 10795, 10796, 10797, 10798, 10799, 10800, 10801, 10802, 10803, 10804, 10805, 10806, 10807, 10808, 10809, 10810, 10811, 10812, 10813, 10814, 10815, 10816, 10817, 10818, 10819, 10820, 10821, 10822, 10823, 10824, 10825, 10826, 10827, 10828, 10829, 10830, 10831, 10832, 10833, 10834, 10835, 10836, 10837, 10838, 10839, 10840, 10841, 10842, 10843, 10844, 10845, 10846, 10847, 10848, 10849, 10850, 10851, 10852, 10853, 10854, 10855, 10856, 10857, 10858, 10859, 10860, 10861, 10862, 10863, 10864, 10865, 10866, 10867, 10868, 10869, 10870, 10871, 10872, 10873, 10874, 10875, 10876, 10877, 10878, 10879, 10880, 10881, 10882, 10883, 10884, 10885, 10886, 10887, 10888, 10889, 10890, 10891, 10892, 10893, 10894, 10895, 10896, 10897, 10898, 10899, 10900, 10901, 10902, 10903, 10904, 10905, 10906, 10907, 10908, 10909, 10910, 10911, 10912, 10913, 10914, 10915, 10916, 10917, 10918, 10919, 10920, 10921, 10922, 10923, 10924, 10925, 10926, 10927, 10928, 10929, 10930, 10931, 10932, 10933, 10934, 10935, 10936, 10937, 10938, 10939, 10940, 10941, 10942, 10943, 10944, 10945, 10946, 10947, 10948, 10949, 10950, 10951, 10952, 10953, 10954, 10955, 10956, 10957, 10958, 10959, 10960, 10961, 10962, 10963, 10964, 10965, 10966, 10967, 10968, 10969, 10970, 10971, 10972, 10973, 10974, 10975, 10976, 10977, 10978, 10979, 10980, 10981, 10982, 10983, 10984, 10985, 10986, 10987, 10988, 10989, 10990, 10991, 10992, 10993, 10994, 10995, 10996, 10997, 10998, 10999, 11000, 11001, 11002, 11003, 11004, 11005, 11006, 11007, 11008, 11009, 11010, 11011, 11012, 11013, 11014, 11015, 11016, 11017, 11018, 11019, 11020, 11021, 11022, 11023, 11024, 11025, 11026, 11027, 11028, 11029, 11030, 11031, 11032, 11033, 11034, 11035, 11036, 11037, 11038, 11039, 11040, 11041, 11042, 11043, 11044, 11045, 11046, 11047, 11048, 11049, 11050, 11051, 11052, 11053, 11054, 11055, 11056, 11057, 11058, 11059, 11060, 11061, 11062, 11063, 11064, 11065, 11066, 11067, 11068, 11069, 11070, 11071, 11072, 11073, 11074, 11075, 11076, 11077, 11078, 11079, 11080, 11081, 11082, 11083, 11084, 11085, 11086, 11087, 11088, 11089, 11090, 11091, 11092, 11093, 11094, 11095, 11096, 11097, 11098, 11099, 11100, 11101, 11102, 11103, 11104, 11105, 11106, 11107, 11108, 11109, 11110, 11111, 11112, 11113, 11114, 11115, 11116, 11117, 11118, 11119, 11120, 11121, 11122, 11123, 11124, 11125, 11126, 11127, 11128, 11129, 11130, 11131, 11132, 11133, 11134, 11135, 11136, 11137, 11138, 11139, 11140, 11141, 11142, 11143, 11144, 11145, 11146, 11147, 11148, 11149, 11150, 11151, 11152, 11153, 11154, 11155, 11156, 11157, 11158, 11159, 11160, 11161, 11162, 11163, 11164, 11165, 11166, 11167, 11168, 11169, 11170, 11171, 11172, 11173, 11174, 11175, 11176, 11177, 11178, 11179, 11180, 11181, 11182, 11183, 11184, 11185, 11186, 11187, 11188, 11189, 11190, 11191, 11192, 11193, 11194, 11195, 11196, 11197, 11198, 11199, 11200, 11201, 11202, 11203, 11204, 11205, 11206, 11207, 11208, 11209, 11210, 11211, 11212, 11213, 11214, 11215, 11216, 11217, 11218, 11219, 11220, 11221, 11222, 11223, 11224, 11225, 11226, 11227, 11228, 11229, 11230, 11231, 11232, 11233, 11234, 11235, 11236, 11237, 11238, 11239, 11240, 11241, 11242, 11243, 11244, 11245, 11246, 11247, 11248, 11249, 11250, 11251, 11252, 11253, 11254, 11255, 11256, 11257, 11258, 11259, 11260, 11261, 11262, 11263, 11264, 11265, 11266, 11267, 11268, 11269, 11270, 11271, 11272, 11273, 11274, 11275, 11276, 11277, 11278, 11279, 11280, 11281, 11282, 11283, 11284, 11285, 11286, 11287, 11288, 11289, 11290, 11291, 11292, 11293, 11294, 11295, 11296, 11297, 11298, 11299, 11300, 11301, 11302, 11303, 11304, 11305, 11306, 11307, 11308, 11309, 11310, 11311, 11312, 11313, 11314, 11315, 11316, 11317, 11318, 11319, 11320, 11321, 11322, 11323, 11324, 11325, 11326, 11327, 11328, 11329, 11330, 11331, 11332, 11333, 11334, 11335, 11336, 11337, 11338, 11339, 11340, 11341, 11342, 11343, 11344, 11345, 11346, 11347, 11348, 11349, 11350, 11351, 11352, 11353, 11354, 11355, 11356, 11357, 11358, 11359, 11360, 11361, 11362, 11363, 11364, 11365, 11366, 11367, 11368, 11369, 11370, 11371, 11372, 11373, 11374, 11375, 11376, 11377, 11378, 11379, 11380, 11381, 11382, 11383, 11384, 11385, 11386, 11387, 11388, 11389, 11390, 11391, 11392, 11393, 11394, 11395, 11396, 11397, 11398, 11399, 11400, 11401, 11402, 11403, 11404, 11405, 11406, 11407, 11408, 11409, 11410, 11411, 11412, 11413, 11414, 11415, 11416, 11417, 11418, 11419, 11420, 11421, 11422, 11423, 11424, 11425, 11426, 11427, 11428, 11429, 11430, 11431, 11432, 11433, 11434, 11435, 11436, 11437, 11438, 11439, 11440, 11441, 11442, 11443, 11444, 11445, 11446, 11447, 11448, 11449, 11450, 11451, 11452, 11453, 11454, 11455, 11456, 11457, 11458, 11459, 11460, 11461, 11462, 11463, 11464, 11465, 11466, 11467, 11468, 11469, 11470, 11471, 11472, 11473, 11474, 11475, 11476, 11477, 11478, 11479, 11480, 11481, 11482, 11483, 11484, 11485, 11486, 11487, 11488, 11489, 11490, 11491, 11492, 11493, 11494, 11495, 11496, 11497, 11498, 11499, 11500, 11501, 11502, 11503, 11504, 11505, 11506, 11507, 11508, 11509, 11510, 11511, 11512, 11513, 11514, 11515, 11516, 11517, 11518, 11519, 11520, 11521, 11522, 11523, 11524, 11525, 11526, 11527, 11528, 11529, 11530, 11531, 11532, 11533, 11534, 11535, 11536, 11537, 11538, 11539, 11540, 11541, 11542, 11543, 11544, 11545, 11546, 11547, 11548, 11549, 11550, 11551, 11552, 11553, 11554, 11555, 11556, 11557, 11558, 11559, 11560, 11561, 11562, 11563, 11564, 11565, 11566, 11567, 11568, 11569, 11570, 11571, 11572, 11573, 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582, 11583, 11584, 11585, 11586, 11587, 11588, 11589, 11590, 11591, 11592, 11593, 11594, 11595, 11596, 11597, 11598, 11599, 11600, 11601, 11602, 11603, 11604, 11605, 11606, 11607, 11608, 11609, 11610, 11611, 11612, 11613, 11614, 11615, 11616, 11617, 11618, 11619, 11620, 11621, 11622, 11623, 11624, 11625, 11626, 11627, 11628, 11629, 11630, 11631, 11632, 11633, 11634, 11635, 11636, 11637, 11638, 11639, 11640, 11641, 11642, 11643, 11644, 11645, 11646, 11647, 11648, 11649, 11650, 11651, 11652, 11653, 11654, 11655, 11656, 11657, 11658, 11659, 11660, 11661, 11662, 11663, 11664, 11665, 11666, 11667, 11668, 11669, 11670, 11671, 11672, 11673, 11674, 11675, 11676, 11677, 11678, 11679, 11680, 11681, 11682, 11683, 11684, 11685, 11686, 11687, 11688, 11689, 11690, 11691, 11692, 11693, 11694, 11695, 11696, 11697, 11698, 11699, 11700, 11701, 11702, 11703, 11704, 11705, 11706, 11707, 11708, 11709, 11710, 11711, 11712, 11713, 11714, 11715, 11716, 11717, 11718, 11719, 11720, 11721, 11722, 11723, 11724, 11725, 11726, 11727, 11728, 11729, 11730, 11731, 11732, 11733, 11734, 11735, 11736, 11737, 11738, 11739, 11740, 11741, 11742, 11743, 11744, 11745, 11746, 11747, 11748, 11749, 11750, 11751, 11752, 11753, 11754, 11755, 11756, 11757, 11758, 11759, 11760, 11761, 11762, 11763, 11764, 11765, 11766, 11767, 11768, 11769, 11770, 11771, 11772, 11773, 11774, 11775, 11776, 11777, 11778, 11779, 11780, 11781, 11782, 11783, 11784, 11785, 11786, 11787, 11788, 11789, 11790, 11791, 11792, 11793, 11794, 11795, 11796, 11797, 11798, 11799, 11800, 11801, 11802, 11803, 11804, 11805, 11806, 11807, 11808, 11809, 11810, 11811, 11812, 11813, 11814, 11815, 11816, 11817, 11818, 11819, 11820, 11821, 11822, 11823, 11824, 11825, 11826, 11827, 11828, 11829, 11830, 11831, 11832, 11833, 11834, 11835, 11836, 11837, 11838, 11839, 11840, 11841, 11842, 11843, 11844, 11845, 11846, 11847, 11848, 11849, 11850, 11851, 11852, 11853, 11854, 11855, 11856, 11857, 11858, 11859, 11860, 11861, 11862, 11863, 11864, 11865, 11866, 11867, 11868, 11869, 11870, 11871, 11872, 11873, 11874, 11875, 11876, 11877, 11878, 11879, 11880, 11881, 11882, 11883, 11884, 11885, 11886, 11887, 11888, 11889, 11890, 11891, 11892, 11893, 11894, 11895, 11896, 11897, 11898, 11899, 11900, 11901, 11902, 11903, 11904, 11905, 11906, 11907, 11908, 11909, 11910, 11911, 11912, 11913, 11914, 11915, 11916, 11917, 11918, 11919, 11920, 11921, 11922, 11923, 11924, 11925, 11926, 11927, 11928, 11929, 11930, 11931, 11932, 11933, 11934, 11935, 11936, 11937, 11938, 11939, 11940, 11941, 11942, 11943, 11944, 11945, 11946, 11947, 11948, 11949, 11950, 11951, 11952, 11953, 11954, 11955, 11956, 11957, 11958, 11959, 11960, 11961, 11962, 11963, 11964, 11965, 11966, 11967, 11968, 11969, 11970, 11971, 11972, 11973, 11974, 11975, 11976, 11977, 11978, 11979, 11980, 11981, 11982, 11983, 11984, 11985, 11986, 11987, 11988, 11989, 11990, 11991, 11992, 11993, 11994, 11995, 11996, 11997, 11998, 11999, 12000, 12001, 12002, 12003, 12004, 12005, 12006, 12007, 12008, 12009, 12010, 12011, 12012, 12013, 12014, 12015, 12016, 12017, 12018, 12019, 12020, 12021, 12022, 12023, 12024, 12025, 12026, 12027, 12028, 12029, 12030, 12031, 12032, 12033, 12034, 12035, 12036, 12037, 12038, 12039, 12040, 12041, 12042, 12043, 12044, 12045, 12046, 12047, 12048, 12049, 12050, 12051, 12052, 12053, 12054, 12055, 12056, 12057, 12058, 12059, 12060, 12061, 12062, 12063, 12064, 12065, 12066, 12067, 12068, 12069, 12070, 12071, 12072, 12073, 12074, 12075, 12076, 12077, 12078, 12079, 12080, 12081, 12082, 12083, 12084, 12085, 12086, 12087, 12088, 12089, 12090, 12091, 12092, 12093, 12094, 12095, 12096, 12097, 12098, 12099, 12100, 12101, 12102, 12103, 12104, 12105, 12106, 12107, 12108, 12109, 12110, 12111, 12112, 12113, 12114, 12115, 12116, 12117, 12118, 12119, 12120, 12121, 12122, 12123, 12124, 12125, 12126, 12127, 12128, 12129, 12130, 12131, 12132, 12133, 12134, 12135, 12136, 12137, 12138, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159, 12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, 12174, 12175, 12176, 12177, 12178, 12179, 12180, 12181, 12182, 12183, 12184, 12185, 12186, 12187, 12188, 12189, 12190, 12191, 12192, 12193, 12194, 12195, 12196, 12197, 12198, 12199, 12200, 12201, 12202, 12203, 12204, 12205, 12206, 12207, 12208, 12209, 12210, 12211, 12212, 12213, 12214, 12215, 12216, 12217, 12218, 12219, 12220, 12221, 12222, 12223, 12224, 12225, 12226, 12227, 12228, 12229, 12230, 12231, 12232, 12233, 12234, 12235, 12236, 12237, 12238, 12239, 12240, 12241, 12242, 12243, 12244, 12245, 12246, 12247, 12248, 12249, 12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259, 12260, 12261, 12262, 12263, 12264, 12265, 12266, 12267, 12268, 12269, 12270, 12271, 12272, 12273, 12274, 12275, 12276, 12277, 12278, 12279, 12280, 12281, 12282, 12283, 12284, 12285, 12286, 12287, 12288, 12289, 12290, 12291, 12292, 12293, 12294, 12295, 12296, 12297, 12298, 12299, 12300, 12301, 12302, 12303, 12304, 12305, 12306, 12307, 12308, 12309, 12310, 12311, 12312, 12313, 12314, 12315, 12316, 12317, 12318, 12319, 12320, 12321, 12322, 12323, 12324, 12325, 12326, 12327, 12328, 12329, 12330, 12331, 12332, 12333, 12334, 12335, 12336, 12337, 12338, 12339, 12340, 12341, 12342, 12343, 12344, 12345, 12346, 12347, 12348, 12349, 12350, 12351, 12352, 12353, 12354, 12355, 12356, 12357, 12358, 12359, 12360, 12361, 12362, 12363, 12364, 12365, 12366, 12367, 12368, 12369, 12370, 12371, 12372, 12373, 12374, 12375, 12376, 12377, 12378, 12379, 12380, 12381, 12382, 12383, 12384, 12385, 12386, 12387, 12388, 12389, 12390, 12391, 12392, 12393, 12394, 12395, 12396, 12397, 12398, 12399, 12400, 12401, 12402, 12403, 12404, 12405, 12406, 12407, 12408, 12409, 12410, 12411, 12412, 12413, 12414, 12415, 12416, 12417, 12418, 12419, 12420, 12421, 12422, 12423, 12424, 12425, 12426, 12427, 12428, 12429, 12430, 12431, 12432, 12433, 12434, 12435, 12436, 12437, 12438, 12439, 12440, 12441, 12442, 12443, 12444, 12445, 12446, 12447, 12448, 12449, 12450, 12451, 12452, 12453, 12454, 12455, 12456, 12457, 12458, 12459, 12460, 12461, 12462, 12463, 12464, 12465, 12466, 12467, 12468, 12469, 12470, 12471, 12472, 12473, 12474, 12475, 12476, 12477, 12478, 12479, 12480, 12481, 12482, 12483, 12484, 12485, 12486, 12487, 12488, 12489, 12490, 12491, 12492, 12493, 12494, 12495, 12496, 12497, 12498, 12499, 12500, 12501, 12502, 12503, 12504, 12505, 12506, 12507, 12508, 12509, 12510, 12511, 12512, 12513, 12514, 12515, 12516, 12517, 12518, 12519, 12520, 12521, 12522, 12523, 12524, 12525, 12526, 12527, 12528, 12529, 12530, 12531, 12532, 12533, 12534, 12535, 12536, 12537, 12538, 12539, 12540, 12541, 12542, 12543, 12544, 12545, 12546, 12547, 12548, 12549, 12550, 12551, 12552, 12553, 12554, 12555, 12556, 12557, 12558, 12559, 12560, 12561, 12562, 12563, 12564, 12565, 12566, 12567, 12568, 12569, 12570, 12571, 12572, 12573, 12574, 12575, 12576, 12577, 12578, 12579, 12580, 12581, 12582, 12583, 12584, 12585, 12586, 12587, 12588, 12589, 12590, 12591, 12592, 12593, 12594, 12595, 12596, 12597, 12598, 12599, 12600, 12601, 12602, 12603, 12604, 12605, 12606, 12607, 12608, 12609, 12610, 12611, 12612, 12613, 12614, 12615, 12616, 12617, 12618, 12619, 12620, 12621, 12622, 12623, 12624, 12625, 12626, 12627, 12628, 12629, 12630, 12631, 12632, 12633, 12634, 12635, 12636, 12637, 12638, 12639, 12640, 12641, 12642, 12643, 12644, 12645, 12646, 12647, 12648, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12656, 12657, 12658, 12659, 12660, 12661, 12662, 12663, 12664, 12665, 12666, 12667, 12668, 12669, 12670, 12671, 12672, 12673, 12674, 12675, 12676, 12677, 12678, 12679, 12680, 12681, 12682, 12683, 12684, 12685, 12686, 12687, 12688, 12689, 12690, 12691, 12692, 12693, 12694, 12695, 12696, 12697, 12698, 12699, 12700, 12701, 12702, 12703, 12704, 12705, 12706, 12707, 12708, 12709, 12710, 12711, 12712, 12713, 12714, 12715, 12716, 12717, 12718, 12719, 12720, 12721, 12722, 12723, 12724, 12725, 12726, 12727, 12728, 12729, 12730, 12731, 12732, 12733, 12734, 12735, 12736, 12737, 12738, 12739, 12740, 12741, 12742, 12743, 12744, 12745, 12746, 12747, 12748, 12749, 12750, 12751, 12752, 12753, 12754, 12755, 12756, 12757, 12758, 12759, 12760, 12761, 12762, 12763, 12764, 12765, 12766, 12767, 12768, 12769, 12770, 12771, 12772, 12773, 12774, 12775, 12776, 12777, 12778, 12779, 12780, 12781, 12782, 12783, 12784, 12785, 12786, 12787, 12788, 12789, 12790, 12791, 12792, 12793, 12794, 12795, 12796, 12797, 12798, 12799, 12800, 12801, 12802, 12803, 12804, 12805, 12806, 12807, 12808, 12809, 12810, 12811, 12812, 12813, 12814, 12815, 12816, 12817, 12818, 12819, 12820, 12821, 12822, 12823, 12824, 12825, 12826, 12827, 12828, 12829, 12830, 12831, 12832, 12833, 12834, 12835, 12836, 12837, 12838, 12839, 12840, 12841, 12842, 12843, 12844, 12845, 12846, 12847, 12848, 12849, 12850, 12851, 12852, 12853, 12854, 12855, 12856, 12857, 12858, 12859, 12860, 12861, 12862, 12863, 12864, 12865, 12866, 12867, 12868, 12869, 12870, 12871, 12872, 12873, 12874, 12875, 12876, 12877, 12878, 12879, 12880, 12881, 12882, 12883, 12884, 12885, 12886, 12887, 12888, 12889, 12890, 12891, 12892, 12893, 12894, 12895, 12896, 12897, 12898, 12899, 12900, 12901, 12902, 12903, 12904, 12905, 12906, 12907, 12908, 12909, 12910, 12911, 12912, 12913, 12914, 12915, 12916, 12917, 12918, 12919, 12920, 12921, 12922, 12923, 12924, 12925, 12926, 12927, 12928, 12929, 12930, 12931, 12932, 12933, 12934, 12935, 12936, 12937, 12938, 12939, 12940, 12941, 12942, 12943, 12944, 12945, 12946, 12947, 12948, 12949, 12950, 12951, 12952, 12953, 12954, 12955, 12956, 12957, 12958, 12959, 12960, 12961, 12962, 12963, 12964, 12965, 12966, 12967, 12968, 12969, 12970, 12971, 12972, 12973, 12974, 12975, 12976, 12977, 12978, 12979, 12980, 12981, 12982, 12983, 12984, 12985, 12986, 12987, 12988, 12989, 12990, 12991, 12992, 12993, 12994, 12995, 12996, 12997, 12998, 12999, 13000, 13001, 13002, 13003, 13004, 13005, 13006, 13007, 13008, 13009, 13010, 13011, 13012, 13013, 13014, 13015, 13016, 13017, 13018, 13019, 13020, 13021, 13022, 13023, 13024, 13025, 13026, 13027, 13028, 13029, 13030, 13031, 13032, 13033, 13034, 13035, 13036, 13037, 13038, 13039, 13040, 13041, 13042, 13043, 13044, 13045, 13046, 13047, 13048, 13049, 13050, 13051, 13052, 13053, 13054, 13055, 13056, 13057, 13058, 13059, 13060, 13061, 13062, 13063, 13064, 13065, 13066, 13067, 13068, 13069, 13070, 13071, 13072, 13073, 13074, 13075, 13076, 13077, 13078, 13079, 13080, 13081, 13082, 13083, 13084, 13085, 13086, 13087, 13088, 13089, 13090, 13091, 13092, 13093, 13094, 13095, 13096, 13097, 13098, 13099, 13100, 13101, 13102, 13103, 13104, 13105, 13106, 13107, 13108, 13109, 13110, 13111, 13112, 13113, 13114, 13115, 13116, 13117, 13118, 13119, 13120, 13121, 13122, 13123, 13124, 13125, 13126, 13127, 13128, 13129, 13130, 13131, 13132, 13133, 13134, 13135, 13136, 13137, 13138, 13139, 13140, 13141, 13142, 13143, 13144, 13145, 13146, 13147, 13148, 13149, 13150, 13151, 13152, 13153, 13154, 13155  C-c C-c  C-c C-c




, 13156, 13157, 13158, 13159, 13160, 13161, 13162, 13163, 13164, 13165, 13166, 13167, 13168, 13169, 13170, 13171, 13172, 13173, 13174, 13175, 13176, 13177, 13178, 13179, 13180, 13181, 13182, 13183, 13184, 13185, 13186, 13187, 13188, 13189, 13190, 13191, 13192, 13193, 13194, 13195, 13196, 13197, 13198, 13199, 13200, 13201, 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13209, 13210, 13211, 13212, 13213, 13214, 13215, 13216, 13217, 13218, 13219, 13220, 13221, 13222, 13223, 13224, 13225, 13226, 13227, 13228, 13229, 13230, 13231, 13232, 13233, 13234, 13235, 13236, 13237, 13238, 13239, 13240, 13241, 13242, 13243, 13244, 13245, 13246, 13247, 13248, 13249, 13250, 13251, 13252, 13253, 13254, 13255, 13256, 13257, 13258, 13259, 13260, 13261, 13262, 13263, 13264, 13265, 13266, 13267, 13268, 13269, 13270, 13271, 13272, 13273, 13274, 13275, 13276, 13277, 13278, 13279, 13280, 13281, 13282, 13283, 13284, 13285, 13286, 13287, 13288, 13289, 13290, 13291, 13292, 13293, 13294, 13295, 13296, 13297, 13298, 13299, 13300, 13301, 13302, 13303, 13304, 13305, 13306, 13307, 13308, 13309, 13310, 13311, 13312, 13313, 13314, 13315, 13316, 13317, 13318, 13319, 13320, 13321, 13322, 13323, 13324, 13325, 13326, 13327, 13328, 13329, 13330, 13331, 13332, 13333, 13334, 13335, 13336, 13337, 13338, 13339, 13340, 13341, 13342, 13343, 13344, 13345, 13346, 13347, 13348, 13349, 13350, 13351, 13352, 13353, 13354, 13355, 13356, 13357, 13358, 13359, 13360, 13361, 13362, 13363, 13364, 13365, 13366, 13367, 13368, 13369, 13370, 13371, 13372, 13373, 13374, 13375, 13376, 13377, 13378, 13379, 13380, 13381, 13382, 13383, 13384, 13385, 13386, 13387, 13388, 13389, 13390, 13391, 13392, 13393, 13394, 13395, 13396, 13397, 13398, 13399, 13400, 13401, 13402, 13403, 13404, 13405, 13406, 13407, 13408, 13409, 13410, 13411, 13412, 13413, 13414, 13415, 13416, 13417, 13418, 13419, 13420, 13421, 13422, 13423, 13424, 13425, 13426, 13427, 13428, 13429, 13430, 13431, 13432, 13433, 13434, 13435, 13436, 13437, 13438, 13439, 13440, 13441, 13442, 13443, 13444, 13445, 13446, 13447, 13448, 13449, 13450, 13451, 13452, 13453, 13454, 13455, 13456, 13457, 13458, 13459, 13460, 13461, 13462, 13463, 13464, 13465, 13466, 13467, 13468, 13469, 13470, 13471, 13472, 13473, 13474, 13475, 13476, 13477, 13478, 13479, 13480, 13481, 13482, 13483, 13484, 13485, 13486, 13487, 13488, 13489, 13490, 13491, 13492, 13493, 13494, 13495, 13496, 13497, 13498, 13499, 13500, 13501, 13502, 13503, 13504, 13505, 13506, 13507, 13508, 13509, 13510, 13511, 13512, 13513, 13514, 13515, 13516, 13517, 13518, 13519, 13520, 13521, 13522, 13523, 13524, 13525, 13526, 13527, 13528, 13529, 13530, 13531, 13532, 13533, 13534, 13535, 13536, 13537, 13538, 13539, 13540, 13541, 13542, 13543, 13544, 13545, 13546, 13547, 13548, 13549, 13550, 13551, 13552, 13553, 13554, 13555, 13556, 13557, 13558, 13559, 13560, 13561, 13562, 13563, 13564, 13565, 13566, 13567, 13568, 13569, 13570, 13571, 13572, 13573, 13574, 13575, 13576, 13577, 13578, 13579, 13580, 13581, 13582, 13583, 13584, 13585, 13586, 13587, 13588, 13589, 13590, 13591, 13592, 13593, 13594, 13595, 13596, 13597, 13598, 13599, 13600, 13601, 13602, 13603, 13604, 13605, 13606, 13607, 13608, 13609, 13610, 13611, 13612, 13613, 13614, 13615, 13616, 13617, 13618, 13619, 13620, 13621, 13622, 13623, 13624, 13625, 13626, 13627, 13628, 13629, 13630, 13631, 13632, 13633, 13634, 13635, 13636, 13637, 13638, 13639, 13640, 13641, 13642, 13643, 13644, 13645, 13646, 13647, 13648, 13649, 13650, 13651, 13652, 13653, 13654, 13655, 13656, 13657, 13658, 13659, 13660, 13661, 13662, 13663, 13664, 13665, 13666, 13667, 13668, 13669, 13670, 13671, 13672, 13673, 13674, 13675, 13676, 13677, 13678, 13679, 13680, 13681, 13682, 13683, 13684, 13685, 13686, 13687, 13688, 13689, 13690, 13691, 13692, 13693, 13694, 13695, 13696, 13697, 13698, 13699, 13700, 13701, 13702, 13703, 13704, 13705, 13706, 13707, 13708, 13709, 13710, 13711, 13712, 13713, 13714, 13715, 13716, 13717, 13718, 13719, 13720, 13721, 13722, 13723, 13724, 13725, 13726, 13727, 13728, 13729, 13730, 13731, 13732, 13733, 13734, 13735, 13736, 13737, 13738, 13739, 13740
KeyboardInterrupt
>>> >>> >>> >>> >>> >>> 
>>> 
>>> 
>>> 
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 212, in <module>
    train, test = supervised_values[0:-n_pred], supervised_values[-n_pred:]
NameError: name 'MinMaxScaler' is not defined
>>> 0
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 237, in <module>
    X1, y1 = test_scaled[i, 0:-1], test_scaled[i, -1]
NameError: name 'lstm_model' is not defined
>>> 0
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 239, in <module>
    yhat = invert_scale(scaler, X1, yhat)
NameError: name 'inverse_difference' is not defined
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 174
    value = dataset[i] - dataset[i - interval]
                                             ^
TabError: inconsistent use of tabs and spaces in indentation
>>> 0
Month=1, Predicted=4.156321, Expected=-0.660686
1
Month=2, Predicted=4.149570, Expected=-0.682432
2
Month=3, Predicted=4.164157, Expected=-0.695409
3
Month=4, Predicted=4.247022, Expected=-0.709939
4
Month=5, Predicted=4.274162, Expected=-0.690953
5
Month=6, Predicted=4.336216, Expected=-0.647497
6
Month=7, Predicted=4.347135, Expected=-0.675780
7
Month=8, Predicted=4.321662, Expected=-0.661788
8
Month=9, Predicted=4.298850, Expected=-0.655916
9
Month=10, Predicted=4.272621, Expected=-0.681322
10
Month=11, Predicted=4.241628, Expected=-0.669880
11
Month=12, Predicted=4.246069, Expected=-0.718542
12
Month=13, Predicted=4.159859, Expected=-0.740351
13
Month=14, Predicted=4.133635, Expected=-0.766131
14
Month=15, Predicted=4.119487, Expected=-0.791758
15
Month=16, Predicted=4.179587, Expected=-0.763466
16
Month=17, Predicted=4.268042, Expected=-0.729802
17
Month=18, Predicted=4.275284, Expected=-0.755108
18
Month=19, Predicted=4.267761, Expected=-0.741106
19
Month=20, Predicted=4.274637, Expected=-0.734695
20
Month=21, Predicted=4.329007, Expected=-0.743750
21
Month=22, Predicted=4.382431, Expected=-0.750938
22
Month=23, Predicted=4.334546, Expected=-0.782552
23
Month=24, Predicted=4.340586, Expected=-0.777578
24
Month=25, Predicted=4.270386, Expected=-0.670986
25
Month=26, Predicted=4.256619, Expected=-0.686134
26
Month=27, Predicted=4.273048, Expected=-0.724167
27
Month=28, Predicted=4.269235, Expected=-0.691324
28
Month=29, Predicted=4.202549, Expected=-0.684652
29
Month=30, Predicted=4.183136, Expected=-0.683542
30
Month=31, Predicted=4.118860, Expected=-0.728298
31
Month=32, Predicted=4.169396, Expected=-0.852321
32
Month=33, Predicted=4.180306, Expected=-0.845659
33
Month=34, Predicted=4.203820, Expected=-0.855854
34
Month=35, Predicted=4.246399, Expected=-0.880706
35
Month=36, Predicted=4.317797, Expected=-0.862932
36
Month=37, Predicted=4.361203, Expected=-0.855068
37
Month=38, Predicted=4.366686, Expected=-0.870818
38
Month=39, Predicted=4.337032, Expected=-0.816451
39
Month=40, Predicted=4.324453, Expected=-0.781020
40
Month=41, Predicted=4.308168, Expected=-0.744506
41
Month=42, Predicted=4.223316, Expected=-0.639102
42
Month=43, Predicted=4.205635, Expected=-0.657750
43
Month=44, Predicted=4.183962, Expected=-0.600724
44
Month=45, Predicted=4.151959, Expected=-0.559654
45
Month=46, Predicted=4.123764, Expected=-0.581009
46
Month=47, Predicted=4.116130, Expected=-0.585657
47
Month=48, Predicted=4.081382, Expected=-0.609369
48
Month=49, Predicted=4.067482, Expected=-0.750560
49
Month=50, Predicted=3.992465, Expected=-0.782552
50
Month=51, Predicted=3.969018, Expected=-0.852713
51
Month=52, Predicted=4.014241, Expected=-0.886257
52
Month=53, Predicted=4.037269, Expected=-0.897393
53
Month=54, Predicted=4.053691, Expected=-0.932672
54
Month=55, Predicted=4.052501, Expected=-0.912574
55
Month=56, Predicted=4.086763, Expected=-0.818389
56
Month=57, Predicted=4.125786, Expected=-0.772230
57
Month=58, Predicted=4.194380, Expected=-0.755488
58
Month=59, Predicted=4.162518, Expected=-0.726795
59
Month=60, Predicted=4.170318, Expected=-0.724917
60
Month=61, Predicted=4.181358, Expected=-0.699128
61
Month=62, Predicted=4.195621, Expected=-0.680583
62
Month=63, Predicted=4.183678, Expected=-0.745262
63
Month=64, Predicted=4.197327, Expected=-0.768417
64
Month=65, Predicted=4.141408, Expected=-0.774903
65
Month=66, Predicted=4.139460, Expected=-0.781403
66
Month=67, Predicted=4.129409, Expected=-0.741861
67
Month=68, Predicted=4.113402, Expected=-0.735448
68
Month=69, Predicted=4.098128, Expected=-0.746396
69
Month=70, Predicted=4.099723, Expected=-0.717419
70
Month=71, Predicted=4.098128, Expected=-0.721540
71
Month=72, Predicted=4.102320, Expected=-0.698384
72
Month=73, Predicted=4.084192, Expected=-0.695038
73
Month=74, Predicted=4.084831, Expected=-0.717419
74
Month=75, Predicted=4.114139, Expected=-0.714050
75
Month=76, Predicted=4.187155, Expected=-0.761184
76
Month=77, Predicted=4.175804, Expected=-0.782552
77
Month=78, Predicted=4.132642, Expected=-0.778725
78
Month=79, Predicted=4.166572, Expected=-0.805236
79
Month=80, Predicted=4.196607, Expected=-0.781403
80
Month=81, Predicted=4.194707, Expected=-0.787535
81
Month=82, Predicted=4.196087, Expected=-0.825766
82
Month=83, Predicted=4.152482, Expected=-0.820329
83
Month=84, Predicted=4.193012, Expected=-0.795987
84
Month=85, Predicted=4.234013, Expected=-0.742239
85
Month=86, Predicted=4.230576, Expected=-0.745640
86
Month=87, Predicted=4.254972, Expected=-0.685023
87
Month=88, Predicted=4.258925, Expected=-0.676518
88
Month=89, Predicted=4.240353, Expected=-0.657383
89
Month=90, Predicted=4.232885, Expected=-0.642749
90
Month=91, Predicted=4.208469, Expected=-0.698384
91
Month=92, Predicted=4.213507, Expected=-0.761945
92
Month=93, Predicted=4.219579, Expected=-0.752833
93
Month=94, Predicted=4.193351, Expected=-0.840575
94
Month=95, Predicted=4.183467, Expected=-0.855854
95
Month=96, Predicted=4.192601, Expected=-0.846442
96
Month=97, Predicted=4.174005, Expected=-0.842529
97
Month=98, Predicted=4.170259, Expected=-0.771086
98
Month=99, Predicted=4.184843, Expected=-0.762706
99
Month=100, Predicted=4.169414, Expected=-0.727922
>>> y
array([-0.70043065, -0.663919  , -0.68397365, ..., -0.72604385,
       -0.74374981, -0.76841652])
>>> train_scaled
array([[-1.        , -0.70043065],
       [ 0.78733255, -0.663919  ],
       [ 0.79189895, -0.68397365],
       ...,
       [ 0.78210375, -0.72604385],
       [ 0.78412919, -0.74374981],
       [ 0.78191476, -0.76841652]])
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 211, in <module>
    diff_values = difference(y, 1)
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 176, in difference
    return Series(diff)
NameError: name 'Series' is not defined
>>> 
>>> 0
Month=1, Predicted=0.058137, Expected=-0.015771
1
Month=2, Predicted=0.067907, Expected=-0.010113
2
Month=3, Predicted=0.037318, Expected=-0.011114
3
Month=4, Predicted=0.032024, Expected=0.010516
4
Month=5, Predicted=0.006403, Expected=0.026309
5
Month=6, Predicted=0.025464, Expected=-0.019990
6
Month=7, Predicted=0.038957, Expected=0.007293
7
Month=8, Predicted=-0.018022, Expected=0.002052
8
Month=9, Predicted=-0.009755, Expected=-0.018133
9
Month=10, Predicted=-0.002303, Expected=0.005647
10
Month=11, Predicted=0.002368, Expected=-0.033142
11
Month=12, Predicted=0.015808, Expected=-0.015812
12
Month=13, Predicted=0.011974, Expected=-0.018375
13
Month=14, Predicted=-0.025996, Expected=-0.018276
14
Month=15, Predicted=0.036467, Expected=0.016522
15
Month=16, Predicted=0.015280, Expected=0.019989
16
Month=17, Predicted=0.000357, Expected=-0.018069
17
Month=18, Predicted=0.015916, Expected=0.007300
18
Month=19, Predicted=0.015474, Expected=0.002400
19
Month=20, Predicted=0.012127, Expected=-0.007581
20
Month=21, Predicted=0.010424, Expected=-0.006376
21
Month=22, Predicted=-0.013732, Expected=-0.022140
22
Month=23, Predicted=-0.004921, Expected=0.001473
23
Month=24, Predicted=0.013074, Expected=0.067055
24
Month=25, Predicted=0.027099, Expected=-0.011513
25
Month=26, Predicted=0.018721, Expected=-0.026283
26
Month=27, Predicted=0.009306, Expected=0.019458
27
Month=28, Predicted=-0.004467, Expected=0.002569
28
Month=29, Predicted=0.005547, Expected=-0.001020
29
Month=30, Predicted=0.015571, Expected=-0.030622
30
Month=31, Predicted=0.048351, Expected=-0.081778
31
Month=32, Predicted=-0.014679, Expected=0.002562
32
Month=33, Predicted=-0.002115, Expected=-0.008316
33
Month=34, Predicted=-0.000185, Expected=-0.017776
34
Month=35, Predicted=0.023801, Expected=0.009733
35
Month=36, Predicted=0.037842, Expected=0.003338
36
Month=37, Predicted=0.022870, Expected=-0.011902
37
Month=38, Predicted=-0.013851, Expected=0.033350
38
Month=39, Predicted=0.007691, Expected=0.021129
39
Month=40, Predicted=0.005724, Expected=0.021829
40
Month=41, Predicted=0.009670, Expected=0.066288
41
Month=42, Predicted=0.032991, Expected=-0.013772
42
Month=43, Predicted=-0.021359, Expected=0.035066
43
Month=44, Predicted=0.059402, Expected=0.024768
44
Month=45, Predicted=0.008789, Expected=-0.015519
45
Month=46, Predicted=0.008089, Expected=-0.004737
46
Month=47, Predicted=0.027373, Expected=-0.017040
47
Month=48, Predicted=0.021436, Expected=-0.092858
48
Month=49, Predicted=0.003923, Expected=-0.022384
49
Month=50, Predicted=0.029465, Expected=-0.047017
50
Month=51, Predicted=-0.042721, Expected=-0.023386
51
Month=52, Predicted=0.070981, Expected=-0.008924
52
Month=53, Predicted=0.044176, Expected=-0.024505
53
Month=54, Predicted=0.051352, Expected=0.011233
54
Month=55, Predicted=0.044965, Expected=0.059048
55
Month=56, Predicted=0.012536, Expected=0.028053
56
Month=57, Predicted=0.009816, Expected=0.009068
57
Month=58, Predicted=0.037284, Expected=0.016780
58
Month=59, Predicted=0.002966, Expected=-0.000525
59
Month=60, Predicted=0.033628, Expected=0.014907
60
Month=61, Predicted=-0.034909, Expected=0.010232
61
Month=62, Predicted=0.006961, Expected=-0.043480
62
Month=63, Predicted=-0.001089, Expected=-0.016681
63
Month=64, Predicted=0.023921, Expected=-0.005923
64
Month=65, Predicted=0.034771, Expected=-0.005932
65
Month=66, Predicted=-0.039988, Expected=0.023783
66
Month=67, Predicted=0.001221, Expected=0.002401
67
Month=68, Predicted=0.050465, Expected=-0.008802
68
Month=69, Predicted=0.025816, Expected=0.016964
69
Month=70, Predicted=0.053599, Expected=-0.004397
70
Month=71, Predicted=-0.004362, Expected=0.013208
71
Month=72, Predicted=0.010256, Expected=0.000422
72
Month=73, Predicted=0.025373, Expected=-0.016181
73
Month=74, Predicted=-0.004097, Expected=0.000437
74
Month=75, Predicted=0.008119, Expected=-0.032156
75
Month=76, Predicted=0.008724, Expected=-0.015527
76
Month=77, Predicted=-0.005576, Expected=0.000733
77
Month=78, Predicted=0.025910, Expected=-0.018847
78
Month=79, Predicted=-0.023363, Expected=0.013644
79
Month=80, Predicted=0.009032, Expected=-0.005694
80
Month=81, Predicted=0.012214, Expected=-0.026410
81
Month=82, Predicted=-0.001224, Expected=0.001772
82
Month=83, Predicted=-0.030539, Expected=0.013972
83
Month=84, Predicted=-0.005512, Expected=0.032951
84
Month=85, Predicted=0.008454, Expected=-0.003932
85
Month=86, Predicted=0.045977, Expected=0.037384
86
Month=87, Predicted=0.085324, Expected=0.003751
87
Month=88, Predicted=0.097857, Expected=0.010612
88
Month=89, Predicted=0.079802, Expected=0.007707
89
Month=90, Predicted=-0.001044, Expected=-0.037643
90
Month=91, Predicted=0.004511, Expected=-0.042758
91
Month=92, Predicted=0.013644, Expected=0.004143
92
Month=93, Predicted=-0.019489, Expected=-0.058364
93
Month=94, Predicted=-0.053521, Expected=-0.011597
94
Month=95, Predicted=-0.073180, Expected=0.004337
95
Month=96, Predicted=-0.088844, Expected=0.000788
96
Month=97, Predicted=0.038590, Expected=0.044371
97
Month=98, Predicted=-0.014045, Expected=0.003671
98
Month=99, Predicted=0.003379, Expected=0.020711
99
Month=100, Predicted=0.000418, Expected=-0.017325
>>> 
0
Month=1, Predicted=0.009275, Expected=-0.015771
1
Month=2, Predicted=0.005608, Expected=-0.010113
2
Month=3, Predicted=0.033766, Expected=-0.011114
3
Month=4, Predicted=-0.000071, Expected=0.010516
4
Month=5, Predicted=-0.021457, Expected=0.026309
5
Month=6, Predicted=0.029218, Expected=-0.019990
6
Month=7, Predicted=-0.009118, Expected=0.007293
7
Month=8, Predicted=-0.036082, Expected=0.002052
8
Month=9, Predicted=-0.026306, Expected=-0.018133
9
Month=10, Predicted=-0.056101, Expected=0.005647
10
Month=11, Predicted=-0.001437, Expected=-0.033142
11
Month=12, Predicted=-0.002930, Expected=-0.015812
12
Month=13, Predicted=-0.056500, Expected=-0.018375
13
Month=14, Predicted=-0.047488, Expected=-0.018276
14
Month=15, Predicted=-0.018744, Expected=0.016522
15
Month=16, Predicted=0.007335, Expected=0.019989
16
Month=17, Predicted=-0.023157, Expected=-0.018069
17
Month=18, Predicted=-0.004734, Expected=0.007300
18
Month=19, Predicted=-0.013724, Expected=0.002400
19
Month=20, Predicted=0.005734, Expected=-0.007581
20
Month=21, Predicted=0.011924, Expected=-0.006376
21
Month=22, Predicted=-0.012208, Expected=-0.022140
22
Month=23, Predicted=-0.027816, Expected=0.001473
23
Month=24, Predicted=-0.000307, Expected=0.067055
24
Month=25, Predicted=-0.021861, Expected=-0.011513
25
Month=26, Predicted=-0.019515, Expected=-0.026283
26
Month=27, Predicted=-0.003824, Expected=0.019458
27
Month=28, Predicted=-0.022018, Expected=0.002569
28
Month=29, Predicted=0.018607, Expected=-0.001020
29
Month=30, Predicted=0.022844, Expected=-0.030622
30
Month=31, Predicted=-0.012071, Expected=-0.081778
31
Month=32, Predicted=-0.020082, Expected=0.002562
32
Month=33, Predicted=-0.022354, Expected=-0.008316
33
Month=34, Predicted=-0.006724, Expected=-0.017776
34
Month=35, Predicted=-0.030339, Expected=0.009733
35
Month=36, Predicted=0.011647, Expected=0.003338
36
Month=37, Predicted=0.026424, Expected=-0.011902
37
Month=38, Predicted=-0.026544, Expected=0.033350
38
Month=39, Predicted=-0.016670, Expected=0.021129
39
Month=40, Predicted=-0.001621, Expected=0.021829
40
Month=41, Predicted=-0.027211, Expected=0.066288
41
Month=42, Predicted=0.006607, Expected=-0.013772
42
Month=43, Predicted=-0.040867, Expected=0.035066
43
Month=44, Predicted=-0.071486, Expected=0.024768
44
Month=45, Predicted=0.016933, Expected=-0.015519
45
Month=46, Predicted=-0.032146, Expected=-0.004737
46
Month=47, Predicted=0.007603, Expected=-0.017040
47
Month=48, Predicted=-0.017556, Expected=-0.092858
48
Month=49, Predicted=-0.038087, Expected=-0.022384
49
Month=50, Predicted=-0.003908, Expected=-0.047017
50
Month=51, Predicted=-0.030417, Expected=-0.023386
51
Month=52, Predicted=-0.011931, Expected=-0.008924
52
Month=53, Predicted=0.006578, Expected=-0.024505
53
Month=54, Predicted=-0.024456, Expected=0.011233
54
Month=55, Predicted=-0.020423, Expected=0.059048
55
Month=56, Predicted=0.013278, Expected=0.028053
56
Month=57, Predicted=-0.008589, Expected=0.009068
57
Month=58, Predicted=0.009031, Expected=0.016780
58
Month=59, Predicted=-0.041421, Expected=-0.000525
59
Month=60, Predicted=0.007275, Expected=0.014907
60
Month=61, Predicted=-0.026053, Expected=0.010232
61
Month=62, Predicted=0.017874, Expected=-0.043480
62
Month=63, Predicted=-0.009092, Expected=-0.016681
63
Month=64, Predicted=-0.028801, Expected=-0.005923
64
Month=65, Predicted=-0.054324, Expected=-0.005932
65
Month=66, Predicted=0.008978, Expected=0.023783
66
Month=67, Predicted=0.000000, Expected=0.002401
67
Month=68, Predicted=-0.014360, Expected=-0.008802
68
Month=69, Predicted=-0.065480, Expected=0.016964
69
Month=70, Predicted=-0.008130, Expected=-0.004397
70
Month=71, Predicted=-0.021580, Expected=0.013208
71
Month=72, Predicted=0.000418, Expected=0.000422
72
Month=73, Predicted=-0.012658, Expected=-0.016181
73
Month=74, Predicted=-0.055611, Expected=0.000437
74
Month=75, Predicted=0.049065, Expected=-0.032156
75
Month=76, Predicted=0.024334, Expected=-0.015527
76
Month=77, Predicted=-0.031775, Expected=0.000733
77
Month=78, Predicted=0.008860, Expected=-0.018847
78
Month=79, Predicted=0.006762, Expected=0.013644
79
Month=80, Predicted=-0.025226, Expected=-0.005694
80
Month=81, Predicted=-0.024375, Expected=-0.026410
81
Month=82, Predicted=-0.097097, Expected=0.001772
82
Month=83, Predicted=-0.028100, Expected=0.013972
83
Month=84, Predicted=0.026995, Expected=0.032951
84
Month=85, Predicted=-0.052621, Expected=-0.003932
85
Month=86, Predicted=0.011139, Expected=0.037384
86
Month=87, Predicted=-0.002602, Expected=0.003751
87
Month=88, Predicted=0.012719, Expected=0.010612
88
Month=89, Predicted=0.029949, Expected=0.007707
89
Month=90, Predicted=-0.013563, Expected=-0.037643
90
Month=91, Predicted=-0.043582, Expected=-0.042758
91
Month=92, Predicted=-0.040821, Expected=0.004143
92
Month=93, Predicted=-0.072672, Expected=-0.058364
93
Month=94, Predicted=-0.025315, Expected=-0.011597
94
Month=95, Predicted=-0.050988, Expected=0.004337
95
Month=96, Predicted=-0.044285, Expected=0.000788
96
Month=97, Predicted=-0.026563, Expected=0.044371
97
Month=98, Predicted=-0.026176, Expected=0.003671
98
Month=99, Predicted=0.002942, Expected=0.020711
99
Month=100, Predicted=0.025905, Expected=-0.017325
>>> ----------------------feature-knock-out---------------------------
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 150, in <module>
    for c in X.columns: X.loc[:,c] = t_r.normPercentile(X[c],perc=[1,99])
AttributeError: 'numpy.ndarray' object has no attribute 'columns'
>>> 
>>> 

  C-c C-cTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 184, in train
    self.model.fit(X_train,y_train,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 611, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 1661, in _filtered_call
    return self._call_flat(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 1745, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 593, in call
    outputs = execute.execute(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
KeyboardInterrupt
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 111, in <module>
    X1 = y1.reshape(-1,1)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py", line 5274, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'Series' object has no attribute 'reshape'
>>> y
array([ 0.0218267 , -0.01467988,  0.01991595, ...,  0.01787098,
        0.03858651,  0.03413722])
>>> y[1:] - y[:-1]
array([-0.03650658,  0.03459583, -0.02236158, ...,  0.02591511,
        0.02071553, -0.00444929])
>>> y
array([ 0.0218267 , -0.01467988,  0.01991595, ...,  0.01787098,
        0.03858651,  0.03413722])
>>> 

  C-c C-cTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 184, in train
    self.model.fit(X_train,y_train,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 611, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 1661, in _filtered_call
    return self._call_flat(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 1745, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 593, in call
    outputs = execute.execute(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
KeyboardInterrupt
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 182, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 156, in splitSet
    X = X1[shuffleL]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: '[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249, 6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289, 6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339, 6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369, 6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379, 6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489, 6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509, 6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539, 6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549, 6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559, 6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599, 6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619, 6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629, 6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649, 6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669, 6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709, 6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749, 6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759, 6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769, 6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809, 6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829, 6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839, 6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859, 6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869, 6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879, 6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899, 6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909, 6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919, 6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929, 6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939, 6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949, 6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989, 6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999, 7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059, 7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069, 7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079, 7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089, 7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099, 7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109, 7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129, 7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139, 7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149, 7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179, 7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209, 7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219, 7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239, 7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259, 7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269, 7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319, 7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349, 7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359, 7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369, 7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379, 7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399, 7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409, 7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419, 7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429, 7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439, 7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449, 7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479, 7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489, 7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509, 7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519, 7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529, 7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7  C-c C-c  C-c C-c




539, 7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549, 7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569, 7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579, 7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609, 7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619, 7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629, 7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639, 7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649, 7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659, 7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669, 7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679, 7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 7700, 7701, 7702, 7703, 7704, 7705, 7706, 7707, 7708, 7709, 7710, 7711, 7712, 7713, 7714, 7715, 7716, 7717, 7718, 7719, 7720, 7721, 7722, 7723, 7724, 7725, 7726, 7727, 7728, 7729, 7730, 7731, 7732, 7733, 7734, 7735, 7736, 7737, 7738, 7739, 7740, 7741, 7742, 7743, 7744, 7745, 7746, 7747, 7748, 7749, 7750, 7751, 7752, 7753, 7754, 7755, 7756, 7757, 7758, 7759, 7760, 7761, 7762, 7763, 7764, 7765, 7766, 7767, 7768, 7769, 7770, 7771, 7772, 7773, 7774, 7775, 7776, 7777, 7778, 7779, 7780, 7781, 7782, 7783, 7784, 7785, 7786, 7787, 7788, 7789, 7790, 7791, 7792, 7793, 7794, 7795, 7796, 7797, 7798, 7799, 7800, 7801, 7802, 7803, 7804, 7805, 7806, 7807, 7808, 7809, 7810, 7811, 7812, 7813, 7814, 7815, 7816, 7817, 7818, 7819, 7820, 7821, 7822, 7823, 7824, 7825, 7826, 7827, 7828, 7829, 7830, 7831, 7832, 7833, 7834, 7835, 7836, 7837, 7838, 7839, 7840, 7841, 7842, 7843, 7844, 7845, 7846, 7847, 7848, 7849, 7850, 7851, 7852, 7853, 7854, 7855, 7856, 7857, 7858, 7859, 7860, 7861, 7862, 7863, 7864, 7865, 7866, 7867, 7868, 7869, 7870, 7871, 7872, 7873, 7874, 7875, 7876, 7877, 7878, 7879, 7880, 7881, 7882, 7883, 7884, 7885, 7886, 7887, 7888, 7889, 7890, 7891, 7892, 7893, 7894, 7895, 7896, 7897, 7898, 7899, 7900, 7901, 7902, 7903, 7904, 7905, 7906, 7907, 7908, 7909, 7910, 7911, 7912, 7913, 7914, 7915, 7916, 7917, 7918, 7919, 7920, 7921, 7922, 7923, 7924, 7925, 7926, 7927, 7928, 7929, 7930, 7931, 7932, 7933, 7934, 7935, 7936, 7937, 7938, 7939, 7940, 7941, 7942, 7943, 7944, 7945, 7946, 7947, 7948, 7949, 7950, 7951, 7952, 7953, 7954, 7955, 7956, 7957, 7958, 7959, 7960, 7961, 7962, 7963, 7964, 7965, 7966, 7967, 7968, 7969, 7970, 7971, 7972, 7973, 7974, 7975, 7976, 7977, 7978, 7979, 7980, 7981, 7982, 7983, 7984, 7985, 7986, 7987, 7988, 7989, 7990, 7991, 7992, 7993, 7994, 7995, 7996, 7997, 7998, 7999, 8000, 8001, 8002, 8003, 8004, 8005, 8006, 8007, 8008, 8009, 8010, 8011, 8012, 8013, 8014, 8015, 8016, 8017, 8018, 8019, 8020, 8021, 8022, 8023, 8024, 8025, 8026, 8027, 8028, 8029, 8030, 8031, 8032, 8033, 8034, 8035, 8036, 8037, 8038, 8039, 8040, 8041, 8042, 8043, 8044, 8045, 8046, 8047, 8048, 8049, 8050, 8051, 8052, 8053, 8054, 8055, 8056, 8057, 8058, 8059, 8060, 8061, 8062, 8063, 8064, 8065, 8066, 8067, 8068, 8069, 8070, 8071, 8072, 8073, 8074, 8075, 8076, 8077, 8078, 8079, 8080, 8081, 8082, 8083, 8084, 8085, 8086, 8087, 8088, 8089, 8090, 8091, 8092, 8093, 8094, 8095, 8096, 8097, 8098, 8099, 8100, 8101, 8102, 8103, 8104, 8105, 8106, 8107, 8108, 8109, 8110, 8111, 8112, 8113, 8114, 8115, 8116, 8117, 8118, 8119, 8120, 8121, 8122, 8123, 8124, 8125, 8126, 8127, 8128, 8129, 8130, 8131, 8132, 8133, 8134, 8135, 8136, 8137, 8138, 8139, 8140, 8141, 8142, 8143, 8144, 8145, 8146, 8147, 8148, 8149, 8150, 8151, 8152, 8153, 8154, 8155, 8156, 8157, 8158, 8159, 8160, 8161, 8162, 8163, 8164, 8165, 8166, 8167, 8168, 8169, 8170, 8171, 8172, 8173, 8174, 8175, 8176, 8177, 8178, 8179, 8180, 8181, 8182, 8183, 8184, 8185, 8186, 8187, 8188, 8189, 8190, 8191, 8192, 8193, 8194, 8195, 8196, 8197, 8198, 8199, 8200, 8201, 8202, 8203, 8204, 8205, 8206, 8207, 8208, 8209, 8210, 8211, 8212, 8213, 8214, 8215, 8216, 8217, 8218, 8219, 8220, 8221Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
KeyboardInterrupt
>>> >>> >>> >>> >>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 116, in <module>
    X_train, X_test, y_train, y_test = tK.splitSet(tK.X,tK.y,shuffle=False)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 182, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 156, in splitSet
    X = X1[shuffleL]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: '[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249, 6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289, 6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339, 6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369, 6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379, 6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489, 6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509, 6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539, 6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549, 6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559, 6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599, 6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619, 6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629, 6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649, 6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669, 6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709, 6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749, 6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759, 6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769, 6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809, 6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829, 6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839, 6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859, 6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869, 6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879, 6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899, 6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909, 6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919, 6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929, 6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939, 6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949, 6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989, 6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999, 7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059, 7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069, 7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079, 7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089, 7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099, 7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109, 7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129, 7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139, 7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149, 7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179, 7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209, 7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219, 7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239, 7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259, 7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269, 7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319, 7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349, 7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359, 7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369, 7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379, 7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399, 7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409, 7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419, 7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429, 7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439, 7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449, 7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479, 7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489, 7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509, 7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519, 7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529, 7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7539, 7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549, 7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569, 7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579, 7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609, 7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619, 7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629, 7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639, 7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649, 7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659, 7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669, 7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679, 7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 7700, 7701, 7702, 7703, 7704, 7705, 7706, 7707, 7708, 7709, 7710, 7711, 7712, 7713, 7714, 7715, 7716, 7717, 7718, 7719, 7720, 7721, 7722, 7723, 7724, 7725, 7726, 7727, 7728, 7729, 7730, 7731, 7732, 7733, 7734, 7735, 7736, 7737, 7738, 7739, 7740, 7741, 7742, 7743, 7744, 7745, 7746, 7747, 7748, 7749, 7750, 7751, 7752, 7753, 7754, 7755, 7756, 7757, 7758, 7759, 7760, 7761, 7762, 7763, 7764, 7765, 7766, 7767, 7768, 7769, 7770, 7771, 7772, 7773, 7774, 7775, 7776, 7777, 7778, 7779, 7780, 7781, 7782, 7783, 7784, 7785, 7786, 7787, 7788, 7789, 7790, 7791, 7792, 7793, 7794, 7795, 7796, 7797, 7798, 7799, 7800, 7801, 7802, 7803, 7804, 7805, 7806, 7807, 7808, 7809, 7810, 7811, 7812, 7813, 7814, 7815, 7816, 7817, 7818, 7819, 7820, 7821, 7822, 7823, 7824, 7825, 7826, 7827, 7828, 7829, 7830, 7831, 7832, 7833, 7834, 7835, 7836, 7837, 7838, 7839, 7840, 7841, 7842, 7843, 7844, 7845, 7846, 7847, 7848, 7849, 7850, 7851, 7852, 7853, 7854, 7855, 7856, 7857, 7858, 7859, 7860, 7861, 7862, 7863, 7864, 7865, 7866, 7867, 7868, 7869, 7870, 7871, 7872, 7873, 7874, 7875, 7876, 7877, 7878, 7879, 7880, 7881, 7882, 7883, 7884, 7885, 7886, 7887, 7888, 7889, 7890, 7891, 7892, 7893, 7894, 7895, 7896, 7897, 7898, 7899, 7900, 7901, 7902, 7903, 7904, 7905, 7906, 7907, 7908, 7909, 7910, 7911, 7912, 7913, 7914, 7915, 7916, 7917, 7918, 7919, 7920, 7921, 7922, 7923, 7924, 7925, 7926, 7927, 7928, 7929, 7930, 7931, 7932, 7933, 7934, 7935, 7936, 7937, 7938, 7939, 7940, 7941, 7942, 7943, 7944, 7945, 7946, 7947, 7948, 7949, 7950, 7951, 7952, 7953, 7954, 7955, 7956, 7957, 7958, 7959, 7960, 7961, 7962, 7963, 7964, 7965, 7966, 7967, 7968, 7969, 7970, 7971, 7972, 7973, 7974, 7975, 7976, 7977, 7978, 7979, 7980, 7981, 7982, 7983, 7984, 7985, 7986, 7987, 7988, 7989, 7990, 7991, 7992, 7993, 7994, 7995, 7996, 7997, 7998, 7999, 8000, 8001, 8002, 8003, 8004, 8005, 8006, 8007, 8008, 8009, 8010, 8011, 8012, 8013, 8014, 8015, 8016, 8017, 8018, 8019, 8020, 8021, 8022, 8023, 8024, 8025, 8026, 8027, 8028, 8029, 8030, 8031, 8032, 8033, 8034, 8035, 8036, 8037, 8038, 8039, 8040, 8041, 8042, 8043, 8044, 8045, 8046, 8047, 8048, 8049, 8050, 8051, 8052, 8053, 8054, 8055, 8056, 8057, 8058, 8059, 8060, 8061, 8062, 8063, 8064, 8065, 8066, 8067, 8068, 8069, 8070, 8071, 8072, 8073, 8074, 8075, 8076, 8077, 8078, 8079, 8080, 8081, 8082, 8083, 8084, 8085, 8086, 8087, 8088, 8089, 8090, 8091, 8092, 8093, 8094, 8095, 8096, 8097, 8098, 8099, 8100, 8101, 8102, 8103, 8104, 8105, 8106, 8107, 8108, 8109, 8110, 8111, 8112, 8113, 8114, 8115, 8116, 8117, 8118, 8119, 8120, 8121, 8122, 8123, 8124, 8125, 8126, 8127, 8128, 8129, 8130, 8131, 8132, 8133, 8134, 8135, 8136, 8137, 8138, 8139, 8140, 8141, 8142, 8143, 8144, 8145, 8146, 8147, 8148, 8149, 8150, 8151, 8152, 8153, 8154, 8155, 8156, 8157, 8158, 8159, 8160, 8161, 8162, 8163, 8164, 8165, 8166, 8167, 8168, 8169, 8170, 8171, 8172, 8173, 8174, 8175, 8176, 8177, 8178, 8179, 8180, 8181, 8182, 8183, 8184, 8185, 8186, 8187, 8188, 8189, 8190, 8191, 8192, 8193, 8194, 8195, 8196, 8197, 8198, 8199, 8200, 8201, 8202, 8203, 8204, 8205, 8206, 8207, 8208, 8209, 8210, 8211, 8212, 8213, 8214, 8215, 8216, 8217, 8218, 8219, 8220, 8221, 8222, 8223, 8224, 8225, 8226, 8227, 8228, 8229, 8230, 8231, 8232, 8233, 8234, 8235, 8236, 8237, 8238, 8239, 8240, 8241, 8242, 8243, 8244, 8245, 8246, 8247, 8248, 8249, 8250, 8251, 8252, 8253, 8254, 8255, 8256, 8257, 8258, 8259, 8260, 8261, 8262, 8263, 8264, 8265, 8266, 8267, 8268, 8269, 8270, 8271, 8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281, 8282, 8283, 8284, 8285, 8286, 8287, 8288, 8289, 8290, 8291, 8292, 8293, 8294, 8295, 8296, 8297, 8298, 8299, 8300, 8301, 8302, 8303, 8304, 8305, 8306, 8307, 8308, 8309, 8310, 8311, 8312, 8313, 8314, 8315, 8316, 8317, 8318, 8319, 8320, 8321, 8322, 8323, 8324, 8325, 8326, 8327, 8328, 8329, 8330, 8331, 8332, 8333, 8334, 8335, 8336, 8337, 8338, 8339, 8340, 8341, 8342, 8343, 8344, 8345, 8346, 8347, 8348, 8349, 8350, 8351, 8352, 8353, 8354, 8355, 8356, 8357, 8358, 8359, 8360, 8361, 8362, 8363, 8364, 8365, 8366, 8367, 8368, 8369, 8370, 8371, 8372, 8373, 8374, 8375, 8376, 8377, 8378, 8379, 8380, 8381, 8382, 8383, 8384, 8385, 8386, 8387, 8388, 8389, 8390, 8391, 8392, 8393, 8394, 8395, 8396, 8397, 8398, 8399, 8400, 8401, 8402, 8403, 8404, 8405, 8406, 8407, 8408, 8409, 8410, 8411, 8412, 8413, 8414, 8415, 8416, 8417, 8418, 8419, 8420, 8421, 8422, 8423, 8424, 8425, 8426, 8427, 8428, 8429, 8430, 8431, 8432, 8433, 8434, 8435, 8436, 8437, 8438, 8439, 8440, 8441, 8442, 8443, 8444, 8445, 8446, 8447, 8448, 8449, 8450, 8451, 8452, 8453, 8454, 8455, 8456, 8457, 8458, 8459, 8460, 8461, 8462, 8463, 8464, 8465, 8466, 8467, 8468, 8469, 8470, 8471, 8472, 8473, 8474, 8475, 8476, 8477, 8478, 8479, 8480, 8481, 8482, 8483, 8484, 8485, 8486, 8487, 8488, 8489, 8490, 8491, 8492, 8493, 8494, 8495, 8496, 8497, 8498, 8499, 8500, 8501, 8502, 8503, 8504, 8505, 8506, 8507, 8508, 8509, 8510, 8511, 8512, 8513, 8514, 8515, 8516, 8517, 8518, 8519, 8520, 8521, 8522, 8523, 8524, 8525, 8526, 8527, 8528, 8529, 8530, 8531, 8532, 8533, 8534, 8535, 8536, 8537, 8538, 8539, 8540, 8541, 8542, 8543, 8544, 8545, 8546, 8547, 8548, 8549, 8550, 8551, 8552, 8553, 8554, 8555, 8556, 8557, 8558, 8559, 8560, 8561, 8562, 8563, 8564, 8565, 8566, 8567, 8568, 8569, 8570, 8571, 8572, 8573, 8574, 8575, 8576, 8577, 8578, 8579, 8580, 8581, 8582, 8583, 8584, 8585, 8586, 8587, 8588, 8589, 8590, 8591, 8592, 8593, 8594, 8595, 8596, 8597, 8598, 8599, 8600, 8601, 8602, 8603, 8604, 8605, 8606, 8607, 8608, 8609, 8610, 8611, 8612, 8613, 8614, 8615, 8616, 8617, 8618, 8619, 8620, 8621, 8622, 8623, 8624, 8625, 8626, 8627, 8628, 8629, 8630, 8631, 8632, 8633, 8634, 8635, 8636, 8637, 8638, 8639, 8640, 8641, 8642, 8643, 8644, 8645, 8646, 8647, 8648, 8649, 8650, 8651, 8652, 8653, 8654, 8655, 8656, 8657, 8658, 8659, 8660, 8661, 8662, 8663, 8664, 8665, 8666, 8667, 8668, 8669, 8670, 8671, 8672, 8673, 8674, 8675, 8676, 8677, 8678, 8679, 8680, 8681, 8682, 8683, 8684, 8685, 8686, 8687, 8688, 8689, 8690, 8691, 8692, 8693, 8694, 8695, 8696, 8697, 8698, 8699, 8700, 8701, 8702, 8703, 8704, 8705, 8706, 8707, 8708, 8709, 8710, 8711, 8712, 8713, 8714, 8715, 8716, 8717, 8718, 8719, 8720, 8721, 8722, 8723, 8724, 8725, 8726, 8727, 8728, 8729, 8730, 8731, 8732, 8733, 8734, 8735, 8736, 8737, 8738, 8739, 8740, 8741, 8742, 8743, 8744, 8745, 8746, 8747, 8748, 8749, 8750, 8751, 8752, 8753, 8754, 8755, 8756, 8757, 8758, 8759, 8760, 8761, 8762, 8763, 8764, 8765, 8766, 8767, 8768, 8769, 8770, 8771, 8772, 8773, 8774, 8775, 8776, 8777, 8778, 8779, 8780, 8781, 8782, 8783, 8784, 8785, 8786, 8787, 8788, 8789, 8790, 8791, 8792, 8793, 8794, 8795, 8796, 8797, 8798, 8799, 8800, 8801, 8802, 8803, 8804, 8805, 8806, 8807, 8808, 8809, 8810, 8811, 8812, 8813, 8814, 8815, 8816, 8817, 8818, 8819, 8820, 8821, 8822, 8823, 8824, 8825, 8826, 8827, 8828, 8829, 8830, 8831, 8832, 8833, 8834, 8835, 8836, 8837, 8838, 8839, 8840, 8841, 8842, 8843, 8844, 8845, 8846, 8847, 8848, 8849, 8850, 8851, 8852, 8853, 8854, 8855, 8856, 8857, 8858, 8859, 8860, 8861, 8862, 8863, 8864, 8865, 8866, 8867, 8868, 8869, 8870, 8871, 8872, 8873, 8874, 8875, 8876, 8877, 8878, 8879, 8880, 8881, 8882, 8883, 8884, 8885, 8886, 8887, 8888, 8889, 8890, 8891, 8892, 8893, 8894, 8895, 8896, 8897, 8898, 8899, 8900, 8901, 8902, 8903, 8904, 8905, 8906, 8907, 8908, 8909, 8910, 8911, 8912, 8913, 8914, 8915, 8916, 8917, 8918, 8919, 8920, 8921, 8922, 8923, 8924, 8925, 8926, 8927, 8928, 8929, 8930, 8931, 8932, 8933, 8934, 8935, 8936, 8937, 8938, 8939, 8940, 8941, 8942, 8943, 8944, 8945, 8946, 8947, 8948, 8949, 8950, 8951, 8952, 8953, 8954, 8955, 8956, 8957, 8958, 8959, 8960, 8961, 8962, 8963, 8964, 8965, 8966, 8967, 8968, 8969, 8970, 8971, 8972, 8973, 8974, 8975, 8976, 8977, 8978, 8979, 8980, 8981, 8982, 8983, 8984, 8985, 8986, 8987, 8988, 8989, 8990, 8991, 8992, 8993, 8994, 8995, 8996, 8997, 8998, 8999, 9000, 9001, 9002, 9003, 9004, 9005, 9006, 9007, 9008, 9009, 9010, 9011, 9012, 9013, 9014, 9015, 9016, 9017, 9018, 9019, 9020, 9021, 9022, 9023, 9024, 9025, 9026, 9027, 9028, 9029, 9030, 9031, 9032, 9033, 9034, 9035, 9036, 9037, 9038, 9039, 9040, 9041, 9042, 9043, 9044, 9045, 9046, 9047, 9048, 9049, 9050, 9051, 9052, 9053, 9054, 9055, 9056, 9057, 9058, 9059, 9060, 9061, 9062, 9063, 9064, 9065, 9066, 9067, 9068, 9069, 9070, 9071, 9072, 9073, 9074, 9075, 9076, 9077, 9078, 9079, 9080, 9081, 9082, 9083, 9084, 9085, 9086, 9087, 9088, 9089, 9090, 9091, 9092, 9093, 9094, 9095, 9096, 9097, 9098, 9099, 9100, 9101, 9102, 9103, 9104, 9105, 9106, 9107, 9108, 9109, 9110, 9111, 9112, 9113, 9114, 9115, 9116, 9117, 9118, 9119, 9120, 9121, 9122, 9123, 9124, 9125, 9126, 9127, 9128, 9129, 9130, 9131, 9132, 9133, 9134, 9135, 9136, 9137, 9138, 9139, 9140, 9141, 9142, 9143, 9144, 9145, 9146, 9147, 9148, 9149, 9150, 9151, 9152, 9153, 9154, 9155, 9156, 9157, 9158, 9159, 9160, 9161, 9162, 9163, 9164, 9165, 9166, 9167, 9168, 9169, 9170, 9171, 9172, 9173, 9174, 9175, 9176, 9177, 9178, 9179, 9180, 9181, 9182, 9183, 9184, 9185, 9186, 9187, 9188, 9189, 9190, 9191, 9192, 9193, 9194, 9195, 9196, 9197, 9198, 9199, 9200, 9201, 9202, 9203, 9204, 9205, 9206, 9207, 9208, 9209, 9210, 9211, 9212, 9213, 9214, 9215, 9216, 9217, 9218, 9219, 9220, 9221, 9222, 9223, 9224, 9225, 9226, 9227, 9228, 9229, 9230, 9231, 9232, 9233, 9234, 9235, 9236, 9237, 9238, 9239, 9240, 9241, 9242, 9243, 9244, 9245, 9246, 9247, 9248, 9249, 9250, 9251, 9252, 9253, 9254, 9255, 9256, 9257, 9258, 9259, 9260, 9261, 9262, 9263, 9264, 9265, 9266, 9267, 9268, 9269, 9270, 9271, 9272, 9273, 9274, 9275, 9276, 9277, 9278, 9279, 9280, 9281, 9282, 9283, 9284, 9285, 9286, 9287, 9288, 9289, 9290, 9291, 9292, 9293, 9294, 9295, 9296, 9297, 9298, 9299, 9300, 9301, 9302, 9303, 9304, 9305, 9306, 9307, 9308, 9309, 9310, 9311, 9312, 9313, 9314, 9315, 9316, 9317, 9318, 9319, 9320, 9321, 9322, 9323, 9324, 9325, 9326, 9327, 9328, 9329, 9330, 9331, 9332, 9333, 9334, 9335, 9336, 9337, 9338, 9339, 9340, 9341, 9342, 9343, 9344, 9345, 9346, 9347, 9348, 9349, 9350, 9351, 9352, 9353, 9354, 9355, 9356, 9357, 9358, 9359, 9360, 9361, 9362, 9363, 9364, 9365, 9366, 9367, 9368, 9369, 9370, 9371, 9372, 9373, 9374, 9375, 9376, 9377, 9378, 9379, 9380, 9381, 9382, 9383, 9384, 9385, 9386, 9387, 9388, 9389, 9390, 9391, 9392, 9393, 9394, 9395, 9396, 9397, 9398, 9399, 9400, 9401, 9402, 9403, 9404, 9405, 9406, 9407, 9408, 9409, 9410, 9411, 9412, 9413, 9414, 9415, 9416, 9417, 9418, 9419, 9420, 9421, 9422, 9423, 9424, 9425, 9426, 9427, 9428, 9429, 9430, 9431, 9432, 9433, 9434, 9435, 9436, 9437, 9438, 9439, 9440, 9441, 9442, 9443, 9444, 9445, 9446, 9447, 9448, 9449, 9450, 9451, 9452, 9453, 9454, 9455, 9456, 9457, 9458, 9459, 9460, 9461, 9462, 9463, 9464, 9465, 9466, 9467, 9468, 9469, 9470, 9471, 9472, 9473, 9474, 9475, 9476, 9477, 9478, 9479, 9480, 9481, 9482, 9483, 9484, 9485, 9486, 9487, 9488, 9489, 9490, 9491, 9492, 9493, 9494, 9495, 9496, 9497, 9498, 9499, 9500, 9501, 9502, 9503, 9504, 9505, 9506, 9507, 9508, 9509, 9510, 9511, 9512, 9513, 9514, 9515, 9516, 9517, 9518, 9519, 9520, 9521, 9522, 9523, 9524, 9525, 9526, 9527, 9528, 9529, 9530, 9531, 9532, 9533, 9534, 9535, 9536, 9537, 9538, 9539, 9540, 9541, 9542, 9543, 9544, 9545, 9546, 9547, 9548, 9549, 9550, 9551, 9552, 9553, 9554, 9555, 9556, 9557, 9558, 9559, 9560, 9561, 9562, 9563, 9564, 9565, 9566, 9567, 9568, 9569, 9570, 9571, 9572, 9573, 9574, 9575, 9576, 9577, 9578, 9579, 9580, 9581, 9582, 9583, 9584, 9585, 9586, 9587, 9588, 9589, 9590, 9591, 9592, 9593, 9594, 9595, 9596, 9597, 9598, 9599, 9600, 9601, 9602, 9603, 9604, 9605, 9606, 9607, 9608, 9609, 9610, 9611, 9612, 9613, 9614, 9615, 9616, 9617, 9618, 9619, 9620, 9621, 9622, 9623, 9624, 9625, 9626, 9627, 9628, 9629, 9630, 9631, 9632, 9633, 9634, 9635, 9636, 9637, 9638, 9639, 9640, 9641, 9642, 9643, 9644, 9645, 9646, 9647, 9648, 9649, 9650, 9651, 9652, 9653, 9654, 9655, 9656, 9657, 9658, 9659, 9660, 9661, 9662, 9663, 9664, 9665, 9666, 9667, 9668, 9669, 9670, 9671, 9672, 9673, 9674, 9675, 9676, 9677, 9678, 9679, 9680, 9681, 9682, 9683, 9684, 9685, 9686, 9687, 9688, 9689, 9690, 9691, 9692, 9693, 9694, 9695, 9696, 9697, 9698, 9699, 9700, 9701, 9702, 9703, 9704, 9705, 9706, 9707, 9708, 9709, 9710, 9711, 9712, 9713, 9714, 9715, 9716, 9717, 9718, 9719, 9720, 9721, 9722, 9723, 9724, 9725, 9726, 9727, 9728, 9729, 9730, 9731, 9732, 9733, 9734, 9735, 9736, 9737, 9738, 9739, 9740, 9741, 9742, 9743, 9744, 9745, 9746, 9747, 9748, 9749, 9750, 9751, 9752, 9753, 9754, 9755, 9756, 9757, 9758, 9759, 9760, 9761, 9762, 9763, 9764, 9765, 9766, 9767, 9768, 9769, 9770, 9771, 9772, 9773, 9774, 9775, 9776, 9777, 9778, 9779, 9780, 9781, 9782, 9783, 9784, 9785, 9786, 9787, 9788, 9789, 9790, 9791, 9792, 9793, 9794, 9795, 9796, 9797, 9798, 9799, 9800, 9801, 9802, 9803, 9804, 9805, 9806, 9807, 9808, 9809, 9810, 9811, 9812, 9813, 9814, 9815, 9816, 9817, 9818, 9819, 9820, 9821, 9822, 9823, 9824, 9825, 9826, 9827, 9828, 9829, 9830, 9831, 9832, 9833, 9834, 9835, 9836, 9837, 9838, 9839, 9840, 9841, 9842, 9843, 9844, 9845, 9846, 9847, 9848, 9849, 9850, 9851, 9852, 9853, 9854, 9855, 9856, 9857, 9858, 9859, 9860, 9861, 9862, 9863, 9864, 9865, 9866, 9867, 9868, 9869, 9870, 9871, 9872, 9873, 9874, 9875, 9876, 9877, 9878, 9879, 9880, 9881, 9882, 9883, 9884, 9885, 9886, 9887, 9888, 9889, 9890, 9891, 9892, 9893, 9894, 9895, 9896, 9897, 9898, 9899, 9900, 9901, 9902, 9903, 9904, 9905, 9906, 9907, 9908, 9909, 9910, 9911, 9912, 9913, 9914, 9915, 9916, 9917, 9918, 9919, 9920, 9921, 9922, 9923, 9924, 9925, 9926, 9927, 9928, 9929, 9930, 9931, 9932, 9933, 9934, 9935, 9936, 9937, 9938, 9939, 9940, 9941, 9942, 9943, 9944, 9945, 9946, 9947, 9948, 9949, 9950, 9951, 9952, 9953, 9954, 9955, 9956, 9957, 9958, 9959, 9960, 9961, 9962, 9963, 9964, 9965, 9966, 9967, 9968, 9969, 9970, 9971, 9972, 9973, 9974, 9975, 9976, 9977, 9978, 9979, 9980, 9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989, 9990, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999, 10000, 10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009, 10010, 10011, 10012, 10013, 10014, 10015, 10016, 10017, 10018, 10019, 10020, 10021, 10022, 10023, 10024, 10025, 10026, 10027, 10028, 10029, 10030, 10031, 10032, 10033, 10034, 10035, 10036, 10037, 10038, 10039, 10040, 10041, 10042, 10043, 10044, 10045, 10046, 10047, 10048, 10049, 10050, 10051, 10052, 10053, 10054, 10055, 10056, 10057, 10058, 10059, 10060, 10061, 10062, 10063, 10064, 10065, 10066, 10067, 10068, 10069, 10070, 10071, 10072, 10073, 10074, 10075, 10076, 10077, 10078, 10079, 10080, 10081, 10082, 10083, 10084, 10085, 10086, 10087, 10088, 10089, 10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099, 10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109, 10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119, 10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129, 10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139, 10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149, 10150, 10151, 10152, 10153, 10154, 10155, 10156, 10157, 10158, 10159, 10160, 10161, 10162, 10163, 10164, 10165, 10166, 10167, 10168, 10169, 10170, 10171, 10172, 10173, 10174, 10175, 10176, 10177, 10178, 10179, 10180, 10181, 10182, 10183, 10184, 10185, 10186, 10187, 10188, 10189, 10190, 10191, 10192, 10193, 10194, 10195, 10196, 10197, 10198, 10199, 10200, 10201, 10202, 10203, 10204, 10205, 10206, 10207, 10208, 10209, 10210, 10211, 10212, 10213, 10214, 10215, 10216, 10217, 10218, 10219, 10220, 10221, 10222, 10223, 10224, 10225, 10  C-c C-c  C-c C-c






226, 10227, 10228, 10229, 10230, 10231, 10232, 10233, 10234, 10235, 10236, 10237, 10238, 10239, 10240, 10241, 10242, 10243, 10244, 10245, 10246, 10247, 10248, 10249, 10250, 10251, 10252, 10253, 10254, 10255, 10256, 10257, 10258, 10259, 10260, 10261, 10262, 10263, 10264, 10265, 10266, 10267, 10268, 10269, 10270, 10271, 10272, 10273, 10274, 10275, 10276, 10277, 10278, 10279, 10280, 10281, 10282, 10283, 10284, 10285, 10286, 10287, 10288, 10289, 10290, 10291, 10292, 10293, 10294, 10295, 10296, 10297, 10298, 10299, 10300, 10301, 10302, 10303, 10304, 10305, 10306, 10307, 10308, 10309, 10310, 10311, 10312, 10313, 10314, 10315, 10316, 10317, 10318, 10319, 10320, 10321, 10322, 10323, 10324, 10325, 10326, 10327, 10328, 10329, 10330, 10331, 10332, 10333, 10334, 10335, 10336, 10337, 10338, 10339, 10340, 10341, 10342, 10343, 10344, 10345, 10346, 10347, 10348, 10349, 10350, 10351, 10352, 10353, 10354, 10355, 10356, 10357, 10358, 10359, 10360, 10361, 10362, 10363, 10364, 10365, 10366, 10367, 10368, 10369, 10370, 10371, 10372, 10373, 10374, 10375, 10376, 10377, 10378, 10379, 10380, 10381, 10382, 10383, 10384, 10385, 10386, 10387, 10388, 10389, 10390, 10391, 10392, 10393, 10394, 10395, 10396, 10397, 10398, 10399, 10400, 10401, 10402, 10403, 10404, 10405, 10406, 10407, 10408, 10409, 10410, 10411, 10412, 10413, 10414, 10415, 10416, 10417, 10418, 10419, 10420, 10421, 10422, 10423, 10424, 10425, 10426, 10427, 10428, 10429, 10430, 10431, 10432, 10433, 10434, 10435, 10436, 10437, 10438, 10439, 10440, 10441, 10442, 10443, 10444, 10445, 10446, 10447, 10448, 10449, 10450, 10451, 10452, 10453, 10454, 10455, 10456, 10457, 10458, 10459, 10460, 10461, 10462, 10463, 10464, 10465, 10466, 10467, 10468, 10469, 10470, 10471, 10472, 10473, 10474, 10475, 10476, 10477, 10478, 10479, 10480, 10481, 10482, 10483, 10484, 10485, 10486, 10487, 10488, 10489, 10490, 10491, 10492, 10493, 10494, 10495, 10496, 10497, 10498, 10499, 10500, 10501, 10502, 10503, 10504, 10505, 10506, 10507, 10508, 10509, 10510, 10511, 10512, 10513, 10514, 10515, 10516, 10517, 10518, 10519, 10520, 10521, 10522, 10523, 10524, 10525, 10526, 10527, 10528, 10529, 10530, 10531, 10532, 10533, 10534, 10535, 10536, 10537, 10538, 10539, 10540, 10541, 10542, 10543, 10544, 10545, 10546, 10547, 10548, 10549, 10550, 10551, 10552, 10553, 10554, 10555, 10556, 10557, 10558, 10559, 10560, 10561, 10562, 10563, 10564, 10565, 10566, 10567, 10568, 10569, 10570, 10571, 10572, 10573, 10574, 10575, 10576, 10577, 10578, 10579, 10580, 10581, 10582, 10583, 10584, 10585, 10586, 10587, 10588, 10589, 10590, 10591, 10592, 10593, 10594, 10595, 10596, 10597, 10598, 10599, 10600, 10601, 10602, 10603, 10604, 10605, 10606, 10607, 10608, 10609, 10610, 10611, 10612, 10613, 10614, 10615, 10616, 10617, 10618, 10619, 10620, 10621, 10622, 10623, 10624, 10625, 10626, 10627, 10628, 10629, 10630, 10631, 10632, 10633, 10634, 10635, 10636, 10637, 10638, 10639, 10640, 10641, 10642, 10643, 10644, 10645, 10646, 10647, 10648, 10649, 10650, 10651, 10652, 10653, 10654, 10655, 10656, 10657, 10658, 10659, 10660, 10661, 10662, 10663, 10664, 10665, 10666, 10667, 10668, 10669, 10670, 10671, 10672, 10673, 10674, 10675, 10676, 10677, 10678, 10679, 10680, 10681, 10682, 10683, 10684, 10685, 10686, 10687, 10688, 10689, 10690, 10691, 10692, 10693, 10694, 10695, 10696, 10697, 10698, 10699, 10700, 10701, 10702, 10703, 10704, 10705, 10706, 10707, 10708, 10709, 10710, 10711, 10712, 10713, 10714, 10715, 10716, 10717, 10718, 10719, 10720, 10721, 10722, 10723, 10724, 10725, 10726, 10727, 10728, 10729, 10730, 10731, 10732, 10733, 10734, 10735, 10736, 10737, 10738, 10739, 10740, 10741, 10742, 10743, 10744, 10745, 10746, 10747, 10748, 10749, 10750, 10751, 10752, 10753, 10754, 10755, 10756, 10757, 10758, 10759, 10760, 10761, 10762, 10763, 10764, 10765, 10766, 10767, 10768, 10769, 10770, 10771, 10772, 10773, 10774, 10775, 10776, 10777, 10778, 10779, 10780, 10781, 10782, 10783, 10784, 10785, 10786, 10787, 10788, 10789, 10790, 10791, 10792, 10793, 10794, 10795, 10796, 10797, 10798, 10799, 10800, 10801, 10802, 10803, 10804, 10805, 10806, 10807, 10808, 10809, 10810, 10>>>   File "<stdin>", line 1
    C-c C-c
    ^
IndentationError: unexpected indent
>>> >>> >>> >>> >>> >>> >>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 116, in <module>
    X_train, X_test, y_train, y_test = tK.splitSet(tK.X,tK.y,shuffle=False)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 182, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 156, in splitSet
    X = X1[shuffleL]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: '[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98] not in index'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 116, in <module>
    X_train, X_test, y_train, y_test = tK.splitSet(tK.X,tK.y,shuffle=False)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 184, in train
    self.model.fit(X_train,y_train,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 856, in fit
    epoch_logs = copy.copy(logs)
UnboundLocalError: local variable 'logs' referenced before assignment
>>> [[-0.05588201]
 [ 0.06728261]
 [-0.03137983]
 [ 0.01144572]
 [ 0.03703508]
 [-0.02437839]
 [-0.03417405]
 [ 0.04069081]
 [-0.03701002]
 [ 0.07782996]
 [-0.04515382]
 [-0.01169816]
 [ 0.02677971]
 [ 0.00265292]
 [ 0.0511291 ]
 [-0.03064904]
 [-0.01523473]
 [ 0.07878021]
 [ 0.02491508]
 [-0.01878613]
 [ 0.05211267]
 [-0.04833407]
 [ 0.03234307]
 [-0.00817726]
 [-0.0214285 ]
 [ 0.01630815]
 [ 0.05350504]
 [-0.02239574]
 [ 0.04564221]
 [-0.04028199]
 [ 0.07291197]
 [-0.04810864]
 [ 0.10346533]
 [-0.14490469]
 [ 0.0334437 ]
 [-0.00537368]
 [ 0.08338515]
 [-0.0449506 ]
 [ 0.0831346 ]
 [-0.15786204]
 [ 0.19356948]
 [ 0.0077582 ]
 [-0.08773264]
 [-0.03125441]
 [ 0.05756324]
 [-0.04496892]
 [ 0.06089748]
 [-0.00638373]
 [-0.05789242]
 [ 0.10551587]
 [ 0.0421393 ]
 [-0.03408446]
 [-0.01434324]
 [ 0.02279455]
 [-0.07700499]
 [ 0.08629866]
 [ 0.02057064]
 [-0.03020002]
 [ 0.04410002]
 [ 0.99999994]
 [-1.        ]
 [ 0.02111026]
 [ 0.04808466]
 [-0.03697406]
 [ 0.04045118]
 [-0.05981389]
 [-0.93976086]
 [ 0.9550689 ]
 [ 0.03296847]
 [-0.04488865]
 [ 0.04143957]
 [-0.04297176]
 [ 0.08406861]
 [-0.01295798]
 [ 0.03501057]
 [-0.02461231]
 [ 0.01336321]
 [ 0.05619264]
 [ 0.01367371]
 [-0.06972668]
 [ 0.07919082]
 [-0.03426797]
 [ 0.06723759]
 [-0.02521473]
 [ 0.02994549]
 [-0.03883101]
 [ 0.01665681]
 [-0.02341863]
 [ 0.03694251]
 [-0.01821575]
 [-0.02849349]
 [-0.06871139]
 [ 0.09967481]
 [ 0.05842551]
 [ 0.03940151]
 [-0.03775475]
 [-0.00671748]
 [ 0.0448299 ]
 [ 0.07701163]]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 116, in <module>
    X_train, X_test, y_train, y_test = tK.splitSet(tK.X,tK.y,shuffle=False)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 185, in train
    self.model.fit(X_train,y_train,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 856, in fit
    epoch_logs = copy.copy(logs)
UnboundLocalError: local variable 'logs' referenced before assignment
>>> []
[]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 116, in <module>
    X_train, X_test, y_train, y_test = tK.splitSet(tK.X,tK.y,shuffle=False)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 186, in train
    self.model.fit(X_train,y_train,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 856, in fit
    epoch_logs = copy.copy(logs)
UnboundLocalError: local variable 'logs' referenced before assignment
>>> []
[]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 116, in <module>
    X_train, X_test, y_train, y_test = tK.splitSet(tK.X,tK.y,shuffle=False)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 186, in train
    self.model.fit(X_train,y_train,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 856, in fit
    epoch_logs = copy.copy(logs)
UnboundLocalError: local variable 'logs' referenced before assignment
>>> empty training set
>>> empty training set
>>> empty training set
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    tK.lstmBatch(1, 4)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 183, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 162, in splitSet
    scaler = scaler.fit(train)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py", line 336, in fit
    return self.partial_fit(X, y)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py", line 369, in partial_fit
    X = self._validate_data(X, reset=first_pass,
  File "/usr/local/lib/python3.8/dist-packages/sklearn/base.py", line 420, in _validate_data
    X = check_array(X, **check_params)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 651, in check_array
    raise ValueError("Found array with %d sample(s) (shape=%s) while a"
ValueError: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by MinMaxScaler.
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    tK.lstmBatch(1, 4)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 183, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 162, in splitSet
    scaler = scaler.fit(train)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py", line 336, in fit
    return self.partial_fit(X, y)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py", line 369, in partial_fit
    X = self._validate_data(X, reset=first_pass,
  File "/usr/local/lib/python3.8/dist-packages/sklearn/base.py", line 420, in _validate_data
    X = check_array(X, **check_params)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 651, in check_array
    raise ValueError("Found array with %d sample(s) (shape=%s) while a"
ValueError: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by MinMaxScaler.
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 182, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 162, in splitSet
    scaler = scaler.fit(train)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py", line 336, in fit
    return self.partial_fit(X, y)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py", line 369, in partial_fit
    X = self._validate_data(X, reset=first_pass,
  File "/usr/local/lib/python3.8/dist-packages/sklearn/base.py", line 420, in _validate_data
    X = check_array(X, **check_params)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 651, in check_array
    raise ValueError("Found array with %d sample(s) (shape=%s) while a"
ValueError: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by MinMaxScaler.
>>> 
>>> 
>>> y1
array([-3.65065810e-02,  3.45958331e-02, -2.23615776e-02,  2.36142994e-03,
        1.71340584e-02, -1.83196715e-02, -2.39746676e-02,  1.92444973e-02,
       -2.56118605e-02,  4.06847694e-02, -3.03132400e-02, -1.09994322e-02,
        1.12136827e-02, -2.71461125e-03,  2.52704814e-02, -2.19396926e-02,
       -1.30410765e-02,  4.12333512e-02,  1.01372394e-02, -1.50912851e-02,
        2.58382884e-02, -3.21491866e-02,  1.44253842e-02, -8.96683105e-03,
       -1.66167192e-02,  5.16849368e-03,  2.66421008e-02, -1.71750988e-02,
        2.21029219e-02, -2.75007547e-02,  3.78456391e-02, -3.20190498e-02,
        5.54839650e-02, -8.78990003e-02,  1.50607730e-02, -7.34833815e-03,
        4.38917669e-02, -3.01959253e-02,  4.37471194e-02, -9.53792172e-02,
        1.07500713e-01,  2.32639756e-04, -5.48938196e-02, -2.22891735e-02,
        2.89848798e-02, -3.02065018e-02,  3.09097230e-02, -7.93143179e-03,
       -3.76671818e-02,  5.66677303e-02,  2.00807058e-02, -2.39229466e-02,
       -1.25264233e-02,  8.91306269e-03, -4.87007880e-02,  4.55737191e-02,
        7.62920726e-03, -2.16804780e-02,  2.12126197e-02,  5.73049624e-01,
       -5.81541900e-01,  7.94072719e-03,  2.35129353e-02, -2.55910996e-02,
        1.91061612e-02, -3.87764341e-02, -5.46766131e-01,  5.47111080e-01,
        1.47864232e-02, -3.01601591e-02,  1.96767529e-02, -2.90535501e-02,
        4.42863172e-02, -1.17267170e-02,  1.59653224e-02, -1.84547145e-02,
        3.46838988e-03,  2.81936384e-02,  3.64764313e-03, -4.44990564e-02,
        4.14703951e-02, -2.40288893e-02,  3.45698447e-02, -1.88024928e-02,
        1.30412739e-02, -2.66631120e-02,  5.36977138e-03, -1.77656102e-02,
        1.70806191e-02, -1.47620088e-02, -2.06953052e-02, -4.39129250e-02,
        5.32957117e-02,  2.94826661e-02,  1.85001876e-02, -2.60417882e-02,
       -8.12410383e-03,  2.16339788e-02,  4.02123572e-02])
>>> empty training set
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 187, in train
    self.model.fit(X_train,y_train,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 856, in fit
    epoch_logs = copy.copy(logs)
UnboundLocalError: local variable 'logs' referenced before assignment
>>> y1
array([-3.65065810e-02,  3.45958331e-02, -2.23615776e-02,  2.36142994e-03,
        1.71340584e-02, -1.83196715e-02, -2.39746676e-02,  1.92444973e-02,
       -2.56118605e-02,  4.06847694e-02, -3.03132400e-02, -1.09994322e-02,
        1.12136827e-02, -2.71461125e-03,  2.52704814e-02, -2.19396926e-02,
       -1.30410765e-02,  4.12333512e-02,  1.01372394e-02, -1.50912851e-02,
        2.58382884e-02, -3.21491866e-02,  1.44253842e-02, -8.96683105e-03,
       -1.66167192e-02,  5.16849368e-03,  2.66421008e-02, -1.71750988e-02,
        2.21029219e-02, -2.75007547e-02,  3.78456391e-02, -3.20190498e-02,
        5.54839650e-02, -8.78990003e-02,  1.50607730e-02, -7.34833815e-03,
        4.38917669e-02, -3.01959253e-02,  4.37471194e-02, -9.53792172e-02,
        1.07500713e-01,  2.32639756e-04, -5.48938196e-02, -2.22891735e-02,
        2.89848798e-02, -3.02065018e-02,  3.09097230e-02, -7.93143179e-03,
       -3.76671818e-02,  5.66677303e-02,  2.00807058e-02, -2.39229466e-02,
       -1.25264233e-02,  8.91306269e-03, -4.87007880e-02,  4.55737191e-02,
        7.62920726e-03, -2.16804780e-02,  2.12126197e-02,  5.73049624e-01,
       -5.81541900e-01,  7.94072719e-03,  2.35129353e-02, -2.55910996e-02,
        1.91061612e-02, -3.87764341e-02, -5.46766131e-01,  5.47111080e-01,
        1.47864232e-02, -3.01601591e-02,  1.96767529e-02, -2.90535501e-02,
        4.42863172e-02, -1.17267170e-02,  1.59653224e-02, -1.84547145e-02,
        3.46838988e-03,  2.81936384e-02,  3.64764313e-03, -4.44990564e-02,
        4.14703951e-02, -2.40288893e-02,  3.45698447e-02, -1.88024928e-02,
        1.30412739e-02, -2.66631120e-02,  5.36977138e-03, -1.77656102e-02,
        1.70806191e-02, -1.47620088e-02, -2.06953052e-02, -4.39129250e-02,
        5.32957117e-02,  2.94826661e-02,  1.85001876e-02, -2.60417882e-02,
       -8.12410383e-03,  2.16339788e-02,  4.02123572e-02])
>>> X1
array([[-3.65065810e-02],
       [ 3.45958331e-02],
       [-2.23615776e-02],
       [ 2.36142994e-03],
       [ 1.71340584e-02],
       [-1.83196715e-02],
       [-2.39746676e-02],
       [ 1.92444973e-02],
       [-2.56118605e-02],
       [ 4.06847694e-02],
       [-3.03132400e-02],
       [-1.09994322e-02],
       [ 1.12136827e-02],
       [-2.71461125e-03],
       [ 2.52704814e-02],
       [-2.19396926e-02],
       [-1.30410765e-02],
       [ 4.12333512e-02],
       [ 1.01372394e-02],
       [-1.50912851e-02],
       [ 2.58382884e-02],
       [-3.21491866e-02],
       [ 1.44253842e-02],
       [-8.96683105e-03],
       [-1.66167192e-02],
       [ 5.16849368e-03],
       [ 2.66421008e-02],
       [-1.71750988e-02],
       [ 2.21029219e-02],
       [-2.75007547e-02],
       [ 3.78456391e-02],
       [-3.20190498e-02],
       [ 5.54839650e-02],
       [-8.78990003e-02],
       [ 1.50607730e-02],
       [-7.34833815e-03],
       [ 4.38917669e-02],
       [-3.01959253e-02],
       [ 4.37471194e-02],
       [-9.53792172e-02],
       [ 1.07500713e-01],
       [ 2.32639756e-04],
       [-5.48938196e-02],
       [-2.22891735e-02],
       [ 2.89848798e-02],
       [-3.02065018e-02],
       [ 3.09097230e-02],
       [-7.93143179e-03],
       [-3.76671818e-02],
       [ 5.66677303e-02],
       [ 2.00807058e-02],
       [-2.39229466e-02],
       [-1.25264233e-02],
       [ 8.91306269e-03],
       [-4.87007880e-02],
       [ 4.55737191e-02],
       [ 7.62920726e-03],
       [-2.16804780e-02],
       [ 2.12126197e-02],
       [ 5.73049624e-01],
       [-5.81541900e-01],
       [ 7.94072719e-03],
       [ 2.35129353e-02],
       [-2.55910996e-02],
       [ 1.91061612e-02],
       [-3.87764341e-02],
       [-5.46766131e-01],
       [ 5.47111080e-01],
       [ 1.47864232e-02],
       [-3.01601591e-02],
       [ 1.96767529e-02],
       [-2.90535501e-02],
       [ 4.42863172e-02],
       [-1.17267170e-02],
       [ 1.59653224e-02],
       [-1.84547145e-02],
       [ 3.46838988e-03],
       [ 2.81936384e-02],
       [ 3.64764313e-03],
       [-4.44990564e-02],
       [ 4.14703951e-02],
       [-2.40288893e-02],
       [ 3.45698447e-02],
       [-1.88024928e-02],
       [ 1.30412739e-02],
       [-2.66631120e-02],
       [ 5.36977138e-03],
       [-1.77656102e-02],
       [ 1.70806191e-02],
       [-1.47620088e-02],
       [-2.06953052e-02],
       [-4.39129250e-02],
       [ 5.32957117e-02],
       [ 2.94826661e-02],
       [ 1.85001876e-02],
       [-2.60417882e-02],
       [-8.12410383e-03],
       [ 2.16339788e-02],
       [ 4.02123572e-02]])
>>> [] [[ 0.00000000e+00]
 [-3.65065810e-02]
 [ 3.45958331e-02]
 [-2.23615776e-02]
 [ 2.36142994e-03]
 [ 1.71340584e-02]
 [-1.83196715e-02]
 [-2.39746676e-02]
 [ 1.92444973e-02]
 [-2.56118605e-02]
 [ 4.06847694e-02]
 [-3.03132400e-02]
 [-1.09994322e-02]
 [ 1.12136827e-02]
 [-2.71461125e-03]
 [ 2.52704814e-02]
 [-2.19396926e-02]
 [-1.30410765e-02]
 [ 4.12333512e-02]
 [ 1.01372394e-02]
 [-1.50912851e-02]
 [ 2.58382884e-02]
 [-3.21491866e-02]
 [ 1.44253842e-02]
 [-8.96683105e-03]
 [-1.66167192e-02]
 [ 5.16849368e-03]
 [ 2.66421008e-02]
 [-1.71750988e-02]
 [ 2.21029219e-02]
 [-2.75007547e-02]
 [ 3.78456391e-02]
 [-3.20190498e-02]
 [ 5.54839650e-02]
 [-8.78990003e-02]
 [ 1.50607730e-02]
 [-7.34833815e-03]
 [ 4.38917669e-02]
 [-3.01959253e-02]
 [ 4.37471194e-02]
 [-9.53792172e-02]
 [ 1.07500713e-01]
 [ 2.32639756e-04]
 [-5.48938196e-02]
 [-2.22891735e-02]
 [ 2.89848798e-02]
 [-3.02065018e-02]
 [ 3.09097230e-02]
 [-7.93143179e-03]
 [-3.76671818e-02]
 [ 5.66677303e-02]
 [ 2.00807058e-02]
 [-2.39229466e-02]
 [-1.25264233e-02]
 [ 8.91306269e-03]
 [-4.87007880e-02]
 [ 4.55737191e-02]
 [ 7.62920726e-03]
 [-2.16804780e-02]
 [ 2.12126197e-02]
 [ 5.73049624e-01]
 [-5.81541900e-01]
 [ 7.94072719e-03]
 [ 2.35129353e-02]
 [-2.55910996e-02]
 [ 1.91061612e-02]
 [-3.87764341e-02]
 [-5.46766131e-01]
 [ 5.47111080e-01]
 [ 1.47864232e-02]
 [-3.01601591e-02]
 [ 1.96767529e-02]
 [-2.90535501e-02]
 [ 4.42863172e-02]
 [-1.17267170e-02]
 [ 1.59653224e-02]
 [-1.84547145e-02]
 [ 3.46838988e-03]
 [ 2.81936384e-02]
 [ 3.64764313e-03]
 [-4.44990564e-02]
 [ 4.14703951e-02]
 [-2.40288893e-02]
 [ 3.45698447e-02]
 [-1.88024928e-02]
 [ 1.30412739e-02]
 [-2.66631120e-02]
 [ 5.36977138e-03]
 [-1.77656102e-02]
 [ 1.70806191e-02]
 [-1.47620088e-02]
 [-2.06953052e-02]
 [-4.39129250e-02]
 [ 5.32957117e-02]
 [ 2.94826661e-02]
 [ 1.85001876e-02]
 [-2.60417882e-02]
 [-8.12410383e-03]
 [ 2.16339788e-02]] [] [-3.65065810e-02  3.45958331e-02 -2.23615776e-02  2.36142994e-03
  1.71340584e-02 -1.83196715e-02 -2.39746676e-02  1.92444973e-02
 -2.56118605e-02  4.06847694e-02 -3.03132400e-02 -1.09994322e-02
  1.12136827e-02 -2.71461125e-03  2.52704814e-02 -2.19396926e-02
 -1.30410765e-02  4.12333512e-02  1.01372394e-02 -1.50912851e-02
  2.58382884e-02 -3.21491866e-02  1.44253842e-02 -8.96683105e-03
 -1.66167192e-02  5.16849368e-03  2.66421008e-02 -1.71750988e-02
  2.21029219e-02 -2.75007547e-02  3.78456391e-02 -3.20190498e-02
  5.54839650e-02 -8.78990003e-02  1.50607730e-02 -7.34833815e-03
  4.38917669e-02 -3.01959253e-02  4.37471194e-02 -9.53792172e-02
  1.07500713e-01  2.32639756e-04 -5.48938196e-02 -2.22891735e-02
  2.89848798e-02 -3.02065018e-02  3.09097230e-02 -7.93143179e-03
 -3.76671818e-02  5.66677303e-02  2.00807058e-02 -2.39229466e-02
 -1.25264233e-02  8.91306269e-03 -4.87007880e-02  4.55737191e-02
  7.62920726e-03 -2.16804780e-02  2.12126197e-02  5.73049624e-01
 -5.81541900e-01  7.94072719e-03  2.35129353e-02 -2.55910996e-02
  1.91061612e-02 -3.87764341e-02 -5.46766131e-01  5.47111080e-01
  1.47864232e-02 -3.01601591e-02  1.96767529e-02 -2.90535501e-02
  4.42863172e-02 -1.17267170e-02  1.59653224e-02 -1.84547145e-02
  3.46838988e-03  2.81936384e-02  3.64764313e-03 -4.44990564e-02
  4.14703951e-02 -2.40288893e-02  3.45698447e-02 -1.88024928e-02
  1.30412739e-02 -2.66631120e-02  5.36977138e-03 -1.77656102e-02
  1.70806191e-02 -1.47620088e-02 -2.06953052e-02 -4.39129250e-02
  5.32957117e-02  2.94826661e-02  1.85001876e-02 -2.60417882e-02
 -8.12410383e-03  2.16339788e-02  4.02123572e-02]
empty training set
[] [[ 0.00000000e+00]
 [-3.65065810e-02]
 [ 3.45958331e-02]
 [-2.23615776e-02]
 [ 2.36142994e-03]
 [ 1.71340584e-02]
 [-1.83196715e-02]
 [-2.39746676e-02]
 [ 1.92444973e-02]
 [-2.56118605e-02]
 [ 4.06847694e-02]
 [-3.03132400e-02]
 [-1.09994322e-02]
 [ 1.12136827e-02]
 [-2.71461125e-03]
 [ 2.52704814e-02]
 [-2.19396926e-02]
 [-1.30410765e-02]
 [ 4.12333512e-02]
 [ 1.01372394e-02]
 [-1.50912851e-02]
 [ 2.58382884e-02]
 [-3.21491866e-02]
 [ 1.44253842e-02]
 [-8.96683105e-03]
 [-1.66167192e-02]
 [ 5.16849368e-03]
 [ 2.66421008e-02]
 [-1.71750988e-02]
 [ 2.21029219e-02]
 [-2.75007547e-02]
 [ 3.78456391e-02]
 [-3.20190498e-02]
 [ 5.54839650e-02]
 [-8.78990003e-02]
 [ 1.50607730e-02]
 [-7.34833815e-03]
 [ 4.38917669e-02]
 [-3.01959253e-02]
 [ 4.37471194e-02]
 [-9.53792172e-02]
 [ 1.07500713e-01]
 [ 2.32639756e-04]
 [-5.48938196e-02]
 [-2.22891735e-02]
 [ 2.89848798e-02]
 [-3.02065018e-02]
 [ 3.09097230e-02]
 [-7.93143179e-03]
 [-3.76671818e-02]
 [ 5.66677303e-02]
 [ 2.00807058e-02]
 [-2.39229466e-02]
 [-1.25264233e-02]
 [ 8.91306269e-03]
 [-4.87007880e-02]
 [ 4.55737191e-02]
 [ 7.62920726e-03]
 [-2.16804780e-02]
 [ 2.12126197e-02]
 [ 5.73049624e-01]
 [-5.81541900e-01]
 [ 7.94072719e-03]
 [ 2.35129353e-02]
 [-2.55910996e-02]
 [ 1.91061612e-02]
 [-3.87764341e-02]
 [-5.46766131e-01]
 [ 5.47111080e-01]
 [ 1.47864232e-02]
 [-3.01601591e-02]
 [ 1.96767529e-02]
 [-2.90535501e-02]
 [ 4.42863172e-02]
 [-1.17267170e-02]
 [ 1.59653224e-02]
 [-1.84547145e-02]
 [ 3.46838988e-03]
 [ 2.81936384e-02]
 [ 3.64764313e-03]
 [-4.44990564e-02]
 [ 4.14703951e-02]
 [-2.40288893e-02]
 [ 3.45698447e-02]
 [-1.88024928e-02]
 [ 1.30412739e-02]
 [-2.66631120e-02]
 [ 5.36977138e-03]
 [-1.77656102e-02]
 [ 1.70806191e-02]
 [-1.47620088e-02]
 [-2.06953052e-02]
 [-4.39129250e-02]
 [ 5.32957117e-02]
 [ 2.94826661e-02]
 [ 1.85001876e-02]
 [-2.60417882e-02]
 [-8.12410383e-03]
 [ 2.16339788e-02]] [] [-3.65065810e-02  3.45958331e-02 -2.23615776e-02  2.36142994e-03
  1.71340584e-02 -1.83196715e-02 -2.39746676e-02  1.92444973e-02
 -2.56118605e-02  4.06847694e-02 -3.03132400e-02 -1.09994322e-02
  1.12136827e-02 -2.71461125e-03  2.52704814e-02 -2.19396926e-02
 -1.30410765e-02  4.12333512e-02  1.01372394e-02 -1.50912851e-02
  2.58382884e-02 -3.21491866e-02  1.44253842e-02 -8.96683105e-03
 -1.66167192e-02  5.16849368e-03  2.66421008e-02 -1.71750988e-02
  2.21029219e-02 -2.75007547e-02  3.78456391e-02 -3.20190498e-02
  5.54839650e-02 -8.78990003e-02  1.50607730e-02 -7.34833815e-03
  4.38917669e-02 -3.01959253e-02  4.37471194e-02 -9.53792172e-02
  1.07500713e-01  2.32639756e-04 -5.48938196e-02 -2.22891735e-02
  2.89848798e-02 -3.02065018e-02  3.09097230e-02 -7.93143179e-03
 -3.76671818e-02  5.66677303e-02  2.00807058e-02 -2.39229466e-02
 -1.25264233e-02  8.91306269e-03 -4.87007880e-02  4.55737191e-02
  7.62920726e-03 -2.16804780e-02  2.12126197e-02  5.73049624e-01
 -5.81541900e-01  7.94072719e-03  2.35129353e-02 -2.55910996e-02
  1.91061612e-02 -3.87764341e-02 -5.46766131e-01  5.47111080e-01
  1.47864232e-02 -3.01601591e-02  1.96767529e-02 -2.90535501e-02
  4.42863172e-02 -1.17267170e-02  1.59653224e-02 -1.84547145e-02
  3.46838988e-03  2.81936384e-02  3.64764313e-03 -4.44990564e-02
  4.14703951e-02 -2.40288893e-02  3.45698447e-02 -1.88024928e-02
  1.30412739e-02 -2.66631120e-02  5.36977138e-03 -1.77656102e-02
  1.70806191e-02 -1.47620088e-02 -2.06953052e-02 -4.39129250e-02
  5.32957117e-02  2.94826661e-02  1.85001876e-02 -2.60417882e-02
 -8.12410383e-03  2.16339788e-02  4.02123572e-02]
>>> [] []
empty training set
[] []
>>> [] []
empty training set
[] []
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 183, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 162, in splitSet
    scaler = scaler.fit(train)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py", line 336, in fit
    return self.partial_fit(X, y)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py", line 369, in partial_fit
    X = self._validate_data(X, reset=first_pass,
  File "/usr/local/lib/python3.8/dist-packages/sklearn/base.py", line 420, in _validate_data
    X = check_array(X, **check_params)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 651, in check_array
    raise ValueError("Found array with %d sample(s) (shape=%s) while a"
ValueError: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by MinMaxScaler.
>>> []
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 184, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 163, in splitSet
    scaler = scaler.fit(train)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py", line 336, in fit
    return self.partial_fit(X, y)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py", line 369, in partial_fit
    X = self._validate_data(X, reset=first_pass,
  File "/usr/local/lib/python3.8/dist-packages/sklearn/base.py", line 420, in _validate_data
    X = check_array(X, **check_params)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 651, in check_array
    raise ValueError("Found array with %d sample(s) (shape=%s) while a"
ValueError: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by MinMaxScaler.
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 183, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 162, in splitSet
    scaler = scaler.fit(train)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py", line 336, in fit
    return self.partial_fit(X, y)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_data.py", line 369, in partial_fit
    X = self._validate_data(X, reset=first_pass,
  File "/usr/local/lib/python3.8/dist-packages/sklearn/base.py", line 420, in _validate_data
    X = check_array(X, **check_params)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 651, in check_array
    raise ValueError("Found array with %d sample(s) (shape=%s) while a"
ValueError: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by MinMaxScaler.
>>> y1
array([-3.65065810e-02,  3.45958331e-02, -2.23615776e-02,  2.36142994e-03,
        1.71340584e-02, -1.83196715e-02, -2.39746676e-02,  1.92444973e-02,
       -2.56118605e-02,  4.06847694e-02, -3.03132400e-02, -1.09994322e-02,
        1.12136827e-02, -2.71461125e-03,  2.52704814e-02, -2.19396926e-02,
       -1.30410765e-02,  4.12333512e-02,  1.01372394e-02, -1.50912851e-02,
        2.58382884e-02, -3.21491866e-02,  1.44253842e-02, -8.96683105e-03,
       -1.66167192e-02,  5.16849368e-03,  2.66421008e-02, -1.71750988e-02,
        2.21029219e-02, -2.75007547e-02,  3.78456391e-02, -3.20190498e-02,
        5.54839650e-02, -8.78990003e-02,  1.50607730e-02, -7.34833815e-03,
        4.38917669e-02, -3.01959253e-02,  4.37471194e-02, -9.53792172e-02,
        1.07500713e-01,  2.32639756e-04, -5.48938196e-02, -2.22891735e-02,
        2.89848798e-02, -3.02065018e-02,  3.09097230e-02, -7.93143179e-03,
       -3.76671818e-02,  5.66677303e-02,  2.00807058e-02, -2.39229466e-02,
       -1.25264233e-02,  8.91306269e-03, -4.87007880e-02,  4.55737191e-02,
        7.62920726e-03, -2.16804780e-02,  2.12126197e-02,  5.73049624e-01,
       -5.81541900e-01,  7.94072719e-03,  2.35129353e-02, -2.55910996e-02,
        1.91061612e-02, -3.87764341e-02, -5.46766131e-01,  5.47111080e-01,
        1.47864232e-02, -3.01601591e-02,  1.96767529e-02, -2.90535501e-02,
        4.42863172e-02, -1.17267170e-02,  1.59653224e-02, -1.84547145e-02,
        3.46838988e-03,  2.81936384e-02,  3.64764313e-03, -4.44990564e-02,
        4.14703951e-02, -2.40288893e-02,  3.45698447e-02, -1.88024928e-02,
        1.30412739e-02, -2.66631120e-02,  5.36977138e-03, -1.77656102e-02,
        1.70806191e-02, -1.47620088e-02, -2.06953052e-02, -4.39129250e-02,
        5.32957117e-02,  2.94826661e-02,  1.85001876e-02, -2.60417882e-02,
       -8.12410383e-03,  2.16339788e-02,  4.02123572e-02])
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 108, in <module>
    importlib.reload(tls)
  File "/usr/lib/python3.8/importlib/__init__.py", line 169, in reload
    _bootstrap._exec(spec, module)
  File "<frozen importlib._bootstrap>", line 604, in _exec
  File "<frozen importlib._bootstrap_external>", line 779, in exec_module
  File "<frozen importlib._bootstrap_external>", line 916, in get_code
  File "<frozen importlib._bootstrap_external>", line 846, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 162
    raise: print("ciccia")
         ^
SyntaxError: invalid syntax
>>> []
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 185, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 162, in splitSet
    if True:  raise Exception('qui')
Exception: qui
>>> []
[[ 0.00000000e+00 -3.65065810e-02]
 [-3.65065810e-02  3.45958331e-02]
 [ 3.45958331e-02 -2.23615776e-02]
 [-2.23615776e-02  2.36142994e-03]
 [ 2.36142994e-03  1.71340584e-02]
 [ 1.71340584e-02 -1.83196715e-02]
 [-1.83196715e-02 -2.39746676e-02]
 [-2.39746676e-02  1.92444973e-02]
 [ 1.92444973e-02 -2.56118605e-02]
 [-2.56118605e-02  4.06847694e-02]
 [ 4.06847694e-02 -3.03132400e-02]
 [-3.03132400e-02 -1.09994322e-02]
 [-1.09994322e-02  1.12136827e-02]
 [ 1.12136827e-02 -2.71461125e-03]
 [-2.71461125e-03  2.52704814e-02]
 [ 2.52704814e-02 -2.19396926e-02]
 [-2.19396926e-02 -1.30410765e-02]
 [-1.30410765e-02  4.12333512e-02]
 [ 4.12333512e-02  1.01372394e-02]
 [ 1.01372394e-02 -1.50912851e-02]
 [-1.50912851e-02  2.58382884e-02]
 [ 2.58382884e-02 -3.21491866e-02]
 [-3.21491866e-02  1.44253842e-02]
 [ 1.44253842e-02 -8.96683105e-03]
 [-8.96683105e-03 -1.66167192e-02]
 [-1.66167192e-02  5.16849368e-03]
 [ 5.16849368e-03  2.66421008e-02]
 [ 2.66421008e-02 -1.71750988e-02]
 [-1.71750988e-02  2.21029219e-02]
 [ 2.21029219e-02 -2.75007547e-02]
 [-2.75007547e-02  3.78456391e-02]
 [ 3.78456391e-02 -3.20190498e-02]
 [-3.20190498e-02  5.54839650e-02]
 [ 5.54839650e-02 -8.78990003e-02]
 [-8.78990003e-02  1.50607730e-02]
 [ 1.50607730e-02 -7.34833815e-03]
 [-7.34833815e-03  4.38917669e-02]
 [ 4.38917669e-02 -3.01959253e-02]
 [-3.01959253e-02  4.37471194e-02]
 [ 4.37471194e-02 -9.53792172e-02]
 [-9.53792172e-02  1.07500713e-01]
 [ 1.07500713e-01  2.32639756e-04]
 [ 2.32639756e-04 -5.48938196e-02]
 [-5.48938196e-02 -2.22891735e-02]
 [-2.22891735e-02  2.89848798e-02]
 [ 2.89848798e-02 -3.02065018e-02]
 [-3.02065018e-02  3.09097230e-02]
 [ 3.09097230e-02 -7.93143179e-03]
 [-7.93143179e-03 -3.76671818e-02]
 [-3.76671818e-02  5.66677303e-02]
 [ 5.66677303e-02  2.00807058e-02]
 [ 2.00807058e-02 -2.39229466e-02]
 [-2.39229466e-02 -1.25264233e-02]
 [-1.25264233e-02  8.91306269e-03]
 [ 8.91306269e-03 -4.87007880e-02]
 [-4.87007880e-02  4.55737191e-02]
 [ 4.55737191e-02  7.62920726e-03]
 [ 7.62920726e-03 -2.16804780e-02]
 [-2.16804780e-02  2.12126197e-02]
 [ 2.12126197e-02  5.73049624e-01]
 [ 5.73049624e-01 -5.81541900e-01]
 [-5.81541900e-01  7.94072719e-03]
 [ 7.94072719e-03  2.35129353e-02]
 [ 2.35129353e-02 -2.55910996e-02]
 [-2.55910996e-02  1.91061612e-02]
 [ 1.91061612e-02 -3.87764341e-02]
 [-3.87764341e-02 -5.46766131e-01]
 [-5.46766131e-01  5.47111080e-01]
 [ 5.47111080e-01  1.47864232e-02]
 [ 1.47864232e-02 -3.01601591e-02]
 [-3.01601591e-02  1.96767529e-02]
 [ 1.96767529e-02 -2.90535501e-02]
 [-2.90535501e-02  4.42863172e-02]
 [ 4.42863172e-02 -1.17267170e-02]
 [-1.17267170e-02  1.59653224e-02]
 [ 1.59653224e-02 -1.84547145e-02]
 [-1.84547145e-02  3.46838988e-03]
 [ 3.46838988e-03  2.81936384e-02]
 [ 2.81936384e-02  3.64764313e-03]
 [ 3.64764313e-03 -4.44990564e-02]
 [-4.44990564e-02  4.14703951e-02]
 [ 4.14703951e-02 -2.40288893e-02]
 [-2.40288893e-02  3.45698447e-02]
 [ 3.45698447e-02 -1.88024928e-02]
 [-1.88024928e-02  1.30412739e-02]
 [ 1.30412739e-02 -2.66631120e-02]
 [-2.66631120e-02  5.36977138e-03]
 [ 5.36977138e-03 -1.77656102e-02]
 [-1.77656102e-02  1.70806191e-02]
 [ 1.70806191e-02 -1.47620088e-02]
 [-1.47620088e-02 -2.06953052e-02]
 [-2.06953052e-02 -4.39129250e-02]
 [-4.39129250e-02  5.32957117e-02]
 [ 5.32957117e-02  2.94826661e-02]
 [ 2.94826661e-02  1.85001876e-02]
 [ 1.85001876e-02 -2.60417882e-02]
 [-2.60417882e-02 -8.12410383e-03]
 [-8.12410383e-03  2.16339788e-02]
 [ 2.16339788e-02  4.02123572e-02]]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 186, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 163, in splitSet
    if True:  raise Exception('qui')
Exception: qui
>>> y1
array([-3.65065810e-02,  3.45958331e-02, -2.23615776e-02,  2.36142994e-03,
        1.71340584e-02, -1.83196715e-02, -2.39746676e-02,  1.92444973e-02,
       -2.56118605e-02,  4.06847694e-02, -3.03132400e-02, -1.09994322e-02,
        1.12136827e-02, -2.71461125e-03,  2.52704814e-02, -2.19396926e-02,
       -1.30410765e-02,  4.12333512e-02,  1.01372394e-02, -1.50912851e-02,
        2.58382884e-02, -3.21491866e-02,  1.44253842e-02, -8.96683105e-03,
       -1.66167192e-02,  5.16849368e-03,  2.66421008e-02, -1.71750988e-02,
        2.21029219e-02, -2.75007547e-02,  3.78456391e-02, -3.20190498e-02,
        5.54839650e-02, -8.78990003e-02,  1.50607730e-02, -7.34833815e-03,
        4.38917669e-02, -3.01959253e-02,  4.37471194e-02, -9.53792172e-02,
        1.07500713e-01,  2.32639756e-04, -5.48938196e-02, -2.22891735e-02,
        2.89848798e-02, -3.02065018e-02,  3.09097230e-02, -7.93143179e-03,
       -3.76671818e-02,  5.66677303e-02,  2.00807058e-02, -2.39229466e-02,
       -1.25264233e-02,  8.91306269e-03, -4.87007880e-02,  4.55737191e-02,
        7.62920726e-03, -2.16804780e-02,  2.12126197e-02,  5.73049624e-01,
       -5.81541900e-01,  7.94072719e-03,  2.35129353e-02, -2.55910996e-02,
        1.91061612e-02, -3.87764341e-02, -5.46766131e-01,  5.47111080e-01,
        1.47864232e-02, -3.01601591e-02,  1.96767529e-02, -2.90535501e-02,
        4.42863172e-02, -1.17267170e-02,  1.59653224e-02, -1.84547145e-02,
        3.46838988e-03,  2.81936384e-02,  3.64764313e-03, -4.44990564e-02,
        4.14703951e-02, -2.40288893e-02,  3.45698447e-02, -1.88024928e-02,
        1.30412739e-02, -2.66631120e-02,  5.36977138e-03, -1.77656102e-02,
        1.70806191e-02, -1.47620088e-02, -2.06953052e-02, -4.39129250e-02,
        5.32957117e-02,  2.94826661e-02,  1.85001876e-02, -2.60417882e-02,
       -8.12410383e-03,  2.16339788e-02,  4.02123572e-02])
>>> 
>>> 
>>> y
array([4.9340242 , 4.94662997, 4.93970603, ..., 4.87694517, 4.88771409,
       4.88351191])
>>> [[ 0.          0.01260577]
 [ 0.01260577 -0.00692394]
 [-0.00692394  0.01158359]
 ...
 [-0.00221426  0.00559133]
 [ 0.00559133 -0.00611304]
 [-0.00611304 -0.00851626]]
[[-0.00851626 -0.01631661]
 [-0.01631661 -0.00846154]
 [-0.00846154 -0.00229559]
 [-0.00229559  0.        ]
 [ 0.          0.00040549]
 [ 0.00040549  0.01741504]
 [ 0.01741504  0.01253728]
 [ 0.01253728  0.01484016]
 [ 0.01484016  0.0061832 ]
 [ 0.0061832   0.0058899 ]
 [ 0.0058899   0.0010208 ]
 [ 0.0010208  -0.00909621]
 [-0.00909621 -0.02251673]
 [-0.02251673 -0.00092184]
 [-0.00092184  0.00904984]
 [ 0.00904984 -0.00261472]
 [-0.00261472 -0.00327805]
 [-0.00327805 -0.0035524 ]
 [-0.0035524   0.01556299]
 [ 0.01556299  0.00801348]
 [ 0.00801348  0.02905374]
 [ 0.02905374 -0.00451185]
 [-0.00451185  0.01743289]
 [ 0.01743289  0.00443405]
 [ 0.00443405  0.00257748]
 [ 0.00257748  0.00842239]
 [ 0.00842239 -0.00621839]
 [-0.00621839 -0.03687513]
 [-0.03687513 -0.01367171]
 [-0.01367171 -0.01699684]
 [-0.01699684  0.01287153]
 [ 0.01287153  0.00720817]
 [ 0.00720817 -0.00295421]
 [-0.00295421  0.02990593]
 [ 0.02990593 -0.00137423]
 [-0.00137423  0.01464409]
 [ 0.01464409  0.00430267]
 [ 0.00430267 -0.01495424]
 [-0.01495424 -0.00862883]
 [-0.00862883 -0.01124104]
 [-0.01124104 -0.02377851]
 [-0.02377851  0.01446306]
 [ 0.01446306  0.0128656 ]
 [ 0.0128656  -0.0085161 ]
 [-0.0085161   0.00560083]
 [ 0.00560083 -0.00012694]
 [-0.00012694 -0.00165174]
 [-0.00165174 -0.0188697 ]
 [-0.0188697  -0.00298488]
 [-0.00298488 -0.02234289]
 [-0.02234289  0.01947943]
 [ 0.01947943  0.00908271]
 [ 0.00908271  0.04779325]
 [ 0.04779325 -0.00481393]
 [-0.00481393  0.00555249]
 [ 0.00555249 -0.00939554]
 [-0.00939554  0.00643806]
 [ 0.00643806 -0.00482465]
 [-0.00482465 -0.02613214]
 [-0.02613214 -0.03550055]
 [-0.03550055  0.00355474]
 [ 0.00355474 -0.00157832]
 [-0.00157832  0.00381003]
 [ 0.00381003  0.02524896]
 [ 0.02524896 -0.01988702]
 [-0.01988702  0.01192806]
 [ 0.01192806 -0.01480162]
 [-0.01480162 -0.00288185]
 [-0.00288185  0.00992046]
 [ 0.00992046  0.02082337]
 [ 0.02082337 -0.03442369]
 [-0.03442369  0.03315078]
 [ 0.03315078 -0.00344498]
 [-0.00344498  0.00408164]
 [ 0.00408164 -0.00127372]
 [-0.00127372  0.00951361]
 [ 0.00951361 -0.0201486 ]
 [-0.0201486   0.00859365]
 [ 0.00859365 -0.02325686]
 [-0.02325686  0.03278982]
 [ 0.03278982 -0.00367531]
 [-0.00367531  0.00468682]
 [ 0.00468682 -0.00633876]
 [-0.00633876 -0.00216437]
 [-0.00216437 -0.01282727]
 [-0.01282727 -0.00426936]
 [-0.00426936 -0.02733775]
 [-0.02733775  0.01073498]
 [ 0.01073498 -0.00105513]
 [-0.00105513 -0.01141353]
 [-0.01141353  0.00386332]
 [ 0.00386332 -0.00026596]
 [-0.00026596  0.00318683]
 [ 0.00318683 -0.01052852]
 [-0.01052852 -0.01620781]
 [-0.01620781 -0.00738011]
 [-0.00738011  0.01443577]
 [ 0.01443577 -0.00639774]
 [-0.00639774  0.01076892]
 [ 0.01076892 -0.00420218]]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 186, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 163, in splitSet
    if True:  raise Exception('qui')
Exception: qui
>>> [[[-0.00173707]]

 [[ 0.0218267 ]]

 [[-0.01467988]]

 ...

 [[-0.00587616]]

 [[ 0.00871471]]

 [[-0.01316409]]] [ 0.0218267  -0.01467988  0.01991595 ...  0.00871471 -0.01316409
 -0.01765639]
  C-c C-cTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    tK.lstmBatch(1, 4)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 187, in train
    self.model.reset_states()
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 611, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 1661, in _filtered_call
    return self._call_flat(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 1745, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 593, in call
    outputs = execute.execute(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
KeyboardInterrupt
>>> [[[-1.64602551e-02]]

 [[ 2.48031133e-02]]

 [[-3.91248752e-02]]

 [[ 2.14571338e-02]]

 [[-1.77010299e-02]]

 [[-1.35658449e-02]]

 [[ 1.64382218e-02]]

 [[-1.56420146e-02]]

 [[-5.76249184e-02]]

 [[-2.39251862e-02]]

 [[-6.87750375e-02]]

 [[ 2.46952718e-03]]

 [[-5.06130788e-02]]

 [[-6.98745805e-02]]

 [[-5.02378969e-02]]

 [[-5.49915504e-02]]

 [[-1.07395006e-02]]

 [[-4.91588863e-02]]

 [[-7.19955849e-02]]

 [[ 2.09621006e-04]]

 [[ 1.79613060e-02]]

 [[-8.46558678e-03]]

 [[ 3.67807704e-02]]

 [[-1.95168279e-02]]

 [[ 5.74398188e-03]]

 [[-9.95815886e-03]]

 [[-3.90562941e-02]]

 [[-3.00055587e-02]]

 [[ 1.66483833e-02]]

 [[-1.34275507e-02]]

 [[ 2.52776717e-02]]

 [[-2.28798902e-02]]

 [[ 4.33929712e-02]]

 [[-1.26767398e-02]]

 [[ 8.44832292e-02]]

 [[-6.94398752e-02]]

 [[-4.30664133e-02]]

 [[-5.59343529e-02]]

 [[ 2.09261005e-02]]

 [[-3.19510716e-02]]

 [[ 4.46560844e-02]]

 [[-1.22365897e-01]]

 [[ 6.58824716e-02]]

 [[ 6.62898555e-02]]

 [[-2.98366887e-02]]

 [[-6.88680629e-02]]

 [[-1.81115962e-02]]

 [[-7.10072893e-02]]

 [[-1.68801603e-02]]

 [[-3.07691761e-02]]

 [[-9.67295344e-02]]

 [[ 2.50336863e-03]]

 [[ 3.76674157e-02]]

 [[-4.22491752e-03]]

 [[-2.61603885e-02]]

 [[-1.05524035e-02]]

 [[-9.58341069e-02]]

 [[-1.60283265e-02]]

 [[-2.66854692e-03]]

 [[-4.06340125e-02]]

 [[-3.48783034e-03]]

 [[ 1.00000000e+00]]

 [[-1.83589621e-02]]

 [[-4.45366882e-03]]

 [[ 3.67206788e-02]]

 [[-8.09281721e-03]]

 [[ 2.53646696e-02]]

 [[-4.25381404e-02]]

 [[-1.00000000e+00]]

 [[-4.19340870e-02]]

 [[-1.60410490e-02]]

 [[-6.88555896e-02]]

 [[-3.43989191e-02]]

 [[-8.52756368e-02]]

 [[-7.72427206e-03]]

 [[-2.82593485e-02]]

 [[-3.01897543e-04]]

 [[-3.26186126e-02]]

 [[-2.65449902e-02]]

 [[ 2.28259053e-02]]

 [[ 2.92134245e-02]]

 [[-4.87104754e-02]]

 [[ 2.39098267e-02]]

 [[-1.81680265e-02]]

 [[ 4.23684734e-02]]

 [[ 9.44275121e-03]]

 [[ 3.22797956e-02]]

 [[-1.44109400e-02]]

 [[-5.00773996e-03]]

 [[-3.61177397e-02]]

 [[-6.20725260e-03]]

 [[-3.20575377e-02]]

 [[-6.82978318e-02]]

 [[-1.45195336e-01]]

 [[-5.18672961e-02]]

 [[-2.39137980e-04]]

 [[ 3.21572067e-02]]

 [[-1.34455063e-02]]

 [[-2.76719171e-02]]

 [[ 1.02121221e-02]]] [ 2.48031133e-02 -3.91248752e-02  2.14571338e-02 -1.77010299e-02
 -1.35658449e-02  1.64382218e-02 -1.56420146e-02 -5.76249184e-02
 -2.39251862e-02 -6.87750375e-02  2.46952718e-03 -5.06130788e-02
 -6.98745805e-02 -5.02378969e-02 -5.49915504e-02 -1.07395006e-02
 -4.91588863e-02 -7.19955849e-02  2.09621006e-04  1.79613060e-02
 -8.46558678e-03  3.67807704e-02 -1.95168279e-02  5.74398188e-03
 -9.95815886e-03 -3.90562941e-02 -3.00055587e-02  1.66483833e-02
 -1.34275507e-02  2.52776717e-02 -2.28798902e-02  4.33929712e-02
 -1.26767398e-02  8.44832292e-02 -6.94398752e-02 -4.30664133e-02
 -5.59343529e-02  2.09261005e-02 -3.19510716e-02  4.46560844e-02
 -1.22365897e-01  6.58824716e-02  6.62898555e-02 -2.98366887e-02
 -6.88680629e-02 -1.81115962e-02 -7.10072893e-02 -1.68801603e-02
 -3.07691761e-02 -9.67295344e-02  2.50336863e-03  3.76674157e-02
 -4.22491752e-03 -2.61603885e-02 -1.05524035e-02 -9.58341069e-02
 -1.60283265e-02 -2.66854692e-03 -4.06340125e-02 -3.48783034e-03
  1.00000000e+00 -1.83589621e-02 -4.45366882e-03  3.67206788e-02
 -8.09281721e-03  2.53646696e-02 -4.25381404e-02 -1.00000000e+00
 -4.19340870e-02 -1.60410490e-02 -6.88555896e-02 -3.43989191e-02
 -8.52756368e-02 -7.72427206e-03 -2.82593485e-02 -3.01897543e-04
 -3.26186126e-02 -2.65449902e-02  2.28259053e-02  2.92134245e-02
 -4.87104754e-02  2.39098267e-02 -1.81680265e-02  4.23684734e-02
  9.44275121e-03  3.22797956e-02 -1.44109400e-02 -5.00773996e-03
 -3.61177397e-02 -6.20725260e-03 -3.20575377e-02 -6.82978318e-02
 -1.45195336e-01 -5.18672961e-02 -2.39137980e-04  3.21572067e-02
 -1.34455063e-02 -2.76719171e-02  1.02121221e-02  8.06294288e-02]
[[[-1.64602551e-02]]

 [[ 2.48031133e-02]]

 [[-3.91248752e-02]]

 [[ 2.14571338e-02]]

 [[-1.77010299e-02]]

 [[-1.35658449e-02]]

 [[ 1.64382218e-02]]

 [[-1.56420146e-02]]

 [[-5.76249184e-02]]

 [[-2.39251862e-02]]

 [[-6.87750375e-02]]

 [[ 2.46952718e-03]]

 [[-5.06130788e-02]]

 [[-6.98745805e-02]]

 [[-5.02378969e-02]]

 [[-5.49915504e-02]]

 [[-1.07395006e-02]]

 [[-4.91588863e-02]]

 [[-7.19955849e-02]]

 [[ 2.09621006e-04]]

 [[ 1.79613060e-02]]

 [[-8.46558678e-03]]

 [[ 3.67807704e-02]]

 [[-1.95168279e-02]]

 [[ 5.74398188e-03]]

 [[-9.95815886e-03]]

 [[-3.90562941e-02]]

 [[-3.00055587e-02]]

 [[ 1.66483833e-02]]

 [[-1.34275507e-02]]

 [[ 2.52776717e-02]]

 [[-2.28798902e-02]]

 [[ 4.33929712e-02]]

 [[-1.26767398e-02]]

 [[ 8.44832292e-02]]

 [[-6.94398752e-02]]

 [[-4.30664133e-02]]

 [[-5.59343529e-02]]

 [[ 2.09261005e-02]]

 [[-3.19510716e-02]]

 [[ 4.46560844e-02]]

 [[-1.22365897e-01]]

 [[ 6.58824716e-02]]

 [[ 6.62898555e-02]]

 [[-2.98366887e-02]]

 [[-6.88680629e-02]]

 [[-1.81115962e-02]]

 [[-7.10072893e-02]]

 [[-1.68801603e-02]]

 [[-3.07691761e-02]]

 [[-9.67295344e-02]]

 [[ 2.50336863e-03]]

 [[ 3.76674157e-02]]

 [[-4.22491752e-03]]

 [[-2.61603885e-02]]

 [[-1.05524035e-02]]

 [[-9.58341069e-02]]

 [[-1.60283265e-02]]

 [[-2.66854692e-03]]

 [[-4.06340125e-02]]

 [[-3.48783034e-03]]

 [[ 1.00000000e+00]]

 [[-1.83589621e-02]]

 [[-4.45366882e-03]]

 [[ 3.67206788e-02]]

 [[-8.09281721e-03]]

 [[ 2.53646696e-02]]

 [[-4.25381404e-02]]

 [[-1.00000000e+00]]

 [[-4.19340870e-02]]

 [[-1.60410490e-02]]

 [[-6.88555896e-02]]

 [[-3.43989191e-02]]

 [[-8.52756368e-02]]

 [[-7.72427206e-03]]

 [[-2.82593485e-02]]

 [[-3.01897543e-04]]

 [[-3.26186126e-02]]

 [[-2.65449902e-02]]

 [[ 2.28259053e-02]]

 [[ 2.92134245e-02]]

 [[-4.87104754e-02]]

 [[ 2.39098267e-02]]

 [[-1.81680265e-02]]

 [[ 4.23684734e-02]]

 [[ 9.44275121e-03]]

 [[ 3.22797956e-02]]

 [[-1.44109400e-02]]

 [[-5.00773996e-03]]

 [[-3.61177397e-02]]

 [[-6.20725260e-03]]

 [[-3.20575377e-02]]

 [[-6.82978318e-02]]

 [[-1.45195336e-01]]

 [[-5.18672961e-02]]

 [[-2.39137980e-04]]

 [[ 3.21572067e-02]]

 [[-1.34455063e-02]]

 [[-2.76719171e-02]]

 [[ 1.02121221e-02]]] [ 2.48031133e-02 -3.91248752e-02  2.14571338e-02 -1.77010299e-02
 -1.35658449e-02  1.64382218e-02 -1.56420146e-02 -5.76249184e-02
 -2.39251862e-02 -6.87750375e-02  2.46952718e-03 -5.06130788e-02
 -6.98745805e-02 -5.02378969e-02 -5.49915504e-02 -1.07395006e-02
 -4.91588863e-02 -7.19955849e-02  2.09621006e-04  1.79613060e-02
 -8.46558678e-03  3.67807704e-02 -1.95168279e-02  5.74398188e-03
 -9.95815886e-03 -3.90562941e-02 -3.00055587e-02  1.66483833e-02
 -1.34275507e-02  2.52776717e-02 -2.28798902e-02  4.33929712e-02
 -1.26767398e-02  8.44832292e-02 -6.94398752e-02 -4.30664133e-02
 -5.59343529e-02  2.09261005e-02 -3.19510716e-02  4.46560844e-02
 -1.22365897e-01  6.58824716e-02  6.62898555e-02 -2.98366887e-02
 -6.88680629e-02 -1.81115962e-02 -7.10072893e-02 -1.68801603e-02
 -3.07691761e-02 -9.67295344e-02  2.50336863e-03  3.76674157e-02
 -4.22491752e-03 -2.61603885e-02 -1.05524035e-02 -9.58341069e-02
 -1.60283265e-02 -2.66854692e-03 -4.06340125e-02 -3.48783034e-03
  1.00000000e+00 -1.83589621e-02 -4.45366882e-03  3.67206788e-02
 -8.09281721e-03  2.53646696e-02 -4.25381404e-02 -1.00000000e+00
 -4.19340870e-02 -1.60410490e-02 -6.88555896e-02 -3.43989191e-02
 -8.52756368e-02 -7.72427206e-03 -2.82593485e-02 -3.01897543e-04
 -3.26186126e-02 -2.65449902e-02  2.28259053e-02  2.92134245e-02
 -4.87104754e-02  2.39098267e-02 -1.81680265e-02  4.23684734e-02
  9.44275121e-03  3.22797956e-02 -1.44109400e-02 -5.00773996e-03
 -3.61177397e-02 -6.20725260e-03 -3.20575377e-02 -6.82978318e-02
 -1.45195336e-01 -5.18672961e-02 -2.39137980e-04  3.21572067e-02
 -1.34455063e-02 -2.76719171e-02  1.02121221e-02  8.06294288e-02]
>>> 
>>>   C-c C-cTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 116, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 186, in train
    self.model.fit(X_train,y_train,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 611, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 1661, in _filtered_call
    return self._call_flat(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 1745, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 593, in call
    outputs = execute.execute(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
KeyboardInterrupt
>>> 
>>> 
>>> 
>>> 0
Month=1, Predicted=4.160469, Expected=-0.660686
1
Month=2, Predicted=4.152455, Expected=-0.682432
2
Month=3, Predicted=4.166363, Expected=-0.695409
3
Month=4, Predicted=4.249027, Expected=-0.709939
4
Month=5, Predicted=4.275915, Expected=-0.690953
5
Month=6, Predicted=4.337888, Expected=-0.647497
6
Month=7, Predicted=4.350628, Expected=-0.675780
7
Month=8, Predicted=4.325967, Expected=-0.661788
8
Month=9, Predicted=4.304536, Expected=-0.655916
9
Month=10, Predicted=4.278899, Expected=-0.681322
10
Month=11, Predicted=4.248639, Expected=-0.669880
11
Month=12, Predicted=4.253304, Expected=-0.718542
12
Month=13, Predicted=4.166205, Expected=-0.740351
13
Month=14, Predicted=4.137912, Expected=-0.766131
14
Month=15, Predicted=4.124310, Expected=-0.791758
15
Month=16, Predicted=4.185109, Expected=-0.763466
16
Month=17, Predicted=4.272682, Expected=-0.729802
17
Month=18, Predicted=4.279744, Expected=-0.755108
18
Month=19, Predicted=4.271996, Expected=-0.741106
19
Month=20, Predicted=4.280582, Expected=-0.734695
20
Month=21, Predicted=4.335301, Expected=-0.743750
21
Month=22, Predicted=4.391764, Expected=-0.750938
22
Month=23, Predicted=4.342825, Expected=-0.782552
23
Month=24, Predicted=4.351243, Expected=-0.777578
24
Month=25, Predicted=4.281459, Expected=-0.670986
25
Month=26, Predicted=4.267874, Expected=-0.686134
26
Month=27, Predicted=4.285563, Expected=-0.724167
27
Month=28, Predicted=4.280867, Expected=-0.691324
28
Month=29, Predicted=4.210402, Expected=-0.684652
29
Month=30, Predicted=4.190693, Expected=-0.683542
30
Month=31, Predicted=4.124533, Expected=-0.728298
31
Month=32, Predicted=4.176355, Expected=-0.852321
32
Month=33, Predicted=4.187390, Expected=-0.845659
33
Month=34, Predicted=4.210151, Expected=-0.855854
34
Month=35, Predicted=4.256369, Expected=-0.880706
35
Month=36, Predicted=4.326737, Expected=-0.862932
36
Month=37, Predicted=4.372000, Expected=-0.855068
37
Month=38, Predicted=4.378013, Expected=-0.870818
38
Month=39, Predicted=4.346586, Expected=-0.816451
39
Month=40, Predicted=4.333833, Expected=-0.781020
40
Month=41, Predicted=4.316477, Expected=-0.744506
41
Month=42, Predicted=4.229067, Expected=-0.639102
42
Month=43, Predicted=4.213526, Expected=-0.657750
43
Month=44, Predicted=4.192450, Expected=-0.600724
44
Month=45, Predicted=4.158878, Expected=-0.559654
45
Month=46, Predicted=4.131913, Expected=-0.581009
46
Month=47, Predicted=4.124124, Expected=-0.585657
47
Month=48, Predicted=4.089102, Expected=-0.609369
48
Month=49, Predicted=4.073305, Expected=-0.750560
49
Month=50, Predicted=3.998493, Expected=-0.782552
50
Month=51, Predicted=3.972512, Expected=-0.852713
51
Month=52, Predicted=4.020248, Expected=-0.886257
52
Month=53, Predicted=4.043481, Expected=-0.897393
53
Month=54, Predicted=4.064683, Expected=-0.932672
54
Month=55, Predicted=4.061981, Expected=-0.912574
55
Month=56, Predicted=4.097406, Expected=-0.818389
56
Month=57, Predicted=4.135943, Expected=-0.772230
57
Month=58, Predicted=4.205411, Expected=-0.755488
58
Month=59, Predicted=4.172964, Expected=-0.726795
59
Month=60, Predicted=4.177909, Expected=-0.724917
60
Month=61, Predicted=4.185938, Expected=-0.699128
61
Month=62, Predicted=4.201353, Expected=-0.680583
62
Month=63, Predicted=4.188438, Expected=-0.745262
63
Month=64, Predicted=4.202095, Expected=-0.768417
64
Month=65, Predicted=4.148890, Expected=-0.774903
65
Month=66, Predicted=4.143894, Expected=-0.781403
66
Month=67, Predicted=4.136027, Expected=-0.741861
67
Month=68, Predicted=4.118221, Expected=-0.735448
68
Month=69, Predicted=4.102827, Expected=-0.746396
69
Month=70, Predicted=4.105757, Expected=-0.717419
70
Month=71, Predicted=4.105834, Expected=-0.721540
71
Month=72, Predicted=4.105704, Expected=-0.698384
72
Month=73, Predicted=4.092581, Expected=-0.695038
73
Month=74, Predicted=4.091834, Expected=-0.717419
74
Month=75, Predicted=4.121406, Expected=-0.714050
75
Month=76, Predicted=4.194777, Expected=-0.761184
76
Month=77, Predicted=4.184343, Expected=-0.782552
77
Month=78, Predicted=4.138799, Expected=-0.778725
78
Month=79, Predicted=4.174341, Expected=-0.805236
79
Month=80, Predicted=4.201544, Expected=-0.781403
80
Month=81, Predicted=4.203699, Expected=-0.787535
81
Month=82, Predicted=4.203706, Expected=-0.825766
82
Month=83, Predicted=4.160569, Expected=-0.820329
83
Month=84, Predicted=4.200806, Expected=-0.795987
84
Month=85, Predicted=4.241606, Expected=-0.742239
85
Month=86, Predicted=4.236917, Expected=-0.745640
86
Month=87, Predicted=4.261107, Expected=-0.685023
87
Month=88, Predicted=4.262078, Expected=-0.676518
88
Month=89, Predicted=4.245292, Expected=-0.657383
89
Month=90, Predicted=4.237094, Expected=-0.642749
90
Month=91, Predicted=4.211120, Expected=-0.698384
91
Month=92, Predicted=4.217141, Expected=-0.761945
92
Month=93, Predicted=4.222850, Expected=-0.752833
93
Month=94, Predicted=4.196854, Expected=-0.840575
94
Month=95, Predicted=4.185801, Expected=-0.855854
95
Month=96, Predicted=4.193491, Expected=-0.846442
96
Month=97, Predicted=4.174569, Expected=-0.842529
97
Month=98, Predicted=4.172303, Expected=-0.771086
98
Month=99, Predicted=4.185465, Expected=-0.762706
99
Month=100, Predicted=4.171439, Expected=-0.727922
>>> 0
Month=1, Predicted=0.036903, Expected=-0.015771
1
Month=2, Predicted=0.045697, Expected=-0.010113
2
Month=3, Predicted=0.015232, Expected=-0.011114
3
Month=4, Predicted=0.011779, Expected=0.010516
4
Month=5, Predicted=-0.013210, Expected=0.026309
5
Month=6, Predicted=0.008099, Expected=-0.019990
6
Month=7, Predicted=0.022939, Expected=0.007293
7
Month=8, Predicted=-0.032875, Expected=0.002052
8
Month=9, Predicted=-0.023551, Expected=-0.018133
9
Month=10, Predicted=-0.015714, Expected=0.005647
10
Month=11, Predicted=-0.011374, Expected=-0.033142
11
Month=12, Predicted=0.002008, Expected=-0.015812
12
Month=13, Predicted=-0.004195, Expected=-0.018375
13
Month=14, Predicted=-0.042898, Expected=-0.018276
14
Month=15, Predicted=0.018475, Expected=0.016522
15
Month=16, Predicted=-0.002238, Expected=0.019989
16
Month=17, Predicted=-0.016275, Expected=-0.018069
17
Month=18, Predicted=-0.000909, Expected=0.007300
18
Month=19, Predicted=-0.000473, Expected=0.002400
19
Month=20, Predicted=-0.002490, Expected=-0.007581
20
Month=21, Predicted=-0.002275, Expected=-0.006376
21
Month=22, Predicted=-0.025095, Expected=-0.022140
22
Month=23, Predicted=-0.016427, Expected=0.001473
23
Month=24, Predicted=0.002676, Expected=0.067055
24
Month=25, Predicted=0.015808, Expected=-0.011513
25
Month=26, Predicted=0.007341, Expected=-0.026283
26
Month=27, Predicted=-0.002362, Expected=0.019458
27
Month=28, Predicted=-0.017099, Expected=0.002569
28
Month=29, Predicted=-0.008980, Expected=-0.001020
29
Month=30, Predicted=0.000384, Expected=-0.030622
30
Month=31, Predicted=0.031193, Expected=-0.081778
31
Month=32, Predicted=-0.031260, Expected=0.002562
32
Month=33, Predicted=-0.019521, Expected=-0.008316
33
Month=34, Predicted=-0.017412, Expected=-0.017776
34
Month=35, Predicted=0.007386, Expected=0.009733
35
Month=36, Predicted=0.023154, Expected=0.003338
36
Month=37, Predicted=0.009445, Expected=-0.011902
37
Month=38, Predicted=-0.026016, Expected=0.033350
38
Month=39, Predicted=-0.003501, Expected=0.021129
39
Month=40, Predicted=-0.004452, Expected=0.021829
40
Month=41, Predicted=-0.000208, Expected=0.066288
41
Month=42, Predicted=0.021625, Expected=-0.013772
42
Month=43, Predicted=-0.033277, Expected=0.035066
43
Month=44, Predicted=0.046123, Expected=0.024768
44
Month=45, Predicted=-0.006185, Expected=-0.015519
45
Month=46, Predicted=-0.008472, Expected=-0.004737
46
Month=47, Predicted=0.009848, Expected=-0.017040
47
Month=48, Predicted=0.002381, Expected=-0.092858
48
Month=49, Predicted=-0.015680, Expected=-0.022384
49
Month=50, Predicted=0.007775, Expected=-0.047017
50
Month=51, Predicted=-0.064608, Expected=-0.023386
51
Month=52, Predicted=0.049961, Expected=-0.008924
52
Month=53, Predicted=0.023308, Expected=-0.024505
53
Month=54, Predicted=0.031020, Expected=0.011233
54
Month=55, Predicted=0.025604, Expected=0.059048
55
Month=56, Predicted=-0.004935, Expected=0.028053
56
Month=57, Predicted=-0.005788, Expected=0.009068
57
Month=58, Predicted=0.024060, Expected=0.016780
58
Month=59, Predicted=-0.009633, Expected=-0.000525
59
Month=60, Predicted=0.023129, Expected=0.014907
60
Month=61, Predicted=-0.043864, Expected=0.010232
61
Month=62, Predicted=-0.000916, Expected=-0.043480
62
Month=63, Predicted=-0.008512, Expected=-0.016681
63
Month=64, Predicted=0.017256, Expected=-0.005923
64
Month=65, Predicted=0.027173, Expected=-0.005932
65
Month=66, Predicted=-0.047337, Expected=0.023783
66
Month=67, Predicted=-0.006850, Expected=0.002401
67
Month=68, Predicted=0.041590, Expected=-0.008802
68
Month=69, Predicted=0.016060, Expected=0.016964
69
Month=70, Predicted=0.043297, Expected=-0.004397
70
Month=71, Predicted=-0.015329, Expected=0.013208
71
Month=72, Predicted=-0.000895, Expected=0.000422
72
Month=73, Predicted=0.013393, Expected=-0.016181
73
Month=74, Predicted=-0.016000, Expected=0.000437
74
Month=75, Predicted=-0.003365, Expected=-0.032156
75
Month=76, Predicted=-0.001573, Expected=-0.015527
76
Month=77, Predicted=-0.016193, Expected=0.000733
77
Month=78, Predicted=0.015421, Expected=-0.018847
78
Month=79, Predicted=-0.032450, Expected=0.013644
79
Month=80, Predicted=0.000667, Expected=-0.005694
80
Month=81, Predicted=0.003769, Expected=-0.026410
81
Month=82, Predicted=-0.009236, Expected=0.001772
82
Month=83, Predicted=-0.039170, Expected=0.013972
83
Month=84, Predicted=-0.013230, Expected=0.032951
84
Month=85, Predicted=0.000939, Expected=-0.003932
85
Month=86, Predicted=0.038097, Expected=0.037384
86
Month=87, Predicted=0.077971, Expected=0.003751
87
Month=88, Predicted=0.090680, Expected=0.010612
88
Month=89, Predicted=0.072369, Expected=0.007707
89
Month=90, Predicted=-0.008491, Expected=-0.037643
90
Month=91, Predicted=-0.003575, Expected=-0.042758
91
Month=92, Predicted=0.005213, Expected=0.004143
92
Month=93, Predicted=-0.028508, Expected=-0.058364
93
Month=94, Predicted=-0.063736, Expected=-0.011597
94
Month=95, Predicted=-0.084021, Expected=0.004337
95
Month=96, Predicted=-0.100069, Expected=0.000788
96
Month=97, Predicted=0.026365, Expected=0.044371
97
Month=98, Predicted=-0.026773, Expected=0.003671
98
Month=99, Predicted=-0.009418, Expected=0.020711
99
Month=100, Predicted=-0.013146, Expected=-0.017325
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 113, in <module>
    X1 = np.array(y1.reshape(-1,1))
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py", line 5274, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'Series' object has no attribute 'reshape'
>>> y1
0       -0.036507
1        0.034596
2       -0.022362
3        0.002361
4        0.017134
           ...   
29339    0.127438
29340   -0.053850
29341    0.015444
29342   -0.002244
29343    0.015958
Length: 29344, dtype: float64
>>> 


  C-c C-cTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 117, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=2)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 186, in train
    self.model.fit(X_train,y_train,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 611, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 1661, in _filtered_call
    return self._call_flat(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 1745, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 593, in call
    outputs = execute.execute(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
KeyboardInterrupt
>>> 


>>> 0
Month=1, Predicted=-0.009599, Expected=0.009217
1
Month=2, Predicted=-0.047073, Expected=0.002539
2
Month=3, Predicted=0.017467, Expected=0.025231
3
Month=4, Predicted=-0.041719, Expected=0.019378
4
Month=5, Predicted=-0.013985, Expected=-0.042879
5
Month=6, Predicted=0.032576, Expected=0.030899
6
Month=7, Predicted=-0.073649, Expected=-0.001712
7
Month=8, Predicted=-0.013151, Expected=-0.016696
8
Month=9, Predicted=0.005956, Expected=0.027387
9
Month=10, Predicted=-0.042907, Expected=-0.035350
10
Month=11, Predicted=0.038560, Expected=0.020919
11
Month=12, Predicted=-0.025788, Expected=0.000973
12
Month=13, Predicted=-0.073071, Expected=0.003642
13
Month=14, Predicted=0.028223, Expected=0.038434
14
Month=15, Predicted=-0.011499, Expected=0.007020
15
Month=16, Predicted=0.016924, Expected=-0.034617
16
Month=17, Predicted=-0.026519, Expected=0.028980
17
Month=18, Predicted=-0.000873, Expected=-0.001369
18
Month=19, Predicted=-0.030671, Expected=-0.006465
19
Month=20, Predicted=0.000268, Expected=0.004751
20
Month=21, Predicted=-0.010470, Expected=-0.012262
21
Month=22, Predicted=-0.021518, Expected=0.027219
22
Month=23, Predicted=-0.031875, Expected=0.069299
23
Month=24, Predicted=-0.004281, Expected=-0.075234
24
Month=25, Predicted=-0.064261, Expected=-0.011266
25
Month=26, Predicted=-0.023691, Expected=0.049406
26
Month=27, Predicted=0.002921, Expected=-0.013391
27
Month=28, Predicted=-0.019522, Expected=-0.000056
28
Month=29, Predicted=0.025577, Expected=-0.026137
29
Month=30, Predicted=-0.016668, Expected=-0.047750
30
Month=31, Predicted=-0.082019, Expected=0.088108
31
Month=32, Predicted=0.013924, Expected=-0.007364
32
Month=33, Predicted=-0.000934, Expected=-0.005942
33
Month=34, Predicted=0.012265, Expected=0.031126
34
Month=35, Predicted=-0.050669, Expected=-0.002869
35
Month=36, Predicted=0.006010, Expected=-0.011738
36
Month=37, Predicted=-0.007258, Expected=0.048915
37
Month=38, Predicted=-0.047843, Expected=-0.008710
38
Month=39, Predicted=-0.014556, Expected=0.004245
39
Month=40, Predicted=-0.006553, Expected=0.048121
40
Month=41, Predicted=-0.049864, Expected=-0.076730
41
Month=42, Predicted=-0.010668, Expected=0.052511
42
Month=43, Predicted=-0.033861, Expected=-0.006782
43
Month=44, Predicted=-0.092939, Expected=-0.036851
44
Month=45, Predicted=0.085598, Expected=0.014354
45
Month=46, Predicted=-0.053572, Expected=-0.008793
46
Month=47, Predicted=0.018943, Expected=-0.072477
47
Month=48, Predicted=-0.038016, Expected=0.074205
48
Month=49, Predicted=-0.016321, Expected=-0.021156
49
Month=50, Predicted=0.011267, Expected=0.027238
50
Month=51, Predicted=0.019834, Expected=0.018044
51
Month=52, Predicted=-0.045214, Expected=-0.012080
52
Month=53, Predicted=-0.007075, Expected=0.039377
53
Month=54, Predicted=-0.062762, Expected=0.051485
54
Month=55, Predicted=-0.020649, Expected=-0.027534
55
Month=56, Predicted=0.037896, Expected=-0.015492
56
Month=57, Predicted=-0.023057, Expected=0.011276
57
Month=58, Predicted=-0.013043, Expected=-0.013809
58
Month=59, Predicted=-0.054471, Expected=0.019016
59
Month=60, Predicted=0.012368, Expected=-0.001144
60
Month=61, Predicted=-0.006389, Expected=-0.050311
61
Month=62, Predicted=0.021302, Expected=0.030413
62
Month=63, Predicted=-0.042202, Expected=0.014329
63
Month=64, Predicted=-0.056879, Expected=0.003534
64
Month=65, Predicted=-0.069678, Expected=0.033337
65
Month=66, Predicted=0.093494, Expected=-0.017895
66
Month=67, Predicted=-0.026579, Expected=-0.007691
67
Month=68, Predicted=-0.072495, Expected=0.029379
68
Month=69, Predicted=-0.075703, Expected=-0.017875
69
Month=70, Predicted=0.011184, Expected=0.021195
70
Month=71, Predicted=-0.001684, Expected=-0.009276
71
Month=72, Predicted=0.013500, Expected=-0.013105
72
Month=73, Predicted=-0.037667, Expected=0.020205
73
Month=74, Predicted=-0.038456, Expected=-0.029137
74
Month=75, Predicted=0.094861, Expected=0.020217
75
Month=76, Predicted=-0.034836, Expected=0.019846
76
Month=77, Predicted=-0.059731, Expected=-0.016089
77
Month=78, Predicted=0.003296, Expected=0.036120
78
Month=79, Predicted=0.011186, Expected=-0.015847
79
Month=80, Predicted=-0.054669, Expected=-0.017228
80
Month=81, Predicted=-0.024673, Expected=0.031800
81
Month=82, Predicted=-0.085038, Expected=0.015776
82
Month=83, Predicted=0.079281, Expected=0.022572
83
Month=84, Predicted=0.040493, Expected=-0.033438
84
Month=85, Predicted=-0.105265, Expected=0.044969
85
Month=86, Predicted=0.005975, Expected=-0.030179
86
Month=87, Predicted=-0.096488, Expected=0.010422
87
Month=88, Predicted=-0.065225, Expected=0.000630
88
Month=89, Predicted=-0.037558, Expected=-0.041927
89
Month=90, Predicted=-0.023146, Expected=-0.001586
90
Month=91, Predicted=-0.028089, Expected=0.050569
91
Month=92, Predicted=-0.006990, Expected=-0.059131
92
Month=93, Predicted=-0.013242, Expected=0.050434
93
Month=94, Predicted=0.086893, Expected=0.019520
94
Month=95, Predicted=0.019981, Expected=-0.000015
95
Month=96, Predicted=0.055751, Expected=0.047242
96
Month=97, Predicted=-0.057049, Expected=-0.037265
97
Month=98, Predicted=-0.008428, Expected=0.020628
98
Month=99, Predicted=0.005952, Expected=-0.034594
99
Month=100, Predicted=0.004735, Expected=0.017946
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 185, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 167, in splitSet
    print(min(train_scaled),max(train_scaled))
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
>>> 
>>> -1.0 0.99999994
-1.0 0.99999994
-1.0 0.99999994
-1.0 0.99999994
>>> -1.0 0.99999994
-1.0 0.99999994
>>> X
array([[[ 0.00354308]],

       [[-0.03306066]],

       [[ 0.038231  ]],

       ...,

       [[ 0.0015073 ]],

       [[ 0.02952717]],

       [[ 0.02431374]]])
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/deep_lernia/deep_lernia/train_longShort.py", line 125, in <module>
    df = pd.DataFrame(X)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 464, in __init__
    mgr = init_ndarray(data, index, columns, dtype=dtype, copy=copy)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/construction.py", line 169, in init_ndarray
    values = prep_ndarray(values, copy=copy)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/construction.py", line 295, in prep_ndarray
    raise ValueError("Must pass 2-d input")
ValueError: Must pass 2-d input
>>> X.shape
(29244, 1, 1)
>>> X1.shape
(200, 1)
>>> X = X1
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/deep_lernia/deep_lernia/train_longShort.py", line 126, in <module>
    columns = [df.shift(i) for i in range(1, n_in+1)]
NameError: name 'n_in' is not defined
>>> n_in = 1
>>> df
            0         0
0    0.000000  0.071292
1    0.071292 -0.057109
2   -0.057109  0.024789
3    0.024789  0.014812
4    0.014812 -0.035548
..        ...       ...
195 -0.019535  0.047258
196  0.047258 -0.084508
197 -0.084508  0.057894
198  0.057894 -0.055222
199 -0.055222  0.052540

[200 rows x 2 columns]
>>> columns
[            0
0         NaN
1    0.071292
2   -0.057109
3    0.024789
4    0.014812
..        ...
195 -0.019535
196  0.047258
197 -0.084508
198  0.057894
199 -0.055222

[200 rows x 1 columns],             0
0    0.071292
1   -0.057109
2    0.024789
3    0.014812
4   -0.035548
..        ...
195  0.047258
196 -0.084508
197  0.057894
198 -0.055222
199  0.052540

[200 rows x 1 columns]]
>>> columns
[            0
0         NaN
1    0.071292
2   -0.057109
3    0.024789
4    0.014812
..        ...
195 -0.019535
196  0.047258
197 -0.084508
198  0.057894
199 -0.055222

[200 rows x 1 columns]]
>>> columns
[            0
0         NaN
1    0.071292
2   -0.057109
3    0.024789
4    0.014812
..        ...
195 -0.019535
196  0.047258
197 -0.084508
198  0.057894
199 -0.055222

[200 rows x 1 columns],             0
0    0.071292
1   -0.057109
2    0.024789
3    0.014812
4   -0.035548
..        ...
195  0.047258
196 -0.084508
197  0.057894
198 -0.055222
199  0.052540

[200 rows x 1 columns]]
>>> df
            0         0
0    0.000000  0.071292
1    0.071292 -0.057109
2   -0.057109  0.024789
3    0.024789  0.014812
4    0.014812 -0.035548
..        ...       ...
195 -0.019535  0.047258
196  0.047258 -0.084508
197 -0.084508  0.057894
198  0.057894 -0.055222
199 -0.055222  0.052540

[200 rows x 2 columns]
>>> columns
[            0
0         NaN
1    0.071292
2   -0.057109
3    0.024789
4    0.014812
..        ...
195 -0.019535
196  0.047258
197 -0.084508
198  0.057894
199 -0.055222

[200 rows x 1 columns],             0
0    0.071292
1   -0.057109
2    0.024789
3    0.014812
4   -0.035548
..        ...
195  0.047258
196 -0.084508
197  0.057894
198 -0.055222
199  0.052540

[200 rows x 1 columns]]
>>> df
            0         0
0    0.000000  0.071292
1    0.071292 -0.057109
2   -0.057109  0.024789
3    0.024789  0.014812
4    0.014812 -0.035548
..        ...       ...
195 -0.019535  0.047258
196  0.047258 -0.084508
197 -0.084508  0.057894
198  0.057894 -0.055222
199 -0.055222  0.052540

[200 rows x 2 columns]
>>> columns
            0
0    0.000000
1    0.071292
2   -0.057109
3    0.024789
4    0.014812
..        ...
195 -0.019535
196  0.047258
197 -0.084508
198  0.057894
199 -0.055222

[200 rows x 1 columns]
>>> n_in = 2
>>> columns
            0         0
0    0.000000  0.000000
1    0.071292  0.000000
2   -0.057109  0.071292
3    0.024789 -0.057109
4    0.014812  0.024789
..        ...       ...
195 -0.019535 -0.030914
196  0.047258 -0.019535
197 -0.084508  0.047258
198  0.057894 -0.084508
199 -0.055222  0.057894

[200 rows x 2 columns]
>>> pre
            0         0
0    0.000000  0.000000
1    0.071292  0.000000
2   -0.057109  0.071292
3    0.024789 -0.057109
4    0.014812  0.024789
..        ...       ...
195 -0.019535 -0.030914
196  0.047258 -0.019535
197 -0.084508  0.047258
198  0.057894 -0.084508
199 -0.055222  0.057894

[200 rows x 2 columns]
>>> post
            0
0    0.071292
1   -0.057109
2    0.024789
3    0.014812
4   -0.035548
..        ...
195  0.047258
196 -0.084508
197  0.057894
198 -0.055222
199  0.052540

[200 rows x 1 columns]
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/deep_lernia/deep_lernia/train_longShort.py", line 154, in <module>
    y = np.array(y1)[shuffleL]
NameError: name 'shuffleL' is not defined
>>> Xy
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'Xy' is not defined
>>> Xy = pd.concat([X,X])
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/reshape/concat.py", line 271, in concat
    op = _Concatenator(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/reshape/concat.py", line 357, in __init__
    raise TypeError(msg)
TypeError: cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid
>>> X
array([[ 7.12916604e-02],
       [-5.71090087e-02],
       [ 2.47888104e-02],
       [ 1.48119474e-02],
       [-3.55480936e-02],
       [-5.67004752e-03],
       [ 4.33341972e-02],
       [-4.49757476e-02],
       [ 6.64730852e-02],
       [-7.11869778e-02],
       [ 1.93652135e-02],
       [ 2.22722374e-02],
       [-1.39653656e-02],
       [ 2.80595778e-02],
       [-4.73358287e-02],
       [ 8.92230065e-03],
       [ 5.44188846e-02],
       [-3.11788772e-02],
       [-2.52956729e-02],
       [ 4.10385118e-02],
       [-5.81418146e-02],
       [ 4.66985338e-02],
       [-2.34544760e-02],
       [-7.67024906e-03],
       [ 2.18431964e-02],
       [ 2.15307613e-02],
       [-4.39338235e-02],
       [ 3.93825632e-02],
       [-4.97357019e-02],
       [ 6.55203200e-02],
       [-7.00506409e-02],
       [ 8.77359131e-02],
       [-1.43764594e-01],
       [ 1.03233811e-01],
       [-2.24687553e-02],
       [ 5.13764858e-02],
       [-7.42848841e-02],
       [ 7.41398517e-02],
       [-1.39496636e-01],
       [ 2.03419915e-01],
       [-1.07553578e-01],
       [-5.52731841e-02],
       [ 3.26914267e-02],
       [ 5.14105244e-02],
       [-5.93489256e-02],
       [ 6.12788920e-02],
       [-3.89445345e-02],
       [-2.98148947e-02],
       [ 9.45859940e-02],
       [-3.66844045e-02],
       [-4.41207727e-02],
       [ 1.14268563e-02],
       [ 2.14965494e-02],
       [-5.77671958e-02],
       [ 9.45254283e-02],
       [-3.80455050e-02],
       [-2.93876959e-02],
       [ 4.30072620e-02],
       [ 5.53305774e-01],
       [-1.15766459e+00],
       [ 5.91051596e-01],
       [ 1.56136551e-02],
       [-4.92347303e-02],
       [ 4.48162271e-02],
       [-5.80366557e-02],
       [-5.09341763e-01],
       [ 1.09678868e+00],
       [-5.33741493e-01],
       [-4.50662122e-02],
       [ 4.99695580e-02],
       [-4.88600036e-02],
       [ 7.35350688e-02],
       [-5.61621187e-02],
       [ 2.77657447e-02],
       [-3.45116494e-02],
       [ 2.19814549e-02],
       [ 2.47910573e-02],
       [-2.46113270e-02],
       [-4.82748468e-02],
       [ 8.61982680e-02],
       [-6.56736175e-02],
       [ 5.87547005e-02],
       [-5.35143934e-02],
       [ 3.19285221e-02],
       [-3.98100632e-02],
       [ 3.21181422e-02],
       [-2.31969588e-02],
       [ 3.49389761e-02],
       [-3.19273803e-02],
       [-5.94908849e-03],
       [-2.32794158e-02],
       [ 9.74673674e-02],
       [-2.38764265e-02],
       [-1.10117095e-02],
       [-4.46605288e-02],
       [ 1.79653741e-02],
       [ 2.98372868e-02],
       [ 1.86278267e-02],
       [-9.16289354e-02],
       [ 3.29560797e-02],
       [ 2.40273182e-02],
       [-6.67813213e-03],
       [ 2.26918691e-02],
       [-5.85273978e-03],
       [-6.22570599e-02],
       [ 7.37784611e-02],
       [-3.26113223e-02],
       [-1.49841947e-02],
       [ 4.40831047e-02],
       [-6.27365180e-02],
       [ 5.62690967e-02],
       [-1.99460475e-02],
       [ 2.66844480e-03],
       [ 3.47917054e-02],
       [-3.14138438e-02],
       [-4.16362583e-02],
       [ 6.35960903e-02],
       [-3.03489396e-02],
       [-5.09540265e-03],
       [ 1.12155105e-02],
       [-1.70129684e-02],
       [ 3.94811616e-02],
       [ 4.20803385e-02],
       [-1.44533558e-01],
       [ 6.39687457e-02],
       [ 6.06714529e-02],
       [-6.27973568e-02],
       [ 1.33356564e-02],
       [-2.60813350e-02],
       [-2.16124984e-02],
       [ 1.35857639e-01],
       [-9.54724676e-02],
       [ 1.42275869e-03],
       [ 3.70672602e-02],
       [-3.39944639e-02],
       [-8.86874648e-03],
       [ 6.06530362e-02],
       [-5.76258917e-02],
       [ 1.29550911e-02],
       [ 4.38760154e-02],
       [-1.24850473e-01],
       [ 1.29240935e-01],
       [-5.92933054e-02],
       [-3.00691907e-02],
       [ 5.12056151e-02],
       [-2.31474221e-02],
       [-6.36836169e-02],
       [ 1.46681552e-01],
       [-9.53603573e-02],
       [ 4.83931695e-02],
       [-9.19381806e-03],
       [-3.01235347e-02],
       [ 5.14564423e-02],
       [ 1.21080883e-02],
       [-7.90188509e-02],
       [ 1.20418143e-02],
       [ 2.67683610e-02],
       [-2.50847243e-02],
       [ 3.28248360e-02],
       [-2.01605277e-02],
       [-4.91667623e-02],
       [ 8.07244270e-02],
       [-1.60839706e-02],
       [-1.07956496e-02],
       [ 2.98035161e-02],
       [-5.12320904e-02],
       [ 1.02042219e-02],
       [ 3.70692554e-02],
       [-4.72540530e-02],
       [ 3.90703534e-02],
       [-3.04710276e-02],
       [-3.82843837e-03],
       [ 3.33096327e-02],
       [-4.93417047e-02],
       [ 4.93532266e-02],
       [-3.70143747e-04],
       [-3.59351316e-02],
       [ 5.22090439e-02],
       [-5.19669754e-02],
       [-1.38146285e-03],
       [ 4.90284541e-02],
       [-1.60243550e-02],
       [ 6.79618366e-03],
       [-5.60102336e-02],
       [ 7.84070394e-02],
       [-7.51479105e-02],
       [ 4.06013848e-02],
       [-9.79219849e-03],
       [-4.25577342e-02],
       [ 4.03415871e-02],
       [ 5.21551839e-02],
       [-1.09699871e-01],
       [ 1.09564554e-01],
       [-3.09141180e-02],
       [-1.95352177e-02],
       [ 4.72576889e-02],
       [-8.45075977e-02],
       [ 5.78937414e-02],
       [-5.52224984e-02],
       [ 5.25396810e-02]])
>>> Xy = pd.concat([pd.DataFrame(X),pd.DataFrame(X)],axis=1)
>>> Xy
            0
0    0.071292
1   -0.057109
2    0.024789
3    0.014812
4   -0.035548
..        ...
195  0.047258
196 -0.084508
197  0.057894
198 -0.055222
199  0.052540

[400 rows x 1 columns]
>>> Xy = pd.concat([pd.DataFrame(X),pd.DataFrame(X)],axis=1)
>>> X1
array([[ 7.12916604e-02],
       [-5.71090087e-02],
       [ 2.47888104e-02],
       [ 1.48119474e-02],
       [-3.55480936e-02],
       [-5.67004752e-03],
       [ 4.33341972e-02],
       [-4.49757476e-02],
       [ 6.64730852e-02],
       [-7.11869778e-02],
       [ 1.93652135e-02],
       [ 2.22722374e-02],
       [-1.39653656e-02],
       [ 2.80595778e-02],
       [-4.73358287e-02],
       [ 8.92230065e-03],
       [ 5.44188846e-02],
       [-3.11788772e-02],
       [-2.52956729e-02],
       [ 4.10385118e-02],
       [-5.81418146e-02],
       [ 4.66985338e-02],
       [-2.34544760e-02],
       [-7.67024906e-03],
       [ 2.18431964e-02],
       [ 2.15307613e-02],
       [-4.39338235e-02],
       [ 3.93825632e-02],
       [-4.97357019e-02],
       [ 6.55203200e-02],
       [-7.00506409e-02],
       [ 8.77359131e-02],
       [-1.43764594e-01],
       [ 1.03233811e-01],
       [-2.24687553e-02],
       [ 5.13764858e-02],
       [-7.42848841e-02],
       [ 7.41398517e-02],
       [-1.39496636e-01],
       [ 2.03419915e-01],
       [-1.07553578e-01],
       [-5.52731841e-02],
       [ 3.26914267e-02],
       [ 5.14105244e-02],
       [-5.93489256e-02],
       [ 6.12788920e-02],
       [-3.89445345e-02],
       [-2.98148947e-02],
       [ 9.45859940e-02],
       [-3.66844045e-02],
       [-4.41207727e-02],
       [ 1.14268563e-02],
       [ 2.14965494e-02],
       [-5.77671958e-02],
       [ 9.45254283e-02],
       [-3.80455050e-02],
       [-2.93876959e-02],
       [ 4.30072620e-02],
       [ 5.53305774e-01],
       [-1.15766459e+00],
       [ 5.91051596e-01],
       [ 1.56136551e-02],
       [-4.92347303e-02],
       [ 4.48162271e-02],
       [-5.80366557e-02],
       [-5.09341763e-01],
       [ 1.09678868e+00],
       [-5.33741493e-01],
       [-4.50662122e-02],
       [ 4.99695580e-02],
       [-4.88600036e-02],
       [ 7.35350688e-02],
       [-5.61621187e-02],
       [ 2.77657447e-02],
       [-3.45116494e-02],
       [ 2.19814549e-02],
       [ 2.47910573e-02],
       [-2.46113270e-02],
       [-4.82748468e-02],
       [ 8.61982680e-02],
       [-6.56736175e-02],
       [ 5.87547005e-02],
       [-5.35143934e-02],
       [ 3.19285221e-02],
       [-3.98100632e-02],
       [ 3.21181422e-02],
       [-2.31969588e-02],
       [ 3.49389761e-02],
       [-3.19273803e-02],
       [-5.94908849e-03],
       [-2.32794158e-02],
       [ 9.74673674e-02],
       [-2.38764265e-02],
       [-1.10117095e-02],
       [-4.46605288e-02],
       [ 1.79653741e-02],
       [ 2.98372868e-02],
       [ 1.86278267e-02],
       [-9.16289354e-02],
       [ 3.29560797e-02],
       [ 2.40273182e-02],
       [-6.67813213e-03],
       [ 2.26918691e-02],
       [-5.85273978e-03],
       [-6.22570599e-02],
       [ 7.37784611e-02],
       [-3.26113223e-02],
       [-1.49841947e-02],
       [ 4.40831047e-02],
       [-6.27365180e-02],
       [ 5.62690967e-02],
       [-1.99460475e-02],
       [ 2.66844480e-03],
       [ 3.47917054e-02],
       [-3.14138438e-02],
       [-4.16362583e-02],
       [ 6.35960903e-02],
       [-3.03489396e-02],
       [-5.09540265e-03],
       [ 1.12155105e-02],
       [-1.70129684e-02],
       [ 3.94811616e-02],
       [ 4.20803385e-02],
       [-1.44533558e-01],
       [ 6.39687457e-02],
       [ 6.06714529e-02],
       [-6.27973568e-02],
       [ 1.33356564e-02],
       [-2.60813350e-02],
       [-2.16124984e-02],
       [ 1.35857639e-01],
       [-9.54724676e-02],
       [ 1.42275869e-03],
       [ 3.70672602e-02],
       [-3.39944639e-02],
       [-8.86874648e-03],
       [ 6.06530362e-02],
       [-5.76258917e-02],
       [ 1.29550911e-02],
       [ 4.38760154e-02],
       [-1.24850473e-01],
       [ 1.29240935e-01],
       [-5.92933054e-02],
       [-3.00691907e-02],
       [ 5.12056151e-02],
       [-2.31474221e-02],
       [-6.36836169e-02],
       [ 1.46681552e-01],
       [-9.53603573e-02],
       [ 4.83931695e-02],
       [-9.19381806e-03],
       [-3.01235347e-02],
       [ 5.14564423e-02],
       [ 1.21080883e-02],
       [-7.90188509e-02],
       [ 1.20418143e-02],
       [ 2.67683610e-02],
       [-2.50847243e-02],
       [ 3.28248360e-02],
       [-2.01605277e-02],
       [-4.91667623e-02],
       [ 8.07244270e-02],
       [-1.60839706e-02],
       [-1.07956496e-02],
       [ 2.98035161e-02],
       [-5.12320904e-02],
       [ 1.02042219e-02],
       [ 3.70692554e-02],
       [-4.72540530e-02],
       [ 3.90703534e-02],
       [-3.04710276e-02],
       [-3.82843837e-03],
       [ 3.33096327e-02],
       [-4.93417047e-02],
       [ 4.93532266e-02],
       [-3.70143747e-04],
       [-3.59351316e-02],
       [ 5.22090439e-02],
       [-5.19669754e-02],
       [-1.38146285e-03],
       [ 4.90284541e-02],
       [-1.60243550e-02],
       [ 6.79618366e-03],
       [-5.60102336e-02],
       [ 7.84070394e-02],
       [-7.51479105e-02],
       [ 4.06013848e-02],
       [-9.79219849e-03],
       [-4.25577342e-02],
       [ 4.03415871e-02],
       [ 5.21551839e-02],
       [-1.09699871e-01],
       [ 1.09564554e-01],
       [-3.09141180e-02],
       [-1.95352177e-02],
       [ 4.72576889e-02],
       [-8.45075977e-02],
       [ 5.78937414e-02],
       [-5.52224984e-02],
       [ 5.25396810e-02]])
>>> Xy = tK.prepareLongShort(X1,y1)
>>> Xy
            0         0
0    0.000000  0.071292
1    0.071292 -0.057109
2   -0.057109  0.024789
3    0.024789  0.014812
4    0.014812 -0.035548
..        ...       ...
195 -0.019535  0.047258
196  0.047258 -0.084508
197 -0.084508  0.057894
198  0.057894 -0.055222
199 -0.055222  0.052540

[200 rows x 2 columns]
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/deep_lernia/deep_lernia/train_longShort.py", line 163, in <module>
    test = test.reshape(test.shape[0], test.shape[1])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py", line 5274, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'DataFrame' object has no attribute 'reshape'
>>> train
           0         0
0   0.000000  0.071292
1   0.071292 -0.057109
2  -0.057109  0.024789
3   0.024789  0.014812
4   0.014812 -0.035548
..       ...       ...
95 -0.044661  0.017965
96  0.017965  0.029837
97  0.029837  0.018628
98  0.018628 -0.091629
99 -0.091629  0.032956

[100 rows x 2 columns]
>>> test
            0         0
100  0.032956  0.024027
101  0.024027 -0.006678
102 -0.006678  0.022692
103  0.022692 -0.005853
104 -0.005853 -0.062257
..        ...       ...
195 -0.019535  0.047258
196  0.047258 -0.084508
197 -0.084508  0.057894
198  0.057894 -0.055222
199 -0.055222  0.052540

[100 rows x 2 columns]
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/deep_lernia/deep_lernia/train_longShort.py", line 163, in <module>
    test = test.reshape(test.shape[0], test.shape[1])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py", line 5274, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'DataFrame' object has no attribute 'reshape'
>>> -1.1576645871394797 1.0967886766721586
>>> train
array([[ 0.        ,  0.07129166],
       [ 0.07129166, -0.05710901],
       [-0.05710901,  0.02478881],
       [ 0.02478881,  0.01481195],
       [ 0.01481195, -0.03554809],
       [-0.03554809, -0.00567005],
       [-0.00567005,  0.0433342 ],
       [ 0.0433342 , -0.04497575],
       [-0.04497575,  0.06647309],
       [ 0.06647309, -0.07118698],
       [-0.07118698,  0.01936521],
       [ 0.01936521,  0.02227224],
       [ 0.02227224, -0.01396537],
       [-0.01396537,  0.02805958],
       [ 0.02805958, -0.04733583],
       [-0.04733583,  0.0089223 ],
       [ 0.0089223 ,  0.05441888],
       [ 0.05441888, -0.03117888],
       [-0.03117888, -0.02529567],
       [-0.02529567,  0.04103851],
       [ 0.04103851, -0.05814181],
       [-0.05814181,  0.04669853],
       [ 0.04669853, -0.02345448],
       [-0.02345448, -0.00767025],
       [-0.00767025,  0.0218432 ],
       [ 0.0218432 ,  0.02153076],
       [ 0.02153076, -0.04393382],
       [-0.04393382,  0.03938256],
       [ 0.03938256, -0.0497357 ],
       [-0.0497357 ,  0.06552032],
       [ 0.06552032, -0.07005064],
       [-0.07005064,  0.08773591],
       [ 0.08773591, -0.14376459],
       [-0.14376459,  0.10323381],
       [ 0.10323381, -0.02246876],
       [-0.02246876,  0.05137649],
       [ 0.05137649, -0.07428488],
       [-0.07428488,  0.07413985],
       [ 0.07413985, -0.13949664],
       [-0.13949664,  0.20341992],
       [ 0.20341992, -0.10755358],
       [-0.10755358, -0.05527318],
       [-0.05527318,  0.03269143],
       [ 0.03269143,  0.05141052],
       [ 0.05141052, -0.05934893],
       [-0.05934893,  0.06127889],
       [ 0.06127889, -0.03894453],
       [-0.03894453, -0.02981489],
       [-0.02981489,  0.09458599],
       [ 0.09458599, -0.0366844 ],
       [-0.0366844 , -0.04412077],
       [-0.04412077,  0.01142686],
       [ 0.01142686,  0.02149655],
       [ 0.02149655, -0.0577672 ],
       [-0.0577672 ,  0.09452543],
       [ 0.09452543, -0.0380455 ],
       [-0.0380455 , -0.0293877 ],
       [-0.0293877 ,  0.04300726],
       [ 0.04300726,  0.55330577],
       [ 0.55330577, -1.15766459],
       [-1.15766459,  0.5910516 ],
       [ 0.5910516 ,  0.01561366],
       [ 0.01561366, -0.04923473],
       [-0.04923473,  0.04481623],
       [ 0.04481623, -0.05803666],
       [-0.05803666, -0.50934176],
       [-0.50934176,  1.09678868],
       [ 1.09678868, -0.53374149],
       [-0.53374149, -0.04506621],
       [-0.04506621,  0.04996956],
       [ 0.04996956, -0.04886   ],
       [-0.04886   ,  0.07353507],
       [ 0.07353507, -0.05616212],
       [-0.05616212,  0.02776574],
       [ 0.02776574, -0.03451165],
       [-0.03451165,  0.02198145],
       [ 0.02198145,  0.02479106],
       [ 0.02479106, -0.02461133],
       [-0.02461133, -0.04827485],
       [-0.04827485,  0.08619827],
       [ 0.08619827, -0.06567362],
       [-0.06567362,  0.0587547 ],
       [ 0.0587547 , -0.05351439],
       [-0.05351439,  0.03192852],
       [ 0.03192852, -0.03981006],
       [-0.03981006,  0.03211814],
       [ 0.03211814, -0.02319696],
       [-0.02319696,  0.03493898],
       [ 0.03493898, -0.03192738],
       [-0.03192738, -0.00594909],
       [-0.00594909, -0.02327942],
       [-0.02327942,  0.09746737],
       [ 0.09746737, -0.02387643],
       [-0.02387643, -0.01101171],
       [-0.01101171, -0.04466053],
       [-0.04466053,  0.01796537],
       [ 0.01796537,  0.02983729],
       [ 0.02983729,  0.01862783],
       [ 0.01862783, -0.09162894],
       [-0.09162894,  0.03295608]])
>>> test
array([[ 0.03295608,  0.02402732],
       [ 0.02402732, -0.00667813],
       [-0.00667813,  0.02269187],
       [ 0.02269187, -0.00585274],
       [-0.00585274, -0.06225706],
       [-0.06225706,  0.07377846],
       [ 0.07377846, -0.03261132],
       [-0.03261132, -0.01498419],
       [-0.01498419,  0.0440831 ],
       [ 0.0440831 , -0.06273652],
       [-0.06273652,  0.0562691 ],
       [ 0.0562691 , -0.01994605],
       [-0.01994605,  0.00266844],
       [ 0.00266844,  0.03479171],
       [ 0.03479171, -0.03141384],
       [-0.03141384, -0.04163626],
       [-0.04163626,  0.06359609],
       [ 0.06359609, -0.03034894],
       [-0.03034894, -0.0050954 ],
       [-0.0050954 ,  0.01121551],
       [ 0.01121551, -0.01701297],
       [-0.01701297,  0.03948116],
       [ 0.03948116,  0.04208034],
       [ 0.04208034, -0.14453356],
       [-0.14453356,  0.06396875],
       [ 0.06396875,  0.06067145],
       [ 0.06067145, -0.06279736],
       [-0.06279736,  0.01333566],
       [ 0.01333566, -0.02608133],
       [-0.02608133, -0.0216125 ],
       [-0.0216125 ,  0.13585764],
       [ 0.13585764, -0.09547247],
       [-0.09547247,  0.00142276],
       [ 0.00142276,  0.03706726],
       [ 0.03706726, -0.03399446],
       [-0.03399446, -0.00886875],
       [-0.00886875,  0.06065304],
       [ 0.06065304, -0.05762589],
       [-0.05762589,  0.01295509],
       [ 0.01295509,  0.04387602],
       [ 0.04387602, -0.12485047],
       [-0.12485047,  0.12924093],
       [ 0.12924093, -0.05929331],
       [-0.05929331, -0.03006919],
       [-0.03006919,  0.05120562],
       [ 0.05120562, -0.02314742],
       [-0.02314742, -0.06368362],
       [-0.06368362,  0.14668155],
       [ 0.14668155, -0.09536036],
       [-0.09536036,  0.04839317],
       [ 0.04839317, -0.00919382],
       [-0.00919382, -0.03012353],
       [-0.03012353,  0.05145644],
       [ 0.05145644,  0.01210809],
       [ 0.01210809, -0.07901885],
       [-0.07901885,  0.01204181],
       [ 0.01204181,  0.02676836],
       [ 0.02676836, -0.02508472],
       [-0.02508472,  0.03282484],
       [ 0.03282484, -0.02016053],
       [-0.02016053, -0.04916676],
       [-0.04916676,  0.08072443],
       [ 0.08072443, -0.01608397],
       [-0.01608397, -0.01079565],
       [-0.01079565,  0.02980352],
       [ 0.02980352, -0.05123209],
       [-0.05123209,  0.01020422],
       [ 0.01020422,  0.03706926],
       [ 0.03706926, -0.04725405],
       [-0.04725405,  0.03907035],
       [ 0.03907035, -0.03047103],
       [-0.03047103, -0.00382844],
       [-0.00382844,  0.03330963],
       [ 0.03330963, -0.0493417 ],
       [-0.0493417 ,  0.04935323],
       [ 0.04935323, -0.00037014],
       [-0.00037014, -0.03593513],
       [-0.03593513,  0.05220904],
       [ 0.05220904, -0.05196698],
       [-0.05196698, -0.00138146],
       [-0.00138146,  0.04902845],
       [ 0.04902845, -0.01602436],
       [-0.01602436,  0.00679618],
       [ 0.00679618, -0.05601023],
       [-0.05601023,  0.07840704],
       [ 0.07840704, -0.07514791],
       [-0.07514791,  0.04060138],
       [ 0.04060138, -0.0097922 ],
       [-0.0097922 , -0.04255773],
       [-0.04255773,  0.04034159],
       [ 0.04034159,  0.05215518],
       [ 0.05215518, -0.10969987],
       [-0.10969987,  0.10956455],
       [ 0.10956455, -0.03091412],
       [-0.03091412, -0.01953522],
       [-0.01953522,  0.04725769],
       [ 0.04725769, -0.0845076 ],
       [-0.0845076 ,  0.05789374],
       [ 0.05789374, -0.0552225 ],
       [-0.0552225 ,  0.05253968]])
>>> train.shape
(100, 2)
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 180, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 157, in splitSet
    pre, post = self.prepareLongShort(X,y,n_in=1,n_out=1).values
AttributeError: 'tuple' object has no attribute 'values'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 184, in train
    self.model.fit(X_train,y_train,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 505, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2657, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:531 train_step  **
        y_pred = self(x, training=True)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__
        input_spec.assert_input_compatibility(self.input_spec, inputs,
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/input_spec.py:176 assert_input_compatibility
        raise ValueError('Input ' + str(input_index) + ' of layer ' +

    ValueError: Input 0 of layer sequential_54 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [1, 1]

>>> 
>>> 
>>> pre, post = tK.prepareLongShort(X1,y1)
>>> pre
            0
0    0.000000
1   -0.128401
2    0.081898
3   -0.009977
4   -0.050360
..        ...
194  0.011379
195  0.066793
196 -0.131765
197  0.142401
198 -0.113116

[199 rows x 1 columns]
>>> post
            0
0   -0.128401
1    0.081898
2   -0.009977
3   -0.050360
4    0.029878
..        ...
194  0.066793
195 -0.131765
196  0.142401
197 -0.113116
198  0.107762

[199 rows x 1 columns]
>>> pre
            0
0    0.000000
1   -0.128401
2    0.081898
3   -0.009977
4   -0.050360
..        ...
194  0.011379
195  0.066793
196 -0.131765
197  0.142401
198 -0.113116

[199 rows x 1 columns]
>>> post
            0
0   -0.128401
1    0.081898
2   -0.009977
3   -0.050360
4    0.029878
..        ...
194  0.066793
195 -0.131765
196  0.142401
197 -0.113116
198  0.107762

[199 rows x 1 columns]
>>> Xy
            0         0
0    0.000000  0.071292
1    0.071292 -0.057109
2   -0.057109  0.024789
3    0.024789  0.014812
4    0.014812 -0.035548
..        ...       ...
195 -0.019535  0.047258
196  0.047258 -0.084508
197 -0.084508  0.057894
198  0.057894 -0.055222
199 -0.055222  0.052540

[200 rows x 2 columns]
>>> X_train
           0
0   0.000000
1  -0.128401
2   0.081898
3  -0.009977
4  -0.050360
..       ...
94 -0.033649
95  0.062626
96  0.011872
97 -0.011209
98 -0.110257

[99 rows x 1 columns]
>>> -1.1576645871394797 1.0967886766721586
>>> X_train
array([[[ 0.        ]],

       [[ 0.07129166]],

       [[-0.05710901]],

       [[ 0.02478881]],

       [[ 0.01481195]],

       [[-0.03554809]],

       [[-0.00567005]],

       [[ 0.0433342 ]],

       [[-0.04497575]],

       [[ 0.06647309]],

       [[-0.07118698]],

       [[ 0.01936521]],

       [[ 0.02227224]],

       [[-0.01396537]],

       [[ 0.02805958]],

       [[-0.04733583]],

       [[ 0.0089223 ]],

       [[ 0.05441888]],

       [[-0.03117888]],

       [[-0.02529567]],

       [[ 0.04103851]],

       [[-0.05814181]],

       [[ 0.04669853]],

       [[-0.02345448]],

       [[-0.00767025]],

       [[ 0.0218432 ]],

       [[ 0.02153076]],

       [[-0.04393382]],

       [[ 0.03938256]],

       [[-0.0497357 ]],

       [[ 0.06552032]],

       [[-0.07005064]],

       [[ 0.08773591]],

       [[-0.14376459]],

       [[ 0.10323381]],

       [[-0.02246876]],

       [[ 0.05137649]],

       [[-0.07428488]],

       [[ 0.07413985]],

       [[-0.13949664]],

       [[ 0.20341992]],

       [[-0.10755358]],

       [[-0.05527318]],

       [[ 0.03269143]],

       [[ 0.05141052]],

       [[-0.05934893]],

       [[ 0.06127889]],

       [[-0.03894453]],

       [[-0.02981489]],

       [[ 0.09458599]],

       [[-0.0366844 ]],

       [[-0.04412077]],

       [[ 0.01142686]],

       [[ 0.02149655]],

       [[-0.0577672 ]],

       [[ 0.09452543]],

       [[-0.0380455 ]],

       [[-0.0293877 ]],

       [[ 0.04300726]],

       [[ 0.55330577]],

       [[-1.15766459]],

       [[ 0.5910516 ]],

       [[ 0.01561366]],

       [[-0.04923473]],

       [[ 0.04481623]],

       [[-0.05803666]],

       [[-0.50934176]],

       [[ 1.09678868]],

       [[-0.53374149]],

       [[-0.04506621]],

       [[ 0.04996956]],

       [[-0.04886   ]],

       [[ 0.07353507]],

       [[-0.05616212]],

       [[ 0.02776574]],

       [[-0.03451165]],

       [[ 0.02198145]],

       [[ 0.02479106]],

       [[-0.02461133]],

       [[-0.04827485]],

       [[ 0.08619827]],

       [[-0.06567362]],

       [[ 0.0587547 ]],

       [[-0.05351439]],

       [[ 0.03192852]],

       [[-0.03981006]],

       [[ 0.03211814]],

       [[-0.02319696]],

       [[ 0.03493898]],

       [[-0.03192738]],

       [[-0.00594909]],

       [[-0.02327942]],

       [[ 0.09746737]],

       [[-0.02387643]],

       [[-0.01101171]],

       [[-0.04466053]],

       [[ 0.01796537]],

       [[ 0.02983729]],

       [[ 0.01862783]],

       [[-0.09162894]]])
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 180, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 166, in splitSet
    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py", line 5274, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'DataFrame' object has no attribute 'reshape'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 180, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 168, in splitSet
    X_test, y_test = test[:, 0:-1], test[:, -1]
NameError: name 'test' is not defined
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 188, in train
    y_pred = self.predict(X_test,batch_size=batch_size)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 196, in predict
    X1 = X_test[i,:]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/range.py", line 353, in get_loc
    return super().get_loc(key, method=method, tolerance=tolerance)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 116, in pandas._libs.index.IndexEngine.get_loc
TypeError: '(0, slice(None, None, None))' is an invalid key
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 190, in train
    kpi = t_s.calcMetrics(y_pred, y_test)
  File "/home/sabeiro/lav//src/lernia/lernia/train_score.py", line 28, in calcMetrics
    mean, rmse, err = relErr(y1,y2)
  File "/home/sabeiro/lav//src/lernia/lernia/train_score.py", line 19, in relErr
    rmse = np.sqrt((y1-y2)**2).sum()/len(y1)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/ops/__init__.py", line 759, in f
    other = _align_method_FRAME(self, other, axis)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/ops/__init__.py", line 647, in _align_method_FRAME
    right = to_series(right)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/ops/__init__.py", line 638, in to_series
    raise ValueError(
ValueError: Unable to coerce to Series, length must be 1: given 100
>>> -1.1576645871394797 1.0967886766721586
>>> y_test
array([ 0.02402732, -0.00667813,  0.02269187, -0.00585274, -0.06225706,
        0.07377846, -0.03261132, -0.01498419,  0.0440831 , -0.06273652,
        0.0562691 , -0.01994605,  0.00266844,  0.03479171, -0.03141384,
       -0.04163626,  0.06359609, -0.03034894, -0.0050954 ,  0.01121551,
       -0.01701297,  0.03948116,  0.04208034, -0.14453356,  0.06396875,
        0.06067145, -0.06279736,  0.01333566, -0.02608133, -0.0216125 ,
        0.13585764, -0.09547247,  0.00142276,  0.03706726, -0.03399446,
       -0.00886875,  0.06065304, -0.05762589,  0.01295509,  0.04387602,
       -0.12485047,  0.12924093, -0.05929331, -0.03006919,  0.05120562,
       -0.02314742, -0.06368362,  0.14668155, -0.09536036,  0.04839317,
       -0.00919382, -0.03012353,  0.05145644,  0.01210809, -0.07901885,
        0.01204181,  0.02676836, -0.02508472,  0.03282484, -0.02016053,
       -0.04916676,  0.08072443, -0.01608397, -0.01079565,  0.02980352,
       -0.05123209,  0.01020422,  0.03706926, -0.04725405,  0.03907035,
       -0.03047103, -0.00382844,  0.03330963, -0.0493417 ,  0.04935323,
       -0.00037014, -0.03593513,  0.05220904, -0.05196698, -0.00138146,
        0.04902845, -0.01602436,  0.00679618, -0.05601023,  0.07840704,
       -0.07514791,  0.04060138, -0.0097922 , -0.04255773,  0.04034159,
        0.05215518, -0.10969987,  0.10956455, -0.03091412, -0.01953522,
        0.04725769, -0.0845076 ,  0.05789374, -0.0552225 ,  0.05253968])
>>> y_test.shape
(100,)
>>> y_test.shape
(100, 1)
>>> post
            0
0   -0.128401
1    0.081898
2   -0.009977
3   -0.050360
4    0.029878
..        ...
194  0.066793
195 -0.131765
196  0.142401
197 -0.113116
198  0.107762

[199 rows x 1 columns]
>>> post[0].shape
0     -0.128401
1      0.081898
2     -0.009977
3     -0.050360
4      0.029878
         ...   
194    0.066793
195   -0.131765
196    0.142401
197   -0.113116
198    0.107762
Name: 0, Length: 199, dtype: float64
>>> post[0].shape
(199,)
>>> y_test
99    -0.016072
100   -0.028661
101    0.006068
102   -0.027411
103   -0.043517
         ...   
194    0.027702
195   -0.087082
196    0.071410
197   -0.076301
198    0.051386
Name: 0, Length: 100, dtype: float32
>>> y_pred
array([-0.05154698, -0.02095295, -0.00092081, -0.01886582, -0.00061399,
        0.01789986, -0.05801386,  0.01808694, -0.00700822, -0.03380752,
        0.0269112 , -0.04734103,  0.00933382, -0.01241711, -0.02425943,
        0.01322937,  0.00334311, -0.05006602,  0.01556907, -0.01119704,
       -0.01693454, -0.00023711, -0.030462  , -0.01837564,  0.06573318,
       -0.07109626, -0.03015985,  0.03467559, -0.02555461,  0.00179847,
       -0.00735372, -0.07703602,  0.06289777, -0.0237758 , -0.02949088,
        0.01323383, -0.01186947, -0.04024542,  0.02931765, -0.0253932 ,
       -0.02843612,  0.05461506, -0.09558462,  0.03932208, -0.00399711,
       -0.04205526,  0.01019355,  0.01501794, -0.09158844,  0.06230226,
       -0.04446346,  0.00293456,  0.00388708, -0.03958163, -0.00373893,
        0.03122208, -0.03329866, -0.0244376 ,  0.00681641, -0.02842746,
        0.00624733,  0.00869208, -0.05881967,  0.01365515, -0.00337662,
       -0.02445292,  0.0195225 , -0.02514314, -0.02672217,  0.01985111,
       -0.03577826,  0.0106819 , -0.01357035, -0.02688193,  0.01920019,
       -0.04141844,  0.00016417,  0.0090937 , -0.04047662,  0.02349512,
       -0.01903088, -0.03456592,  0.00883905, -0.01256604,  0.01635149,
       -0.05802368,  0.03785954, -0.04182491,  0.00050439,  0.0080155 ,
       -0.03861781, -0.02530972,  0.05282131, -0.08091342,  0.02454468,
       -0.00196989, -0.03514943,  0.03696072, -0.05366575,  0.02271559],
      dtype=float32)
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 115, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 187, in train
    self.model.fit(X_train,y_train,epochs=1,batch_size=batch_size,verbose=0,shuffle=False)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 802, in fit
    data_handler = data_adapter.DataHandler(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/data_adapter.py", line 1100, in __init__
    self._adapter = adapter_cls(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/data_adapter.py", line 282, in __init__
    raise ValueError(msg)
ValueError: Data cardinality is ambiguous:
  x sizes: 99
  y sizes: 0
Please provide data which shares the same first dimension.
>>> y1
array([ 7.12916604e-02, -5.71090087e-02,  2.47888104e-02,  1.48119474e-02,
       -3.55480936e-02, -5.67004752e-03,  4.33341972e-02, -4.49757476e-02,
        6.64730852e-02, -7.11869778e-02,  1.93652135e-02,  2.22722374e-02,
       -1.39653656e-02,  2.80595778e-02, -4.73358287e-02,  8.92230065e-03,
        5.44188846e-02, -3.11788772e-02, -2.52956729e-02,  4.10385118e-02,
       -5.81418146e-02,  4.66985338e-02, -2.34544760e-02, -7.67024906e-03,
        2.18431964e-02,  2.15307613e-02, -4.39338235e-02,  3.93825632e-02,
       -4.97357019e-02,  6.55203200e-02, -7.00506409e-02,  8.77359131e-02,
       -1.43764594e-01,  1.03233811e-01, -2.24687553e-02,  5.13764858e-02,
       -7.42848841e-02,  7.41398517e-02, -1.39496636e-01,  2.03419915e-01,
       -1.07553578e-01, -5.52731841e-02,  3.26914267e-02,  5.14105244e-02,
       -5.93489256e-02,  6.12788920e-02, -3.89445345e-02, -2.98148947e-02,
        9.45859940e-02, -3.66844045e-02, -4.41207727e-02,  1.14268563e-02,
        2.14965494e-02, -5.77671958e-02,  9.45254283e-02, -3.80455050e-02,
       -2.93876959e-02,  4.30072620e-02,  5.53305774e-01, -1.15766459e+00,
        5.91051596e-01,  1.56136551e-02, -4.92347303e-02,  4.48162271e-02,
       -5.80366557e-02, -5.09341763e-01,  1.09678868e+00, -5.33741493e-01,
       -4.50662122e-02,  4.99695580e-02, -4.88600036e-02,  7.35350688e-02,
       -5.61621187e-02,  2.77657447e-02, -3.45116494e-02,  2.19814549e-02,
        2.47910573e-02, -2.46113270e-02, -4.82748468e-02,  8.61982680e-02,
       -6.56736175e-02,  5.87547005e-02, -5.35143934e-02,  3.19285221e-02,
       -3.98100632e-02,  3.21181422e-02, -2.31969588e-02,  3.49389761e-02,
       -3.19273803e-02, -5.94908849e-03, -2.32794158e-02,  9.74673674e-02,
       -2.38764265e-02, -1.10117095e-02, -4.46605288e-02,  1.79653741e-02,
        2.98372868e-02,  1.86278267e-02, -9.16289354e-02,  3.29560797e-02,
        2.40273182e-02, -6.67813213e-03,  2.26918691e-02, -5.85273978e-03,
       -6.22570599e-02,  7.37784611e-02, -3.26113223e-02, -1.49841947e-02,
        4.40831047e-02, -6.27365180e-02,  5.62690967e-02, -1.99460475e-02,
        2.66844480e-03,  3.47917054e-02, -3.14138438e-02, -4.16362583e-02,
        6.35960903e-02, -3.03489396e-02, -5.09540265e-03,  1.12155105e-02,
       -1.70129684e-02,  3.94811616e-02,  4.20803385e-02, -1.44533558e-01,
        6.39687457e-02,  6.06714529e-02, -6.27973568e-02,  1.33356564e-02,
       -2.60813350e-02, -2.16124984e-02,  1.35857639e-01, -9.54724676e-02,
        1.42275869e-03,  3.70672602e-02, -3.39944639e-02, -8.86874648e-03,
        6.06530362e-02, -5.76258917e-02,  1.29550911e-02,  4.38760154e-02,
       -1.24850473e-01,  1.29240935e-01, -5.92933054e-02, -3.00691907e-02,
        5.12056151e-02, -2.31474221e-02, -6.36836169e-02,  1.46681552e-01,
       -9.53603573e-02,  4.83931695e-02, -9.19381806e-03, -3.01235347e-02,
        5.14564423e-02,  1.21080883e-02, -7.90188509e-02,  1.20418143e-02,
        2.67683610e-02, -2.50847243e-02,  3.28248360e-02, -2.01605277e-02,
       -4.91667623e-02,  8.07244270e-02, -1.60839706e-02, -1.07956496e-02,
        2.98035161e-02, -5.12320904e-02,  1.02042219e-02,  3.70692554e-02,
       -4.72540530e-02,  3.90703534e-02, -3.04710276e-02, -3.82843837e-03,
        3.33096327e-02, -4.93417047e-02,  4.93532266e-02, -3.70143747e-04,
       -3.59351316e-02,  5.22090439e-02, -5.19669754e-02, -1.38146285e-03,
        4.90284541e-02, -1.60243550e-02,  6.79618366e-03, -5.60102336e-02,
        7.84070394e-02, -7.51479105e-02,  4.06013848e-02, -9.79219849e-03,
       -4.25577342e-02,  4.03415871e-02,  5.21551839e-02, -1.09699871e-01,
        1.09564554e-01, -3.09141180e-02, -1.95352177e-02,  4.72576889e-02,
       -8.45075977e-02,  5.78937414e-02, -5.52224984e-02,  5.25396810e-02])
>>> y1.shape
(200,)
>>> y.shape
(200,)
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 124, in <module>
    tK.plotPrediction()
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 160, in plotPrediction
    X = self.reshape(self.X,mode=reshape)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 223, in reshape
    X_test = Xy[:,:]
TypeError: tuple indices must be integers or slices, not tuple
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 126, in <module>
    
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 142, in plotHistory
    raise Exception("train the model first")
Exception: train the model first
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 123, in <module>
    
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 160, in plotPrediction
    X = self.reshape(self.X,mode=reshape)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 225, in reshape
    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))
UnboundLocalError: local variable 'X_test' referenced before assignment
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 126, in <module>
    
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 147, in plotHistory
    val_loss = np.concatenate([x.history['val_loss'] for x in history])
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 147, in <listcomp>
    val_loss = np.concatenate([x.history['val_loss'] for x in history])
KeyError: 'val_loss'
>>> tK.history()
[<tensorflow.python.keras.callbacks.History object at 0x7efe1036cdc0>]
>>> tK.history()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'list' object is not callable
>>> <tensorflow.python.keras.callbacks.History object at 0x7efe0cf9beb0>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f6ffe50>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f730b50>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f77abb0>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f687eb0>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f6b8970>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f690160>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f737f40>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f6ae0d0>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f6f6eb0>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f737580>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f6bdf10>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f6b2f40>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f5bb610>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f667e20>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f6946d0>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f57bd60>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f5944f0>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f667f70>
<tensorflow.python.keras.callbacks.History object at 0x7efe6f77a8e0>
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 123, in <module>
    
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 160, in plotPrediction
    X = self.reshape(self.X,mode=reshape)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 224, in reshape
    X_test = X.reshape((X.shape[0], 1, X.shape[1]))
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py", line 5274, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'DataFrame' object has no attribute 'reshape'
>>> h = tK.history
[<tensorflow.python.keras.callbacks.History object at 0x7efe6f77a8e0>]
>>> h = tK.history
>>> 
>>> 150/150 - 0s - loss: 0.3294
<tensorflow.python.keras.callbacks.History object at 0x7efe142807f0>
150/150 - 0s - loss: 0.1155
<tensorflow.python.keras.callbacks.History object at 0x7efe14c108b0>
150/150 - 0s - loss: 0.1032
<tensorflow.python.keras.callbacks.History object at 0x7efe14bcfe20>
150/150 - 0s - loss: 0.0968
<tensorflow.python.keras.callbacks.History object at 0x7efe17f2b0a0>
150/150 - 0s - loss: 0.0916
<tensorflow.python.keras.callbacks.History object at 0x7efe32457e80>
150/150 - 0s - loss: 0.0872
<tensorflow.python.keras.callbacks.History object at 0x7efe0e047160>
150/150 - 0s - loss: 0.0833
<tensorflow.python.keras.callbacks.History object at 0x7efe0e050640>
150/150 - 0s - loss: 0.0800
<tensorflow.python.keras.callbacks.History object at 0x7efe8c37f6a0>
150/150 - 0s - loss: 0.0769
<tensorflow.python.keras.callbacks.History object at 0x7efe14408b20>
150/150 - 0s - loss: 0.0742
<tensorflow.python.keras.callbacks.History object at 0x7efe127c18e0>
150/150 - 0s - loss: 0.0717
<tensorflow.python.keras.callbacks.History object at 0x7efe12d3e9a0>
150/150 - 0s - loss: 0.0694
<tensorflow.python.keras.callbacks.History object at 0x7efe14723b80>
150/150 - 0s - loss: 0.0672
<tensorflow.python.keras.callbacks.History object at 0x7efe8c1f60d0>
150/150 - 0s - loss: 0.0651
<tensorflow.python.keras.callbacks.History object at 0x7efe8d750ee0>
150/150 - 0s - loss: 0.0632
<tensorflow.python.keras.callbacks.History object at 0x7efe14723970>
150/150 - 0s - loss: 0.0613
<tensorflow.python.keras.callbacks.History object at 0x7efe6d312a30>
150/150 - 0s - loss: 0.0595
<tensorflow.python.keras.callbacks.History object at 0x7efe17e97370>
150/150 - 0s - loss: 0.0579
<tensorflow.python.keras.callbacks.History object at 0x7efe48904430>
150/150 - 0s - loss: 0.0563
<tensorflow.python.keras.callbacks.History object at 0x7efe8c37fe80>
150/150 - 0s - loss: 0.0548
<tensorflow.python.keras.callbacks.History object at 0x7efe11bbc250>
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 126, in <module>
    
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 147, in plotHistory
    val_loss = np.concatenate([x.history['val_loss'] for x in history])
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 147, in <listcomp>
    val_loss = np.concatenate([x.history['val_loss'] for x in history])
KeyError: 'val_loss'
>>> h = tK.history[0]
[<tensorflow.python.keras.callbacks.History object at 0x7efe142807f0>, <tensorflow.python.keras.callbacks.History object at 0x7efe14c108b0>, <tensorflow.python.keras.callbacks.History object at 0x7efe14bcfe20>, <tensorflow.python.keras.callbacks.History object at 0x7efe17f2b0a0>, <tensorflow.python.keras.callbacks.History object at 0x7efe32457e80>, <tensorflow.python.keras.callbacks.History object at 0x7efe0e047160>, <tensorflow.python.keras.callbacks.History object at 0x7efe0e050640>, <tensorflow.python.keras.callbacks.History object at 0x7efe8c37f6a0>, <tensorflow.python.keras.callbacks.History object at 0x7efe14408b20>, <tensorflow.python.keras.callbacks.History object at 0x7efe127c18e0>, <tensorflow.python.keras.callbacks.History object at 0x7efe12d3e9a0>, <tensorflow.python.keras.callbacks.History object at 0x7efe14723b80>, <tensorflow.python.keras.callbacks.History object at 0x7efe8c1f60d0>, <tensorflow.python.keras.callbacks.History object at 0x7efe8d750ee0>, <tensorflow.python.keras.callbacks.History object at 0x7efe14723970>, <tensorflow.python.keras.callbacks.History object at 0x7efe6d312a30>, <tensorflow.python.keras.callbacks.History object at 0x7efe17e97370>, <tensorflow.python.keras.callbacks.History object at 0x7efe48904430>, <tensorflow.python.keras.callbacks.History object at 0x7efe8c37fe80>, <tensorflow.python.keras.callbacks.History object at 0x7efe11bbc250>]
>>> h = tK.history[0]
>>> h
<tensorflow.python.keras.callbacks.History object at 0x7efe142807f0>
>>> h.history
{'loss': [0.3294074535369873]}
>>> h.params
{'verbose': 2, 'epochs': 1, 'steps': 150}
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 123, in <module>
    
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 163, in plotPrediction
    ax.plot(y_pred[:,0],label='predicted',linestyle="dashed",color=color,linewidth=1,alpha=1.)
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
>>> t = tK.predict(X1)
>>> t
array([0.31795666, 0.37016484, 0.38626003, 0.39104468, 0.39323002,
       0.39419025, 0.39424032, 0.39448497, 0.39520177, 0.39541435,
       0.3962071 , 0.39600897, 0.39651033, 0.3972869 , 0.3978016 ,
       0.39837596, 0.3983296 , 0.39879942, 0.39960015, 0.39940175,
       0.398924  , 0.3987974 , 0.3980437 , 0.3980596 , 0.39774132,
       0.39763844, 0.39794666, 0.39814427, 0.39769104, 0.39763656,
       0.39704853, 0.39711735, 0.3962801 , 0.3961997 , 0.39478952,
       0.39547437, 0.39584914, 0.3964129 , 0.39591646, 0.39612252,
       0.39527854, 0.3967224 , 0.39561737, 0.3944513 , 0.39458996,
       0.39530483, 0.39534876, 0.39611274, 0.39614508, 0.3963537 ,
       0.3974802 , 0.39725605, 0.39650825, 0.39631426, 0.39643526,
       0.3963534 , 0.39745572, 0.39748487, 0.39730522, 0.3976393 ,
       0.3974684 , 0.38361302, 0.38325843, 0.3829662 , 0.38222823,
       0.38208646, 0.38152194, 0.38184747, 0.39508417, 0.3958493 ,
       0.3960084 , 0.39677948, 0.39707008, 0.39805385, 0.39797133,
       0.398146  , 0.3979274 , 0.3981482 , 0.3982943 , 0.39775142,
       0.39709768, 0.3975199 , 0.39696237, 0.39697018, 0.39614633,
       0.39575896, 0.39506224, 0.39500552, 0.39483646, 0.39509994,
       0.39496386, 0.3951774 , 0.395901  , 0.3977174 , 0.39828074,
       0.3980947 , 0.39742076, 0.39735773, 0.3975055 , 0.39713636,
       0.3957752 , 0.3956236 , 0.39594388, 0.39615256, 0.39638904,
       0.39610106, 0.39541405, 0.39583254, 0.3956193 , 0.39552328,
       0.39591616, 0.39574823, 0.39650995, 0.39687654, 0.39730257,
       0.39772654, 0.39730063, 0.39676288, 0.39714044, 0.3969261 ,
       0.3968208 , 0.3969577 , 0.39707372, 0.39757663, 0.39751622,
       0.39584392, 0.39602792, 0.39661333, 0.39611486, 0.39599937,
       0.3959738 , 0.3966735 , 0.3986516 , 0.3986199 , 0.39880386,
       0.39920926, 0.39894515, 0.3988174 , 0.3990611 , 0.3982066 ,
       0.39762053, 0.39701638, 0.39533377, 0.3955629 , 0.3946587 ,
       0.39398777, 0.39428848, 0.39436105, 0.394734  , 0.396958  ,
       0.39753887, 0.398692  , 0.39927492, 0.39948824, 0.4000674 ,
       0.39977294, 0.39827675, 0.397496  , 0.39718962, 0.3967156 ,
       0.39666376, 0.39624965, 0.39594278, 0.39694324, 0.39733604,
       0.39746103, 0.39757392, 0.39695784, 0.39683905, 0.39700142,
       0.39654854, 0.39659953, 0.39623272, 0.39616778, 0.39651287,
       0.39646953, 0.3972127 , 0.39757404, 0.39753392, 0.39795682,
       0.39759582, 0.39768583, 0.39828986, 0.3982246 , 0.3978444 ,
       0.39698502, 0.39700654, 0.3960445 , 0.39587793, 0.39556155,
       0.39531806, 0.39617696, 0.3971999 , 0.3970984 , 0.39849266,
       0.39878327, 0.39866063, 0.3986016 , 0.3974713 , 0.3973016 ],
      dtype=float32)
>>> t.shape
(200,)
150/150 [==============================] - 0s 1ms/step - loss: 0.2334
<tensorflow.python.keras.callbacks.History object at 0x7efe0d0dc8e0>
150/150 [==============================] - 0s 1ms/step - loss: 0.1198
<tensorflow.python.keras.callbacks.History object at 0x7efe100be370>
150/150 [==============================] - 0s 1ms/step - loss: 0.1115
<tensorflow.python.keras.callbacks.History object at 0x7efe1f0e6400>
150/150 [==============================] - 0s 1ms/step - loss: 0.1052
<tensorflow.python.keras.callbacks.History object at 0x7efe1ce49e20>
150/150 [==============================] - 0s 1ms/step - loss: 0.0998
<tensorflow.python.keras.callbacks.History object at 0x7efe6f74f5e0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0951
<tensorflow.python.keras.callbacks.History object at 0x7efe6f53f340>
150/150 [==============================] - 0s 1ms/step - loss: 0.0908
<tensorflow.python.keras.callbacks.History object at 0x7efe6f670cd0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0868
<tensorflow.python.keras.callbacks.History object at 0x7efe0e5402e0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0830
<tensorflow.python.keras.callbacks.History object at 0x7efe1f061610>
150/150 [==============================] - 0s 1ms/step - loss: 0.0793
<tensorflow.python.keras.callbacks.History object at 0x7efe0d5e4610>
150/150 [==============================] - 0s 1ms/step - loss: 0.0759
<tensorflow.python.keras.callbacks.History object at 0x7efe0e540ac0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0728
<tensorflow.python.keras.callbacks.History object at 0x7efe0d70d040>
150/150 [==============================] - 0s 1ms/step - loss: 0.0699
<tensorflow.python.keras.callbacks.History object at 0x7efe0ebc6df0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0672
<tensorflow.python.keras.callbacks.History object at 0x7efe0ebc6940>
150/150 [==============================] - 0s 1ms/step - loss: 0.0648
<tensorflow.python.keras.callbacks.History object at 0x7efe1127c970>
150/150 [==============================] - 0s 1ms/step - loss: 0.0625
<tensorflow.python.keras.callbacks.History object at 0x7efe0d74c790>
150/150 [==============================] - 0s 1ms/step - loss: 0.0605
<tensorflow.python.keras.callbacks.History object at 0x7efe12507f10>
150/150 [==============================] - 0s 1ms/step - loss: 0.0586
<tensorflow.python.keras.callbacks.History object at 0x7efe1c8cedc0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0569
<tensorflow.python.keras.callbacks.History object at 0x7efe1127c070>
150/150 [==============================] - 0s 1ms/step - loss: 0.0553
<tensorflow.python.keras.callbacks.History object at 0x7efe6f77adf0>
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 116, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 167, in train
    kpi = t_s.calcMetrics(y_pred, y_test)
  File "/home/sabeiro/lav//src/lernia/lernia/train_score.py", line 30, in calcMetrics
    "cor":sp.stats.pearsonr(y1,y2)[0]
  File "/usr/local/lib/python3.8/dist-packages/scipy/stats/stats.py", line 3519, in pearsonr
    xmean = x.mean(dtype=dtype)
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py", line 160, in _mean
    ret = umr_sum(arr, axis, dtype, out, keepdims)
TypeError: No loop matching the specified signature and casting was found for ufunc add
>>> y_pred = tK.predict(X1)
>>> y_pred
array([[0.2721733 ],
       [0.47350964],
       [0.5189524 ],
       [0.5331922 ],
       [0.5389548 ],
       [0.5416397 ],
       [0.542987  ],
       [0.54367304],
       [0.54403377],
       [0.5442464 ],
       [0.54435086],
       [0.544435  ],
       [0.5444593 ],
       [0.54448336],
       [0.54451287],
       [0.54453295],
       [0.5445579 ],
       [0.5445601 ],
       [0.54457176],
       [0.5446003 ],
       [0.5446042 ],
       [0.5445937 ],
       [0.54459524],
       [0.5445722 ],
       [0.54457116],
       [0.54456073],
       [0.54455197],
       [0.5445569 ],
       [0.54456717],
       [0.5445559 ],
       [0.54455876],
       [0.54454005],
       [0.54454786],
       [0.5445233 ],
       [0.54453486],
       [0.54448193],
       [0.5444874 ],
       [0.54448986],
       [0.5445131 ],
       [0.5444999 ],
       [0.5445162 ],
       [0.5444749 ],
       [0.54452556],
       [0.5445159 ],
       [0.54447776],
       [0.54446393],
       [0.54448026],
       [0.5444735 ],
       [0.54449606],
       [0.544499  ],
       [0.5444953 ],
       [0.54453224],
       [0.5445412 ],
       [0.54452467],
       [0.5445154 ],
       [0.54451764],
       [0.5445027 ],
       [0.54453313],
       [0.54454094],
       [0.54453534],
       [0.5445473 ],
       [0.54484904],
       [0.5444723 ],
       [0.5443951 ],
       [0.5443697 ],
       [0.54432994],
       [0.54432815],
       [0.54429156],
       [0.54397285],
       [0.5443213 ],
       [0.54440874],
       [0.54444426],
       [0.5444859 ],
       [0.544501  ],
       [0.5445401 ],
       [0.5445468 ],
       [0.5445591 ],
       [0.54455465],
       [0.54456073],
       [0.54457146],
       [0.5445641 ],
       [0.5445392 ],
       [0.54455286],
       [0.54453665],
       [0.5445435 ],
       [0.54452324],
       [0.5445167 ],
       [0.54449296],
       [0.54448766],
       [0.54447544],
       [0.54448193],
       [0.54447496],
       [0.5444706 ],
       [0.5444718 ],
       [0.54452264],
       [0.54455173],
       [0.5445616 ],
       [0.54454696],
       [0.54454243],
       [0.54454947],
       [0.54455525],
       [0.54451656],
       [0.5445001 ],
       [0.54450166],
       [0.5445036 ],
       [0.54451436],
       [0.54451656],
       [0.54449075],
       [0.54450154],
       [0.5444968 ],
       [0.5444879 ],
       [0.5445001 ],
       [0.5444874 ],
       [0.5445049 ],
       [0.54451466],
       [0.5445266 ],
       [0.5445472 ],
       [0.54454523],
       [0.54452604],
       [0.5445366 ],
       [0.5445322 ],
       [0.5445268 ],
       [0.54452854],
       [0.5445268 ],
       [0.5445425 ],
       [0.5445628 ],
       [0.54451394],
       [0.5445043 ],
       [0.5445237 ],
       [0.5445121 ],
       [0.5445072 ],
       [0.5444965 ],
       [0.5444966 ],
       [0.5445565 ],
       [0.5445657 ],
       [0.54457414],
       [0.5445919 ],
       [0.5445895 ],
       [0.5445851 ],
       [0.54459894],
       [0.54458094],
       [0.54456663],
       [0.5445652 ],
       [0.54450834],
       [0.54451585],
       [0.54449534],
       [0.5444641 ],
       [0.54446393],
       [0.5444579 ],
       [0.54443926],
       [0.5444979 ],
       [0.5445164 ],
       [0.5445547 ],
       [0.54458094],
       [0.54459256],
       [0.54461676],
       [0.5446252 ],
       [0.5445911 ],
       [0.5445654 ],
       [0.5445544 ],
       [0.54453576],
       [0.5445337 ],
       [0.54452246],
       [0.54449815],
       [0.5445189 ],
       [0.544532  ],
       [0.54453796],
       [0.54455024],
       [0.5445359 ],
       [0.5445287 ],
       [0.5445367 ],
       [0.54452306],
       [0.5445265 ],
       [0.5445158 ],
       [0.54450744],
       [0.54451615],
       [0.54450756],
       [0.54452544],
       [0.54454005],
       [0.5445388 ],
       [0.54455626],
       [0.5445482 ],
       [0.54454523],
       [0.5445636 ],
       [0.5445683 ],
       [0.5445675 ],
       [0.5445416 ],
       [0.5445485 ],
       [0.5445205 ],
       [0.5445135 ],
       [0.54450357],
       [0.54448164],
       [0.5444911 ],
       [0.54452384],
       [0.5445146 ],
       [0.5445553 ],
       [0.5445732 ],
       [0.5445762 ],
       [0.54458755],
       [0.5445579 ]], dtype=float32)
>>> y_pred[:,0]
array([0.2721733 , 0.47350964, 0.5189524 , 0.5331922 , 0.5389548 ,
       0.5416397 , 0.542987  , 0.54367304, 0.54403377, 0.5442464 ,
       0.54435086, 0.544435  , 0.5444593 , 0.54448336, 0.54451287,
       0.54453295, 0.5445579 , 0.5445601 , 0.54457176, 0.5446003 ,
       0.5446042 , 0.5445937 , 0.54459524, 0.5445722 , 0.54457116,
       0.54456073, 0.54455197, 0.5445569 , 0.54456717, 0.5445559 ,
       0.54455876, 0.54454005, 0.54454786, 0.5445233 , 0.54453486,
       0.54448193, 0.5444874 , 0.54448986, 0.5445131 , 0.5444999 ,
       0.5445162 , 0.5444749 , 0.54452556, 0.5445159 , 0.54447776,
       0.54446393, 0.54448026, 0.5444735 , 0.54449606, 0.544499  ,
       0.5444953 , 0.54453224, 0.5445412 , 0.54452467, 0.5445154 ,
       0.54451764, 0.5445027 , 0.54453313, 0.54454094, 0.54453534,
       0.5445473 , 0.54484904, 0.5444723 , 0.5443951 , 0.5443697 ,
       0.54432994, 0.54432815, 0.54429156, 0.54397285, 0.5443213 ,
       0.54440874, 0.54444426, 0.5444859 , 0.544501  , 0.5445401 ,
       0.5445468 , 0.5445591 , 0.54455465, 0.54456073, 0.54457146,
       0.5445641 , 0.5445392 , 0.54455286, 0.54453665, 0.5445435 ,
       0.54452324, 0.5445167 , 0.54449296, 0.54448766, 0.54447544,
       0.54448193, 0.54447496, 0.5444706 , 0.5444718 , 0.54452264,
       0.54455173, 0.5445616 , 0.54454696, 0.54454243, 0.54454947,
       0.54455525, 0.54451656, 0.5445001 , 0.54450166, 0.5445036 ,
       0.54451436, 0.54451656, 0.54449075, 0.54450154, 0.5444968 ,
       0.5444879 , 0.5445001 , 0.5444874 , 0.5445049 , 0.54451466,
       0.5445266 , 0.5445472 , 0.54454523, 0.54452604, 0.5445366 ,
       0.5445322 , 0.5445268 , 0.54452854, 0.5445268 , 0.5445425 ,
       0.5445628 , 0.54451394, 0.5445043 , 0.5445237 , 0.5445121 ,
       0.5445072 , 0.5444965 , 0.5444966 , 0.5445565 , 0.5445657 ,
       0.54457414, 0.5445919 , 0.5445895 , 0.5445851 , 0.54459894,
       0.54458094, 0.54456663, 0.5445652 , 0.54450834, 0.54451585,
       0.54449534, 0.5444641 , 0.54446393, 0.5444579 , 0.54443926,
       0.5444979 , 0.5445164 , 0.5445547 , 0.54458094, 0.54459256,
       0.54461676, 0.5446252 , 0.5445911 , 0.5445654 , 0.5445544 ,
       0.54453576, 0.5445337 , 0.54452246, 0.54449815, 0.5445189 ,
       0.544532  , 0.54453796, 0.54455024, 0.5445359 , 0.5445287 ,
       0.5445367 , 0.54452306, 0.5445265 , 0.5445158 , 0.54450744,
       0.54451615, 0.54450756, 0.54452544, 0.54454005, 0.5445388 ,
       0.54455626, 0.5445482 , 0.54454523, 0.5445636 , 0.5445683 ,
       0.5445675 , 0.5445416 , 0.5445485 , 0.5445205 , 0.5445135 ,
       0.54450357, 0.54448164, 0.5444911 , 0.54452384, 0.5445146 ,
       0.5445553 , 0.5445732 , 0.5445762 , 0.54458755, 0.5445579 ],
      dtype=float32)
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 116, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 157, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 152, in splitSet
    y_test = y_test.reshape(len(y_train),-1)
ValueError: cannot reshape array of size 50 into shape (150,newaxis)
150/150 [==============================] - 0s 1ms/step - loss: 0.2494
<tensorflow.python.keras.callbacks.History object at 0x7efe0e2496d0>
150/150 [==============================] - 0s 1ms/step - loss: 0.1079
<tensorflow.python.keras.callbacks.History object at 0x7efe0e5b2370>
150/150 [==============================] - 0s 1ms/step - loss: 0.0983
<tensorflow.python.keras.callbacks.History object at 0x7efe12acfd00>
150/150 [==============================] - 0s 1ms/step - loss: 0.0910
<tensorflow.python.keras.callbacks.History object at 0x7efe6ff1c340>
150/150 [==============================] - 0s 1ms/step - loss: 0.0850
<tensorflow.python.keras.callbacks.History object at 0x7efe10468ca0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0799
<tensorflow.python.keras.callbacks.History object at 0x7efe104689d0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0756
<tensorflow.python.keras.callbacks.History object at 0x7efe0ec4b520>
150/150 [==============================] - 0s 1ms/step - loss: 0.0718
<tensorflow.python.keras.callbacks.History object at 0x7efe10459b20>
150/150 [==============================] - 0s 1ms/step - loss: 0.0685
<tensorflow.python.keras.callbacks.History object at 0x7efe128192e0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0656
<tensorflow.python.keras.callbacks.History object at 0x7efe104597f0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0630
<tensorflow.python.keras.callbacks.History object at 0x7efe12accd00>
150/150 [==============================] - 0s 1ms/step - loss: 0.0608
<tensorflow.python.keras.callbacks.History object at 0x7efe10448280>
150/150 [==============================] - 0s 1ms/step - loss: 0.0587
<tensorflow.python.keras.callbacks.History object at 0x7efe11483a30>
150/150 [==============================] - 0s 1ms/step - loss: 0.0569
<tensorflow.python.keras.callbacks.History object at 0x7efe12823520>
150/150 [==============================] - 0s 1ms/step - loss: 0.0553
<tensorflow.python.keras.callbacks.History object at 0x7efe10461b20>
150/150 [==============================] - 0s 1ms/step - loss: 0.0538
<tensorflow.python.keras.callbacks.History object at 0x7efe111f2670>
150/150 [==============================] - 0s 1ms/step - loss: 0.0524
<tensorflow.python.keras.callbacks.History object at 0x7efe1116acd0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0512
<tensorflow.python.keras.callbacks.History object at 0x7efe12acc160>
150/150 [==============================] - 0s 1ms/step - loss: 0.0500
<tensorflow.python.keras.callbacks.History object at 0x7efe6ff11460>
150/150 [==============================] - 0s 1ms/step - loss: 0.0490
<tensorflow.python.keras.callbacks.History object at 0x7efe1042ed60>
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 126, in <module>
    
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 147, in plotHistory
    val_loss = np.concatenate([x.history['val_loss'] for x in history])
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 147, in <listcomp>
    val_loss = np.concatenate([x.history['val_loss'] for x in history])
KeyError: 'val_loss'
>>> 
150/150 [==============================] - 1s 4ms/step - loss: 0.2035 - val_loss: 0.0170
<tensorflow.python.keras.callbacks.History object at 0x7efe11c154f0>
150/150 [==============================] - 0s 1ms/step - loss: 0.1337 - val_loss: 0.0161
<tensorflow.python.keras.callbacks.History object at 0x7efe12995970>
150/150 [==============================] - 0s 2ms/step - loss: 0.1191 - val_loss: 0.0153
<tensorflow.python.keras.callbacks.History object at 0x7efe0dfe02e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.1087 - val_loss: 0.0147
<tensorflow.python.keras.callbacks.History object at 0x7efe0dd951c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.1007 - val_loss: 0.0140
<tensorflow.python.keras.callbacks.History object at 0x7efe135e5310>
150/150 [==============================] - 0s 2ms/step - loss: 0.0941 - val_loss: 0.0134
<tensorflow.python.keras.callbacks.History object at 0x7efe0e4bbdf0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0886 - val_loss: 0.0129
<tensorflow.python.keras.callbacks.History object at 0x7efe0dd45c40>
150/150 [==============================] - 0s 2ms/step - loss: 0.0838 - val_loss: 0.0123
<tensorflow.python.keras.callbacks.History object at 0x7efe0dd9c9a0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0796 - val_loss: 0.0118
<tensorflow.python.keras.callbacks.History object at 0x7efe0dd95af0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0758 - val_loss: 0.0113
<tensorflow.python.keras.callbacks.History object at 0x7efe104f5370>
150/150 [==============================] - 0s 2ms/step - loss: 0.0723 - val_loss: 0.0108
<tensorflow.python.keras.callbacks.History object at 0x7efe129ac3d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0692 - val_loss: 0.0104
<tensorflow.python.keras.callbacks.History object at 0x7efe6ffd2370>
150/150 [==============================] - 0s 2ms/step - loss: 0.0663 - val_loss: 0.0100
<tensorflow.python.keras.callbacks.History object at 0x7efe490d0520>
150/150 [==============================] - 0s 2ms/step - loss: 0.0637 - val_loss: 0.0096
<tensorflow.python.keras.callbacks.History object at 0x7efe1ed21460>
150/150 [==============================] - 0s 2ms/step - loss: 0.0613 - val_loss: 0.0092
<tensorflow.python.keras.callbacks.History object at 0x7efe0d488490>
150/150 [==============================] - 0s 2ms/step - loss: 0.0591 - val_loss: 0.0088
<tensorflow.python.keras.callbacks.History object at 0x7efe0e32c3d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0572 - val_loss: 0.0085
<tensorflow.python.keras.callbacks.History object at 0x7efe490bdf10>
150/150 [==============================] - 0s 2ms/step - loss: 0.0554 - val_loss: 0.0081
<tensorflow.python.keras.callbacks.History object at 0x7efe0f339970>
150/150 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.0078
<tensorflow.python.keras.callbacks.History object at 0x7efe0e345bb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0523 - val_loss: 0.0075
<tensorflow.python.keras.callbacks.History object at 0x7efe12b4bf10>
150/150 [==============================] - 1s 4ms/step - loss: 0.3193 - val_loss: 0.0381
<tensorflow.python.keras.callbacks.History object at 0x7efe0cea3670>
150/150 [==============================] - 0s 2ms/step - loss: 0.1356 - val_loss: 0.0186
<tensorflow.python.keras.callbacks.History object at 0x7efe08aac400>
150/150 [==============================] - 0s 2ms/step - loss: 0.1251 - val_loss: 0.0176
<tensorflow.python.keras.callbacks.History object at 0x7efe0809f340>
150/150 [==============================] - 0s 2ms/step - loss: 0.1176 - val_loss: 0.0170
<tensorflow.python.keras.callbacks.History object at 0x7efe07fb1eb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.1113 - val_loss: 0.0164
<tensorflow.python.keras.callbacks.History object at 0x7efe0809ffd0>
150/150 [==============================] - 0s 2ms/step - loss: 0.1058 - val_loss: 0.0160
<tensorflow.python.keras.callbacks.History object at 0x7efe07f3b880>
150/150 [==============================] - 0s 2ms/step - loss: 0.1008 - val_loss: 0.0155
<tensorflow.python.keras.callbacks.History object at 0x7efe08080eb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.0151
<tensorflow.python.keras.callbacks.History object at 0x7efe07eb10d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0921 - val_loss: 0.0147
<tensorflow.python.keras.callbacks.History object at 0x7efe07f54880>
150/150 [==============================] - 0s 2ms/step - loss: 0.0882 - val_loss: 0.0143
<tensorflow.python.keras.callbacks.History object at 0x7efe07e6c730>
150/150 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.0139
<tensorflow.python.keras.callbacks.History object at 0x7efe07f6bc10>
150/150 [==============================] - 0s 2ms/step - loss: 0.0811 - val_loss: 0.0134
<tensorflow.python.keras.callbacks.History object at 0x7efe07e410a0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0778 - val_loss: 0.0130
<tensorflow.python.keras.callbacks.History object at 0x7efe08066eb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0747 - val_loss: 0.0125
<tensorflow.python.keras.callbacks.History object at 0x7efe07ee76d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0719 - val_loss: 0.0120
<tensorflow.python.keras.callbacks.History object at 0x7efe07f01fa0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0692 - val_loss: 0.0116
<tensorflow.python.keras.callbacks.History object at 0x7efe07e77700>
150/150 [==============================] - 0s 2ms/step - loss: 0.0667 - val_loss: 0.0111
<tensorflow.python.keras.callbacks.History object at 0x7efe07ddac40>
150/150 [==============================] - 0s 2ms/step - loss: 0.0644 - val_loss: 0.0106
<tensorflow.python.keras.callbacks.History object at 0x7efe07ee6460>
150/150 [==============================] - 0s 2ms/step - loss: 0.0622 - val_loss: 0.0101
<tensorflow.python.keras.callbacks.History object at 0x7efe0cf3ba60>
150/150 [==============================] - 0s 2ms/step - loss: 0.0603 - val_loss: 0.0097
<tensorflow.python.keras.callbacks.History object at 0x7efe07f3bb20>
150/150 [==============================] - 0s 2ms/step - loss: 0.0585 - val_loss: 0.0093
<tensorflow.python.keras.callbacks.History object at 0x7efe12f80880>
150/150 [==============================] - 0s 2ms/step - loss: 0.0568 - val_loss: 0.0089
<tensorflow.python.keras.callbacks.History object at 0x7efe12f813d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0552 - val_loss: 0.0085
<tensorflow.python.keras.callbacks.History object at 0x7efe6f6c2820>
150/150 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.0082
<tensorflow.python.keras.callbacks.History object at 0x7efe6fd7ba30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0525 - val_loss: 0.0078
<tensorflow.python.keras.callbacks.History object at 0x7efe105867f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0513 - val_loss: 0.0075
<tensorflow.python.keras.callbacks.History object at 0x7efe0ea35280>
150/150 [==============================] - 0s 2ms/step - loss: 0.0501 - val_loss: 0.0073
<tensorflow.python.keras.callbacks.History object at 0x7efe1106da00>
150/150 [==============================] - 0s 2ms/step - loss: 0.0490 - val_loss: 0.0070
<tensorflow.python.keras.callbacks.History object at 0x7efe6fffba90>
150/150 [==============================] - 0s 2ms/step - loss: 0.0480 - val_loss: 0.0068
<tensorflow.python.keras.callbacks.History object at 0x7efe147edee0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0471 - val_loss: 0.0065
<tensorflow.python.keras.callbacks.History object at 0x7efe0dce2190>
150/150 [==============================] - 0s 2ms/step - loss: 0.0462 - val_loss: 0.0063
<tensorflow.python.keras.callbacks.History object at 0x7efe1119dc10>
150/150 [==============================] - 0s 2ms/step - loss: 0.0454 - val_loss: 0.0061
<tensorflow.python.keras.callbacks.History object at 0x7efe1f2044f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0446 - val_loss: 0.0060
<tensorflow.python.keras.callbacks.History object at 0x7efe6f76b580>
150/150 [==============================] - 0s 1ms/step - loss: 0.0439 - val_loss: 0.0058
<tensorflow.python.keras.callbacks.History object at 0x7efe0d76ef10>
150/150 [==============================] - 0s 2ms/step - loss: 0.0432 - val_loss: 0.0057
<tensorflow.python.keras.callbacks.History object at 0x7efe0d5dc370>
150/150 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.0056
<tensorflow.python.keras.callbacks.History object at 0x7efe4903e280>
150/150 [==============================] - 0s 2ms/step - loss: 0.0419 - val_loss: 0.0055
<tensorflow.python.keras.callbacks.History object at 0x7efe12823370>
150/150 [==============================] - 0s 2ms/step - loss: 0.0413 - val_loss: 0.0054
<tensorflow.python.keras.callbacks.History object at 0x7efe1137d1c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.0053
<tensorflow.python.keras.callbacks.History object at 0x7efe0e5b2d30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0402 - val_loss: 0.0052
<tensorflow.python.keras.callbacks.History object at 0x7efe129ad040>
150/150 [==============================] - 0s 2ms/step - loss: 0.0397 - val_loss: 0.0052
<tensorflow.python.keras.callbacks.History object at 0x7efe48f630d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0392 - val_loss: 0.0051
<tensorflow.python.keras.callbacks.History object at 0x7efe12f81730>
150/150 [==============================] - 0s 2ms/step - loss: 0.0387 - val_loss: 0.0051
<tensorflow.python.keras.callbacks.History object at 0x7efe6f57b580>
150/150 [==============================] - 0s 2ms/step - loss: 0.0383 - val_loss: 0.0050
<tensorflow.python.keras.callbacks.History object at 0x7efe10586940>
150/150 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 0.0050
<tensorflow.python.keras.callbacks.History object at 0x7efe0d49f3a0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0375 - val_loss: 0.0050
<tensorflow.python.keras.callbacks.History object at 0x7efe6ffca4f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 0.0050
<tensorflow.python.keras.callbacks.History object at 0x7efe12819580>
150/150 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.0049
<tensorflow.python.keras.callbacks.History object at 0x7efe0d7e6490>
150/150 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.0049
<tensorflow.python.keras.callbacks.History object at 0x7efea032de50>
150/150 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.0049
<tensorflow.python.keras.callbacks.History object at 0x7efe0dc40a60>
150/150 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0049
<tensorflow.python.keras.callbacks.History object at 0x7efe8c1b7c10>
150/150 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 0.0049
<tensorflow.python.keras.callbacks.History object at 0x7efe8d3b3670>
150/150 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 0.0049
<tensorflow.python.keras.callbacks.History object at 0x7efe0dc85b50>
150/150 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.0049
<tensorflow.python.keras.callbacks.History object at 0x7efe1204a160>
150/150 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0049
<tensorflow.python.keras.callbacks.History object at 0x7efe0d70a550>
150/150 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 0.0049
<tensorflow.python.keras.callbacks.History object at 0x7efe6f773b50>
150/150 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe10586130>
150/150 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe49057790>
150/150 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe0e9e8d30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe1036a910>
150/150 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe6f53f100>
150/150 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe32453970>
150/150 [==============================] - 0s 2ms/step - loss: 0.0321 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe14798ee0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe6f808250>
150/150 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe0e41fb80>
150/150 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe12dfd1f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0312 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe100fc070>
150/150 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe0cfb6c40>
150/150 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe12b5c280>
150/150 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe0f3e3730>
150/150 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe0d4f9910>
150/150 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe0e910820>
150/150 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe11483190>
150/150 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.0048
<tensorflow.python.keras.callbacks.History object at 0x7efe07f54f40>
150/150 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe080669d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe11483c70>
150/150 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe4914d8b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe0e9e86a0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe4910b130>
150/150 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe07f2d190>
150/150 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe1c985850>
150/150 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe0d737850>
150/150 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe12002820>
150/150 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe07f2d430>
150/150 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe1f60f5b0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0285 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe8c4675b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe11bf5940>
150/150 [==============================] - 0s 1ms/step - loss: 0.0284 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe0d73e5e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe0dcf4dc0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe11483be0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe0dcccdc0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0280 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe8c4445e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0047
<tensorflow.python.keras.callbacks.History object at 0x7efe49060c70>
150/150 [==============================] - 0s 2ms/step - loss: 0.0279 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe49035820>
150/150 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe07f2d880>
150/150 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe48c7b1c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe48f61f40>
150/150 [==============================] - 0s 2ms/step - loss: 0.0276 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe1117d0d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe48f6beb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe6f42d0a0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe1e5f5eb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efea0038070>
150/150 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe48f6bc40>
150/150 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe0e31eb50>
150/150 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe0ffa6070>
150/150 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe6d937190>
150/150 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe493cdd60>
150/150 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe6d906ee0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe07f2d6d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe6d92a2b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe0d73eaf0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe0e231e20>
150/150 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe493c0ee0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0046
<tensorflow.python.keras.callbacks.History object at 0x7efe493bc760>
150/150 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe48dc1250>
150/150 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe0d74a3d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe4959d220>
150/150 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe4905cdf0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe130f31c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe6fce0bb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe0dfd5190>
150/150 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe140be190>
150/150 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe0d4af160>
150/150 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe49180190>
150/150 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe1cdbc130>
150/150 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe1cdd69a0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe6ef058e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe49180e20>
150/150 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe140b16a0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0259 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe1cdd62b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe140e0490>
150/150 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe12b722e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe4917db80>
150/150 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe14405790>
150/150 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe6ef05580>
150/150 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe1052b760>
150/150 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe490ea4f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0045
<tensorflow.python.keras.callbacks.History object at 0x7efe11bc0730>
150/150 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe0a0fe310>
150/150 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe0e5be700>
150/150 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe0ec38730>
150/150 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe12d4d3d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe0f35ca30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe10757460>
150/150 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe112dc8e0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0253 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe1ecfea60>
150/150 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe0d446ca0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe0d529bb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe1ed1ed30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe1075ba90>
150/150 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe0d601280>
150/150 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe112dcc10>
150/150 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe1f02aa30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe0f413e20>
150/150 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe12c5deb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe112dc100>
150/150 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe0e416d90>
150/150 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe128c7730>
150/150 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe6fd01b20>
150/150 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe0d818be0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe1f0cfb50>
150/150 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.0044
<tensorflow.python.keras.callbacks.History object at 0x7efe12f8ad60>
150/150 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe1147db20>
150/150 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe0e16d520>
150/150 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe0e159bb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe12f8abe0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe12d024c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe110775b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe12ddde20>
150/150 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe0e90ba60>
150/150 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe12d030a0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe0e3714f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe12d033a0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe12bfb100>
150/150 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe12d03af0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe11228bb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe1ee98910>
150/150 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe0ec58340>
150/150 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe0e466400>
150/150 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe0e258310>
150/150 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe08043250>
150/150 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0043
<tensorflow.python.keras.callbacks.History object at 0x7efe10ecf2e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe102e28b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe10eef160>
150/150 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0cef1d60>
150/150 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe6f8e17f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe080436d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0e4f8550>
150/150 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0ce36430>
150/150 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0e3864f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe1ee48670>
150/150 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0e46afa0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe1f519970>
150/150 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0e3868e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0cc73610>
150/150 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0803b5b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0ce6c5e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0ed3e580>
150/150 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe10119760>
150/150 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe6ff91610>
150/150 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe15096190>
150/150 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0f02d070>
150/150 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe15083eb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0cbdceb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0042
<tensorflow.python.keras.callbacks.History object at 0x7efe0cc25070>
150/150 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe0cb5dc40>
150/150 [==============================] - 0s 1ms/step - loss: 0.0233 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe0cc23ca0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe096a5d00>
150/150 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe15041460>
150/150 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe0966b8b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe0cb14af0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe09685520>
150/150 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe0cb23a30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe09627d30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe150f9940>
150/150 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe0cb14850>
150/150 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe0cb23730>
150/150 [==============================] - 0s 1ms/step - loss: 0.0230 - val_loss: 0.0041
<tensorflow.python.keras.callbacks.History object at 0x7efe15041070>
150/150 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe0952caf0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe0cb83820>
150/150 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe094a3ac0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe1509a820>
150/150 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe09430a90>
150/150 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe0cba0f40>
150/150 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe093b1a60>
150/150 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe09502cd0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe09332a30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe09404a60>
150/150 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe092b4e20>
150/150 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe095012e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe0927a460>
150/150 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe0938d0d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe092138e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe095025e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe0928a880>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe093f3fa0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe091ea190>
150/150 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe1509a3a0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe09366a90>
150/150 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe091f5b50>
150/150 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe093fc580>
150/150 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe090ff400>
150/150 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe0cb19b20>
150/150 [==============================] - 0s 1ms/step - loss: 0.0309 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe09081370>
150/150 [==============================] - 0s 1ms/step - loss: 0.0311 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe093e5c10>
150/150 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0040
<tensorflow.python.keras.callbacks.History object at 0x7efe09003d30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0246 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe090557f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08f158e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08fb0430>
150/150 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08f090d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08f398e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08e882b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08ed2e50>
150/150 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08e162b0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0232 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08ecd7f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08eb5790>
150/150 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe093e5a00>
150/150 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08ea9790>
150/150 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08e9fc10>
150/150 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08e9fdf0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08e08820>
150/150 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08fb0fa0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe07d23f40>
150/150 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08d8c4c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe07ca4ee0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe08f37c10>
150/150 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe07c27d00>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe07c6bca0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe07babca0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe07d37a90>
150/150 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe07b2aaf0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe07be38e0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0252 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe07a2eac0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe07ced640>
150/150 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe07ad6940>
150/150 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0039
<tensorflow.python.keras.callbacks.History object at 0x7efe07be3490>
150/150 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe07ade1f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe093f38b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe094302b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe08e08d90>
150/150 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0928aa90>
150/150 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0ed272b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe154e5d00>
150/150 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1112c5b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe12bfba30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe6f8e1d00>
150/150 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0e5be040>
150/150 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe09055f40>
150/150 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe6fce0a30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0d818a00>
150/150 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe490351c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe09055640>
150/150 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe48b31e50>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe128c7370>
150/150 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0e047610>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1c985310>
150/150 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0cfb68e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1f60f130>
150/150 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0e32c790>
150/150 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1cdd67c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0f44c700>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe11f3dfd0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe6d937b80>
150/150 [==============================] - 0s 2ms/step - loss: 0.0225 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe17f2b310>
150/150 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1f02aca0>
150/150 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe112d2970>
150/150 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe07d23670>
150/150 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1404ef10>
150/150 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe07d23ac0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0d62d550>
150/150 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe8c0f40a0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe6fcf6af0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0d55ed00>
150/150 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe122281c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe133659d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1288d040>
150/150 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe128b46a0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe10459a00>
150/150 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe33762160>
150/150 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe4903ea30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe12d21d30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0d49f0d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1edba280>
150/150 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0dc58670>
150/150 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0ed997c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0dc58640>
150/150 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe6ee44160>
150/150 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0d49f8b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe07f01be0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0e41f0a0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe107a7760>
150/150 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0dddd9d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe100fcdc0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe129a4550>
150/150 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe10e44640>
150/150 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe12f81940>
150/150 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0809f9d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe112e14f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe08f39310>
150/150 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe08f09e80>
150/150 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe6f7d4670>
150/150 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1117d2b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0dddd670>
150/150 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0d488430>
150/150 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe8c0f4d30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe4905f670>
150/150 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0f3ce280>
150/150 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0d557820>
150/150 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0e4f8d30>
150/150 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe121bfac0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0d4870d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1f0c4460>
150/150 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0dddd7f0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1471ac40>
150/150 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe4910ab80>
150/150 [==============================] - 0s 1ms/step - loss: 0.0220 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1effa070>
150/150 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe13537ee0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0e79fc10>
150/150 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0e79de20>
150/150 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1471a550>
150/150 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe07bf3970>
150/150 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe13537430>
150/150 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe48f36640>
150/150 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe07d37970>
150/150 [==============================] - 0s 1ms/step - loss: 0.0231 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1e612940>
150/150 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe13537b80>
150/150 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1cf8e5e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0db971c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe13001730>
150/150 [==============================] - 0s 1ms/step - loss: 0.0219 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe145b15e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1c205bb0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe145d3700>
150/150 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe14861790>
150/150 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe11bf74c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1c23b9d0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe49144ac0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1357dc40>
150/150 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe135945e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe08e98f40>
150/150 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1edad910>
150/150 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe08e98910>
150/150 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe6d9118e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1353e400>
150/150 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe490bde20>
150/150 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0db97b20>
150/150 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe1454dac0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0ddbce20>
150/150 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0cf9ec10>
150/150 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe8c1d2ac0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0e182d90>
150/150 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0ddc0220>
150/150 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe8c1bb2b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe10fd6a00>
150/150 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0e150040>
150/150 [==============================] - 0s 1ms/step - loss: 0.0227 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe6f449520>
150/150 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe111761c0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe11fe65e0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe07b2ae50>
150/150 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe0e3422b0>
150/150 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0038
<tensorflow.python.keras.callbacks.History object at 0x7efe49173f10>
>>> X
                         room_jitter  vehicle_jitter  ...  ram_usage  cpu_usage
deci                                                  ...                      
2020-07-02 08:04:53.800     1.609438        2.351375  ...   7.524115  68.403366
2020-07-02 08:05:40.300     1.609438        2.351375  ...   7.524115  68.403366
2020-07-02 08:05:57.700     1.686399        2.174752  ...   7.524115  68.403366
2020-07-02 08:05:59.200     1.686399        2.054124  ...   7.525381  68.403366
2020-07-02 08:05:59.400     1.686399        1.131403  ...   7.525381  68.403366
...                              ...             ...  ...        ...        ...
2020-07-02 08:51:19.600     2.411440        2.379546  ...   7.729978  66.334991
2020-07-02 08:52:28.300     2.766319        2.379546  ...   7.749253  66.334991
2020-07-02 08:53:23.800     2.766319        2.379546  ...   7.749253  66.334991
2020-07-02 08:53:47.900     2.766319        2.379546  ...   7.749253  66.334991
2020-07-02 08:54:03.400     2.766319        2.379546  ...   7.749253  66.334991

[29546 rows x 11 columns]
>>> xL
['room_jitter', 'vehicle_jitter', 'camera_jitter', 'vehicle_ping', 'joystick_freq', 'wheel_speed', 'radar_speed', 'force_lateral', 'force_longitudinal', 'ram_usage', 'cpu_usage']
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 116, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=200)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 158, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 141, in splitSet
    y = np.array(y1)[shuffleL]
IndexError: index 200 is out of bounds for axis 0 with size 200
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 116, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=200)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 160, in train
    history = self.model.fit(X_train,y_train
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 505, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2657, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:531 train_step  **
        y_pred = self(x, training=True)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__
        input_spec.assert_input_compatibility(self.input_spec, inputs,
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/input_spec.py:224 assert_input_compatibility
        raise ValueError('Input ' + str(input_index) +

    ValueError: Input 0 is incompatible with layer sequential_75: expected shape=(1, None, 1), found shape=[1, 1, 11]

>>> X
                         room_jitter  vehicle_jitter  ...  ram_usage  cpu_usage
deci                                                  ...                      
2020-07-02 08:04:53.800     1.609438        2.351375  ...   7.524115  68.403366
2020-07-02 08:05:40.300     1.609438        2.351375  ...   7.524115  68.403366
2020-07-02 08:05:57.700     1.686399        2.174752  ...   7.524115  68.403366
2020-07-02 08:05:59.200     1.686399        2.054124  ...   7.525381  68.403366
2020-07-02 08:05:59.400     1.686399        1.131403  ...   7.525381  68.403366
...                              ...             ...  ...        ...        ...
2020-07-02 08:51:19.600     2.411440        2.379546  ...   7.729978  66.334991
2020-07-02 08:52:28.300     2.766319        2.379546  ...   7.749253  66.334991
2020-07-02 08:53:23.800     2.766319        2.379546  ...   7.749253  66.334991
2020-07-02 08:53:47.900     2.766319        2.379546  ...   7.749253  66.334991
2020-07-02 08:54:03.400     2.766319        2.379546  ...   7.749253  66.334991

[29546 rows x 11 columns]
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 113, in <module>
    X1 = np.array(y1.reshape(-1,1))
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py", line 5274, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'DataFrame' object has no attribute 'reshape'
>>> y1
                         room_jitter  vehicle_jitter
deci                                                
2020-07-02 08:04:53.800     1.609438        2.351375
2020-07-02 08:05:40.300     1.609438        2.351375
2020-07-02 08:05:57.700     1.686399        2.174752
2020-07-02 08:05:59.200     1.686399        2.054124
2020-07-02 08:05:59.400     1.686399        1.131403
...                              ...             ...
2020-07-02 08:15:09.200     2.397895        1.131403
2020-07-02 08:15:11.900     2.397895        1.131403
2020-07-02 08:15:14.000     2.370244        0.336474
2020-07-02 08:15:44.600     2.297573        2.028148
2020-07-02 08:16:46.400     1.983298        1.960095

[200 rows x 2 columns]
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 117, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=200)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 160, in train
    history = self.model.fit(X_train,y_train
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 505, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2657, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:531 train_step  **
        y_pred = self(x, training=True)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__
        input_spec.assert_input_compatibility(self.input_spec, inputs,
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/input_spec.py:224 assert_input_compatibility
        raise ValueError('Input ' + str(input_index) +

    ValueError: Input 0 is incompatible with layer sequential_76: expected shape=(1, None, 1), found shape=[1, 1, 2]

>>> (200, 2)
>>> tK.printModel()
Model: "sequential_77"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_77 (LSTM)               (1, 4)                    96        
_________________________________________________________________
dense_77 (Dense)             (1, 1)                    5         
=================================================================
Total params: 101
Trainable params: 101
Non-trainable params: 0
_________________________________________________________________
None
>>> (200, 2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 117, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=200)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 161, in train
    history = self.model.fit(X_train,y_train
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 505, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2657, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:531 train_step  **
        y_pred = self(x, training=True)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__
        input_spec.assert_input_compatibility(self.input_spec, inputs,
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/input_spec.py:224 assert_input_compatibility
        raise ValueError('Input ' + str(input_index) +

    ValueError: Input 0 is incompatible with layer sequential_78: expected shape=(1, None, 1), found shape=[1, 1, 2]

>>> tK.printModel()
Model: "sequential_78"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_78 (LSTM)               (1, 4)                    96        
_________________________________________________________________
dense_78 (Dense)             (1, 2)                    10        
=================================================================
Total params: 106
Trainable params: 106
Non-trainable params: 0
_________________________________________________________________
None
>>> (200, 2)
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_79_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_79_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_79_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
150/150 - 1s - loss: 0.1774 - val_loss: 0.0373
150/150 - 1s - loss: 0.0781 - val_loss: 0.0181
150/150 - 0s - loss: 0.0732 - val_loss: 0.0170
150/150 - 0s - loss: 0.0711 - val_loss: 0.0161
150/150 - 0s - loss: 0.0688 - val_loss: 0.0152
150/150 - 0s - loss: 0.0665 - val_loss: 0.0144
150/150 - 0s - loss: 0.0643 - val_loss: 0.0136
150/150 - 0s - loss: 0.0621 - val_loss: 0.0128
150/150 - 0s - loss: 0.0598 - val_loss: 0.0120
150/150 - 0s - loss: 0.0574 - val_loss: 0.0111
150/150 - 0s - loss: 0.0550 - val_loss: 0.0102
150/150 - 0s - loss: 0.0527 - val_loss: 0.0094
150/150 - 0s - loss: 0.0504 - val_loss: 0.0088
150/150 - 0s - loss: 0.0483 - val_loss: 0.0082
150/150 - 0s - loss: 0.0463 - val_loss: 0.0078
150/150 - 0s - loss: 0.0446 - val_loss: 0.0074
150/150 - 0s - loss: 0.0429 - val_loss: 0.0072
150/150 - 0s - loss: 0.0415 - val_loss: 0.0070
150/150 - 0s - loss: 0.0401 - val_loss: 0.0068
150/150 - 0s - loss: 0.0389 - val_loss: 0.0067
150/150 - 0s - loss: 0.0378 - val_loss: 0.0067
150/150 - 0s - loss: 0.0367 - val_loss: 0.0067
150/150 - 0s - loss: 0.0358 - val_loss: 0.0067
150/150 - 0s - loss: 0.0349 - val_loss: 0.0067
150/150 - 0s - loss: 0.0341 - val_loss: 0.0068
150/150 - 0s - loss: 0.0334 - val_loss: 0.0068
150/150 - 0s - loss: 0.0327 - val_loss: 0.0069
150/150 - 0s - loss: 0.0321 - val_loss: 0.0070
150/150 - 0s - loss: 0.0316 - val_loss: 0.0071
150/150 - 0s - loss: 0.0310 - val_loss: 0.0072
150/150 - 0s - loss: 0.0306 - val_loss: 0.0073
150/150 - 0s - loss: 0.0301 - val_loss: 0.0074
150/150 - 0s - loss: 0.0297 - val_loss: 0.0076
150/150 - 0s - loss: 0.0293 - val_loss: 0.0077
150/150 - 0s - loss: 0.0290 - val_loss: 0.0078
150/150 - 0s - loss: 0.0287 - val_loss: 0.0079
150/150 - 0s - loss: 0.0284 - val_loss: 0.0081
150/150 - 0s - loss: 0.0281 - val_loss: 0.0082
150/150 - 0s - loss: 0.0279 - val_loss: 0.0083
150/150 - 0s - loss: 0.0276 - val_loss: 0.0084
150/150 - 0s - loss: 0.0274 - val_loss: 0.0086
150/150 - 0s - loss: 0.0272 - val_loss: 0.0087
150/150 - 0s - loss: 0.0271 - val_loss: 0.0088
150/150 - 0s - loss: 0.0269 - val_loss: 0.0089
150/150 - 0s - loss: 0.0268 - val_loss: 0.0090
150/150 - 0s - loss: 0.0266 - val_loss: 0.0091
150/150 - 0s - loss: 0.0265 - val_loss: 0.0091
150/150 - 0s - loss: 0.0264 - val_loss: 0.0092
150/150 - 0s - loss: 0.0263 - val_loss: 0.0093
150/150 - 0s - loss: 0.0262 - val_loss: 0.0094
150/150 - 0s - loss: 0.0261 - val_loss: 0.0094
150/150 - 0s - loss: 0.0260 - val_loss: 0.0095
150/150 - 0s - loss: 0.0259 - val_loss: 0.0095
150/150 - 0s - loss: 0.0258 - val_loss: 0.0096
150/150 - 0s - loss: 0.0258 - val_loss: 0.0096
150/150 - 0s - loss: 0.0257 - val_loss: 0.0096
150/150 - 0s - loss: 0.0256 - val_loss: 0.0097
150/150 - 0s - loss: 0.0256 - val_loss: 0.0097
150/150 - 0s - loss: 0.0255 - val_loss: 0.0097
150/150 - 0s - loss: 0.0254 - val_loss: 0.0097
150/150 - 0s - loss: 0.0254 - val_loss: 0.0097
150/150 - 0s - loss: 0.0253 - val_loss: 0.0097
150/150 - 0s - loss: 0.0253 - val_loss: 0.0098
150/150 - 0s - loss: 0.0252 - val_loss: 0.0098
150/150 - 0s - loss: 0.0252 - val_loss: 0.0098
150/150 - 0s - loss: 0.0251 - val_loss: 0.0098
150/150 - 0s - loss: 0.0251 - val_loss: 0.0098
150/150 - 0s - loss: 0.0251 - val_loss: 0.0098
150/150 - 0s - loss: 0.0250 - val_loss: 0.0098
150/150 - 0s - loss: 0.0250 - val_loss: 0.0098
150/150 - 0s - loss: 0.0249 - val_loss: 0.0098
150/150 - 0s - loss: 0.0249 - val_loss: 0.0098
150/150 - 0s - loss: 0.0249 - val_loss: 0.0098
150/150 - 0s - loss: 0.0248 - val_loss: 0.0098
150/150 - 0s - loss: 0.0248 - val_loss: 0.0098
150/150 - 0s - loss: 0.0247 - val_loss: 0.0098
150/150 - 0s - loss: 0.0247 - val_loss: 0.0098
150/150 - 0s - loss: 0.0247 - val_loss: 0.0098
150/150 - 0s - loss: 0.0246 - val_loss: 0.0098
150/150 - 0s - loss: 0.0246 - val_loss: 0.0098
150/150 - 0s - loss: 0.0246 - val_loss: 0.0098
150/150 - 0s - loss: 0.0246 - val_loss: 0.0098
150/150 - 0s - loss: 0.0245 - val_loss: 0.0098
150/150 - 0s - loss: 0.0245 - val_loss: 0.0097
150/150 - 0s - loss: 0.0245 - val_loss: 0.0097
150/150 - 0s - loss: 0.0244 - val_loss: 0.0097
150/150 - 0s - loss: 0.0244 - val_loss: 0.0097
150/150 - 0s - loss: 0.0244 - val_loss: 0.0097
150/150 - 0s - loss: 0.0244 - val_loss: 0.0097
150/150 - 0s - loss: 0.0243 - val_loss: 0.0097
150/150 - 0s - loss: 0.0243 - val_loss: 0.0097
150/150 - 0s - loss: 0.0243 - val_loss: 0.0097
150/150 - 0s - loss: 0.0243 - val_loss: 0.0097
150/150 - 0s - loss: 0.0242 - val_loss: 0.0097
150/150 - 0s - loss: 0.0242 - val_loss: 0.0097
150/150 - 0s - loss: 0.0242 - val_loss: 0.0097
150/150 - 0s - loss: 0.0242 - val_loss: 0.0097
150/150 - 0s - loss: 0.0242 - val_loss: 0.0097
150/150 - 0s - loss: 0.0241 - val_loss: 0.0097
150/150 - 0s - loss: 0.0241 - val_loss: 0.0097
150/150 - 0s - loss: 0.0241 - val_loss: 0.0097
150/150 - 0s - loss: 0.0241 - val_loss: 0.0097
150/150 - 0s - loss: 0.0241 - val_loss: 0.0097
150/150 - 0s - loss: 0.0240 - val_loss: 0.0097
150/150 - 0s - loss: 0.0240 - val_loss: 0.0097
150/150 - 0s - loss: 0.0240 - val_loss: 0.0097
150/150 - 0s - loss: 0.0240 - val_loss: 0.0097
150/150 - 0s - loss: 0.0240 - val_loss: 0.0097
150/150 - 0s - loss: 0.0240 - val_loss: 0.0097
150/150 - 0s - loss: 0.0239 - val_loss: 0.0097
150/150 - 0s - loss: 0.0239 - val_loss: 0.0097
150/150 - 0s - loss: 0.0239 - val_loss: 0.0097
150/150 - 0s - loss: 0.0239 - val_loss: 0.0097
150/150 - 0s - loss: 0.0239 - val_loss: 0.0097
150/150 - 0s - loss: 0.0238 - val_loss: 0.0097
150/150 - 0s - loss: 0.0238 - val_loss: 0.0097
150/150 - 0s - loss: 0.0238 - val_loss: 0.0097
150/150 - 0s - loss: 0.0238 - val_loss: 0.0097
150/150 - 0s - loss: 0.0238 - val_loss: 0.0097
150/150 - 0s - loss: 0.0238 - val_loss: 0.0097
150/150 - 0s - loss: 0.0238 - val_loss: 0.0097
150/150 - 0s - loss: 0.0237 - val_loss: 0.0097
150/150 - 0s - loss: 0.0237 - val_loss: 0.0097
150/150 - 0s - loss: 0.0237 - val_loss: 0.0097
150/150 - 0s - loss: 0.0237 - val_loss: 0.0097
150/150 - 0s - loss: 0.0237 - val_loss: 0.0097
150/150 - 0s - loss: 0.0237 - val_loss: 0.0097
150/150 - 0s - loss: 0.0236 - val_loss: 0.0097
150/150 - 0s - loss: 0.0236 - val_loss: 0.0097
150/150 - 0s - loss: 0.0236 - val_loss: 0.0097
150/150 - 0s - loss: 0.0236 - val_loss: 0.0097
150/150 - 0s - loss: 0.0236 - val_loss: 0.0097
150/150 - 0s - loss: 0.0236 - val_loss: 0.0097
150/150 - 0s - loss: 0.0235 - val_loss: 0.0097
150/150 - 0s - loss: 0.0235 - val_loss: 0.0097
150/150 - 0s - loss: 0.0235 - val_loss: 0.0097
150/150 - 0s - loss: 0.0235 - val_loss: 0.0097
150/150 - 0s - loss: 0.0235 - val_loss: 0.0097
150/150 - 0s - loss: 0.0235 - val_loss: 0.0097
150/150 - 0s - loss: 0.0235 - val_loss: 0.0097
150/150 - 0s - loss: 0.0234 - val_loss: 0.0097
150/150 - 0s - loss: 0.0234 - val_loss: 0.0097
150/150 - 0s - loss: 0.0234 - val_loss: 0.0097
150/150 - 0s - loss: 0.0234 - val_loss: 0.0097
150/150 - 0s - loss: 0.0234 - val_loss: 0.0097
150/150 - 0s - loss: 0.0234 - val_loss: 0.0097
150/150 - 0s - loss: 0.0233 - val_loss: 0.0097
150/150 - 0s - loss: 0.0233 - val_loss: 0.0097
150/150 - 0s - loss: 0.0233 - val_loss: 0.0097
150/150 - 0s - loss: 0.0233 - val_loss: 0.0097
150/150 - 0s - loss: 0.0233 - val_loss: 0.0097
150/150 - 0s - loss: 0.0233 - val_loss: 0.0097
150/150 - 0s - loss: 0.0233 - val_loss: 0.0097
150/150 - 0s - loss: 0.0232 - val_loss: 0.0097
150/150 - 0s - loss: 0.0232 - val_loss: 0.0097
150/150 - 0s - loss: 0.0232 - val_loss: 0.0097
150/150 - 0s - loss: 0.0232 - val_loss: 0.0097
150/150 - 0s - loss: 0.0232 - val_loss: 0.0097
150/150 - 0s - loss: 0.0232 - val_loss: 0.0097
150/150 - 0s - loss: 0.0231 - val_loss: 0.0097
150/150 - 0s - loss: 0.0231 - val_loss: 0.0097
150/150 - 0s - loss: 0.0231 - val_loss: 0.0097
150/150 - 0s - loss: 0.0231 - val_loss: 0.0097
150/150 - 0s - loss: 0.0231 - val_loss: 0.0097
150/150 - 0s - loss: 0.0231 - val_loss: 0.0097
150/150 - 0s - loss: 0.0230 - val_loss: 0.0097
150/150 - 0s - loss: 0.0230 - val_loss: 0.0097
150/150 - 0s - loss: 0.0230 - val_loss: 0.0097
150/150 - 0s - loss: 0.0230 - val_loss: 0.0097
150/150 - 0s - loss: 0.0230 - val_loss: 0.0097
150/150 - 0s - loss: 0.0230 - val_loss: 0.0097
150/150 - 0s - loss: 0.0230 - val_loss: 0.0097
150/150 - 0s - loss: 0.0229 - val_loss: 0.0097
150/150 - 0s - loss: 0.0229 - val_loss: 0.0097
150/150 - 0s - loss: 0.0229 - val_loss: 0.0097
150/150 - 0s - loss: 0.0229 - val_loss: 0.0097
150/150 - 0s - loss: 0.0229 - val_loss: 0.0097
150/150 - 0s - loss: 0.0229 - val_loss: 0.0097
150/150 - 0s - loss: 0.0228 - val_loss: 0.0097
150/150 - 0s - loss: 0.0228 - val_loss: 0.0097
150/150 - 0s - loss: 0.0228 - val_loss: 0.0097
150/150 - 0s - loss: 0.0228 - val_loss: 0.0097
150/150 - 0s - loss: 0.0228 - val_loss: 0.0097
150/150 - 0s - loss: 0.0228 - val_loss: 0.0097
150/150 - 0s - loss: 0.0227 - val_loss: 0.0097
150/150 - 0s - loss: 0.0227 - val_loss: 0.0097
150/150 - 0s - loss: 0.0227 - val_loss: 0.0097
150/150 - 0s - loss: 0.0227 - val_loss: 0.0097
150/150 - 0s - loss: 0.0227 - val_loss: 0.0097
150/150 - 0s - loss: 0.0227 - val_loss: 0.0097
150/150 - 0s - loss: 0.0226 - val_loss: 0.0097
150/150 - 0s - loss: 0.0226 - val_loss: 0.0097
150/150 - 0s - loss: 0.0226 - val_loss: 0.0097
150/150 - 0s - loss: 0.0226 - val_loss: 0.0097
150/150 - 0s - loss: 0.0226 - val_loss: 0.0097
150/150 - 0s - loss: 0.0226 - val_loss: 0.0097
150/150 - 0s - loss: 0.0225 - val_loss: 0.0097
150/150 - 0s - loss: 0.0225 - val_loss: 0.0097
150/150 - 0s - loss: 0.0225 - val_loss: 0.0097
150/150 - 0s - loss: 0.0225 - val_loss: 0.0097
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_79_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 117, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=200)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 168, in train
    y_pred = self.predict(X_test,batch_size=batch_size)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 177, in predict
    X1 = X1.reshape(1, 1, len(X1))
ValueError: cannot reshape array of size 2 into shape (1,1,1)
>>> tK.predict(X1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 176, in predict
    X1 = X_test[i,:]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 116, in pandas._libs.index.IndexEngine.get_loc
TypeError: '(0, slice(None, None, None))' is an invalid key
>>> X1
                         room_jitter  vehicle_jitter
deci                                                
2020-07-02 08:04:53.800     1.609438        2.351375
2020-07-02 08:05:40.300     1.609438        2.351375
2020-07-02 08:05:57.700     1.686399        2.174752
2020-07-02 08:05:59.200     1.686399        2.054124
2020-07-02 08:05:59.400     1.686399        1.131403
...                              ...             ...
2020-07-02 08:15:09.200     2.397895        1.131403
2020-07-02 08:15:11.900     2.397895        1.131403
2020-07-02 08:15:14.000     2.370244        0.336474
2020-07-02 08:15:44.600     2.297573        2.028148
2020-07-02 08:16:46.400     1.983298        1.960095

[200 rows x 2 columns]
>>> tK.model
<tensorflow.python.keras.engine.sequential.Sequential object at 0x7efe6ff1c700>
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 128, in <module>
    def difference(dataset, interval=1):
NameError: name 'shuffle' is not defined
>>> X_train
array([[[ 0.        ,  0.        ]],

       [[ 0.4823568 ,  0.72122526]],

       [[ 0.4823568 ,  0.72122526]],

       [[ 0.5115191 ,  0.70242083]],

       [[ 0.5115191 ,  0.68957806]],

       [[ 0.5115191 ,  0.5913396 ]],

       [[ 0.41327113,  0.5913396 ]],

       [[ 0.42813265,  0.6042603 ]],

       [[ 0.34175232,  0.67805696]],

       [[ 0.34175232,  0.5766312 ]],

       [[ 0.37081283,  0.5766312 ]],

       [[ 0.4407454 ,  0.5766312 ]],

       [[ 0.4407454 ,  0.60725963]],

       [[ 0.50012195,  0.60725963]],

       [[ 0.48235673,  0.60725963]],

       [[ 0.62157935,  0.68957806]],

       [[ 0.59419733,  0.7439636 ]],

       [[ 0.6987139 ,  0.7439636 ]],

       [[ 0.6834238 ,  0.7439636 ]],

       [[ 0.64101565,  0.7439636 ]],

       [[ 0.6152286 ,  0.7439636 ]],

       [[ 0.62052834,  0.7439636 ]],

       [[ 0.57688314,  0.7439636 ]],

       [[ 0.6021985 ,  0.8028394 ]],

       [[ 0.32947394,  0.8028394 ]],

       [[ 0.32028744,  0.759199  ]],

       [[ 0.3499015 ,  0.65803576]],

       [[ 0.38495648,  0.65803576]],

       [[ 0.51385087,  0.65803576]],

       [[ 0.51385087,  0.4708836 ]],

       [[ 0.51385087,  0.55956006]],

       [[ 0.7634937 ,  0.55956006]],

       [[ 0.7092696 ,  0.69359607]],

       [[ 0.6721119 ,  0.61301583]],

       [[ 0.6721119 ,  0.7389328 ]],

       [[ 0.62575495,  0.7036239 ]],

       [[ 0.62575495,  0.7036239 ]],

       [[ 0.62575495,  0.7191778 ]],

       [[ 0.5714309 ,  0.7191778 ]],

       [[ 0.6592659 ,  0.58050305]],

       [[ 0.69871396, -1.        ]],

       [[ 0.62157947, -1.        ]],

       [[ 0.6679555 , -1.        ]],

       [[ 0.68490696, -1.        ]],

       [[ 0.68490696, -1.        ]],

       [[ 0.70857495, -1.        ]],

       [[ 0.681935  , -1.        ]],

       [[ 0.6753135 ,  0.7372016 ]],

       [[ 0.7534373 ,  0.7372016 ]],

       [[ 0.7598676 ,  0.70715344]],

       [[ 0.7598676 ,  0.7363254 ]],

       [[ 0.7598676 ,  0.4988164 ]],

       [[ 0.22721145,  0.74798167]],

       [[-0.04294094,  0.74798167]],

       [[-0.04294094,  0.5913395 ]],

       [[ 0.22472662,  0.6717924 ]],

       [[ 0.22472662,  0.6717924 ]],

       [[ 0.35787916,  0.5334629 ]],

       [[ 0.31560943,  0.5334629 ]],

       [[ 0.3656923 ,  0.64640975]],

       [[ 0.47081506,  0.5273775 ]],

       [[ 0.46530622,  0.5273775 ]],

       [[ 0.45891076,  0.6400832 ]],

       [[ 0.45891076,  0.6400832 ]],

       [[ 0.3978026 ,  0.6400832 ]],

       [[ 0.49355727,  1.0000001 ]],

       [[ 0.49355727,  1.0000001 ]],

       [[ 0.4424333 ,  0.5273775 ]],

       [[ 0.70926946,  0.5273775 ]],

       [[ 0.5482716 ,  0.5273775 ]],

       [[ 0.56386733,  0.5273775 ]],

       [[ 0.56386733,  0.59799576]],

       [[ 0.56386733,  0.4988164 ]],

       [[ 0.56386733,  0.5878485 ]],

       [[ 0.56386733,  0.5878485 ]],

       [[ 0.60985374,  0.5878485 ]],

       [[ 0.7092696 ,  0.5878485 ]],

       [[ 0.79796493,  0.5878485 ]],

       [[ 0.6231507 ,  0.77919996]],

       [[ 0.57192993,  0.77919996]],

       [[ 0.68564636,  0.77919996]],

       [[ 0.6716523 ,  0.7116846 ]],

       [[ 0.6321885 ,  0.7116846 ]],

       [[ 0.67073154,  0.76060915]],

       [[ 0.63683784,  0.7042203 ]],

       [[ 0.6798407 ,  0.7042203 ]],

       [[ 0.6789396 ,  0.77252465]],

       [[ 0.593256  ,  0.48103073]],

       [[ 0.5875578 ,  0.48103073]],

       [[ 0.5875578 ,  0.72223437]],

       [[ 0.4162904 ,  0.65238136]],

       [[ 0.43391794,  0.65238136]],

       [[ 0.17978412,  0.65238136]],

       [[-1.        ,  0.65238136]],

       [[-1.        ,  0.65238136]],

       [[-1.        ,  0.65238136]],

       [[ 0.12555972,  0.65238136]],

       [[ 0.6257549 ,  0.65238136]],

       [[ 0.62445503,  0.65238136]],

       [[ 0.73216   ,  0.5805032 ]],

       [[ 0.6867527 ,  0.68957806]],

       [[ 0.6798407 ,  0.73455083]],

       [[ 0.6516672 ,  0.73455083]],

       [[ 0.68873596,  0.6598558 ]],

       [[ 0.63498497,  0.6598558 ]],

       [[ 0.6385145 ,  0.7059899 ]],

       [[ 0.72156   ,  0.65429974]],

       [[ 0.69049025,  0.4988164 ]],

       [[ 0.6997827 ,  0.4988164 ]],

       [[ 0.88383484,  0.6011742 ]],

       [[ 0.88383484,  0.7138798 ]],

       [[ 0.8831775 ,  0.7138798 ]],

       [[ 0.8831775 ,  0.7620009 ]],

       [[ 0.8615189 ,  0.67016697]],

       [[ 1.        ,  0.61847687]],

       [[ 0.9324577 ,  0.61847687]],

       [[ 0.6965671 ,  0.61847687]],

       [[ 0.6359967 ,  0.6484378 ]],

       [[ 0.5472087 ,  0.59799576]],

       [[ 0.5472087 ,  0.74798167]],

       [[ 0.56842357,  0.74798167]],

       [[ 0.57232857,  0.74798167]],

       [[ 0.5132693 ,  0.34270263]],

       [[ 0.5062193 ,  0.67497075]],

       [[ 0.44032234,  0.67497075]],

       [[ 0.4838695 ,  0.6825247 ]],

       [[ 0.4480058 ,  0.7821011 ]],

       [[ 0.3996926 ,  0.7345509 ]],

       [[ 0.26534745,  0.5498747 ]],

       [[ 0.26534745,  0.6922735 ]],

       [[ 0.20106846,  0.6922735 ]],

       [[ 0.44745225,  0.6504278 ]],

       [[ 0.38820922,  0.6504278 ]],

       [[ 0.43677777,  0.6504278 ]],

       [[ 0.4972185 ,  0.8033095 ]],

       [[ 0.6044015 ,  0.48103073]],

       [[ 0.5687254 ,  0.49881673]],

       [[ 0.6334621 ,  0.49881673]],

       [[ 0.5658991 ,  0.49881673]],

       [[ 0.5658991 ,  0.29953182]],

       [[ 0.69548905,  0.7083044 ]],

       [[ 0.7134108 ,  0.6157813 ]],

       [[ 0.74877644,  0.7780169 ]],

       [[ 0.90241635,  0.7780169 ]],

       [[ 0.9324577 ,  0.7780169 ]],

       [[ 0.85731643,  0.6422341 ]],

       [[ 0.85731643,  0.6422341 ]],

       [[ 0.8285483 ,  0.7667335 ]],

       [[ 0.8285483 ,  0.72904277]],

       [[ 0.8285483 ,  0.72904277]]], dtype=float32)
>>> X_train.shape
(150, 1, 2)
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 130, in <module>
    for i in range(interval, len(dataset)):
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 177, in predict
    X1 = X1.reshape(1, 1, len(X1))
ValueError: cannot reshape array of size 2 into shape (1,1,1)
>>> 
>>> (200, 2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 117, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=200)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 159, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 149, in splitSet
    X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[1])
ValueError: cannot reshape array of size 300 into shape (150,2,2)
>>> (200, 2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 117, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=200)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 161, in train
    history = self.model.fit(X_train,y_train
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 848, in fit
    tmp_logs = train_function(iterator)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 505, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2657, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:531 train_step  **
        y_pred = self(x, training=True)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__
        input_spec.assert_input_compatibility(self.input_spec, inputs,
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/input_spec.py:224 assert_input_compatibility
        raise ValueError('Input ' + str(input_index) +

    ValueError: Input 0 is incompatible with layer sequential_81: expected shape=(1, None, 2), found shape=[1, 2, 1]

>>> (200, 2)
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_82_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_82_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_82_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
150/150 - 0s - loss: 0.1314 - val_loss: 0.0167
150/150 - 0s - loss: 0.0812 - val_loss: 0.0144
150/150 - 0s - loss: 0.0727 - val_loss: 0.0135
150/150 - 0s - loss: 0.0677 - val_loss: 0.0126
150/150 - 0s - loss: 0.0640 - val_loss: 0.0117
150/150 - 0s - loss: 0.0609 - val_loss: 0.0109
150/150 - 0s - loss: 0.0581 - val_loss: 0.0102
150/150 - 0s - loss: 0.0557 - val_loss: 0.0096
150/150 - 0s - loss: 0.0535 - val_loss: 0.0090
150/150 - 0s - loss: 0.0514 - val_loss: 0.0086
150/150 - 0s - loss: 0.0495 - val_loss: 0.0082
150/150 - 0s - loss: 0.0478 - val_loss: 0.0078
150/150 - 0s - loss: 0.0462 - val_loss: 0.0076
150/150 - 0s - loss: 0.0447 - val_loss: 0.0074
150/150 - 0s - loss: 0.0433 - val_loss: 0.0072
150/150 - 0s - loss: 0.0420 - val_loss: 0.0071
150/150 - 0s - loss: 0.0407 - val_loss: 0.0070
150/150 - 0s - loss: 0.0396 - val_loss: 0.0069
150/150 - 0s - loss: 0.0385 - val_loss: 0.0068
150/150 - 0s - loss: 0.0375 - val_loss: 0.0068
150/150 - 0s - loss: 0.0366 - val_loss: 0.0068
150/150 - 0s - loss: 0.0357 - val_loss: 0.0068
150/150 - 0s - loss: 0.0349 - val_loss: 0.0068
150/150 - 0s - loss: 0.0342 - val_loss: 0.0068
150/150 - 0s - loss: 0.0334 - val_loss: 0.0068
150/150 - 0s - loss: 0.0328 - val_loss: 0.0069
150/150 - 0s - loss: 0.0322 - val_loss: 0.0069
150/150 - 0s - loss: 0.0316 - val_loss: 0.0070
150/150 - 0s - loss: 0.0310 - val_loss: 0.0070
150/150 - 0s - loss: 0.0305 - val_loss: 0.0071
150/150 - 0s - loss: 0.0301 - val_loss: 0.0071
150/150 - 0s - loss: 0.0296 - val_loss: 0.0072
150/150 - 0s - loss: 0.0292 - val_loss: 0.0073
150/150 - 0s - loss: 0.0288 - val_loss: 0.0074
150/150 - 0s - loss: 0.0285 - val_loss: 0.0075
150/150 - 0s - loss: 0.0281 - val_loss: 0.0076
150/150 - 0s - loss: 0.0278 - val_loss: 0.0076
150/150 - 0s - loss: 0.0275 - val_loss: 0.0077
150/150 - 0s - loss: 0.0272 - val_loss: 0.0078
150/150 - 0s - loss: 0.0269 - val_loss: 0.0079
150/150 - 0s - loss: 0.0267 - val_loss: 0.0080
150/150 - 0s - loss: 0.0265 - val_loss: 0.0081
150/150 - 0s - loss: 0.0263 - val_loss: 0.0082
150/150 - 0s - loss: 0.0261 - val_loss: 0.0083
150/150 - 0s - loss: 0.0259 - val_loss: 0.0084
150/150 - 0s - loss: 0.0257 - val_loss: 0.0085
150/150 - 0s - loss: 0.0255 - val_loss: 0.0086
150/150 - 0s - loss: 0.0254 - val_loss: 0.0086
150/150 - 0s - loss: 0.0252 - val_loss: 0.0087
150/150 - 0s - loss: 0.0251 - val_loss: 0.0088
150/150 - 0s - loss: 0.0249 - val_loss: 0.0089
150/150 - 0s - loss: 0.0248 - val_loss: 0.0089
150/150 - 0s - loss: 0.0247 - val_loss: 0.0090
150/150 - 0s - loss: 0.0246 - val_loss: 0.0091
150/150 - 0s - loss: 0.0245 - val_loss: 0.0091
150/150 - 0s - loss: 0.0244 - val_loss: 0.0092
150/150 - 0s - loss: 0.0243 - val_loss: 0.0092
150/150 - 0s - loss: 0.0242 - val_loss: 0.0093
150/150 - 0s - loss: 0.0241 - val_loss: 0.0093
150/150 - 0s - loss: 0.0240 - val_loss: 0.0094
150/150 - 0s - loss: 0.0239 - val_loss: 0.0094
150/150 - 0s - loss: 0.0239 - val_loss: 0.0095
150/150 - 0s - loss: 0.0238 - val_loss: 0.0095
150/150 - 0s - loss: 0.0237 - val_loss: 0.0095
150/150 - 0s - loss: 0.0236 - val_loss: 0.0096
150/150 - 0s - loss: 0.0236 - val_loss: 0.0096
150/150 - 0s - loss: 0.0235 - val_loss: 0.0096
150/150 - 0s - loss: 0.0234 - val_loss: 0.0096
150/150 - 0s - loss: 0.0233 - val_loss: 0.0096
150/150 - 0s - loss: 0.0233 - val_loss: 0.0097
150/150 - 0s - loss: 0.0232 - val_loss: 0.0097
150/150 - 0s - loss: 0.0231 - val_loss: 0.0097
150/150 - 0s - loss: 0.0231 - val_loss: 0.0097
150/150 - 0s - loss: 0.0230 - val_loss: 0.0097
150/150 - 0s - loss: 0.0230 - val_loss: 0.0097
150/150 - 0s - loss: 0.0229 - val_loss: 0.0097
150/150 - 0s - loss: 0.0228 - val_loss: 0.0097
150/150 - 0s - loss: 0.0228 - val_loss: 0.0097
150/150 - 0s - loss: 0.0227 - val_loss: 0.0097
150/150 - 0s - loss: 0.0226 - val_loss: 0.0097
150/150 - 0s - loss: 0.0226 - val_loss: 0.0097
150/150 - 0s - loss: 0.0225 - val_loss: 0.0097
150/150 - 0s - loss: 0.0225 - val_loss: 0.0097
150/150 - 0s - loss: 0.0224 - val_loss: 0.0097
150/150 - 0s - loss: 0.0223 - val_loss: 0.0097
150/150 - 0s - loss: 0.0223 - val_loss: 0.0097
150/150 - 0s - loss: 0.0222 - val_loss: 0.0097
150/150 - 0s - loss: 0.0222 - val_loss: 0.0097
150/150 - 0s - loss: 0.0221 - val_loss: 0.0097
150/150 - 0s - loss: 0.0221 - val_loss: 0.0097
150/150 - 0s - loss: 0.0220 - val_loss: 0.0097
150/150 - 0s - loss: 0.0220 - val_loss: 0.0097
150/150 - 0s - loss: 0.0219 - val_loss: 0.0097
150/150 - 0s - loss: 0.0218 - val_loss: 0.0097
150/150 - 0s - loss: 0.0218 - val_loss: 0.0097
150/150 - 0s - loss: 0.0217 - val_loss: 0.0096
150/150 - 0s - loss: 0.0217 - val_loss: 0.0096
150/150 - 0s - loss: 0.0216 - val_loss: 0.0096
150/150 - 0s - loss: 0.0216 - val_loss: 0.0096
150/150 - 0s - loss: 0.0215 - val_loss: 0.0096
150/150 - 0s - loss: 0.0215 - val_loss: 0.0096
150/150 - 0s - loss: 0.0214 - val_loss: 0.0096
150/150 - 0s - loss: 0.0214 - val_loss: 0.0096
150/150 - 0s - loss: 0.0213 - val_loss: 0.0096
150/150 - 0s - loss: 0.0213 - val_loss: 0.0096
150/150 - 0s - loss: 0.0212 - val_loss: 0.0096
150/150 - 0s - loss: 0.0212 - val_loss: 0.0096
150/150 - 0s - loss: 0.0211 - val_loss: 0.0096
150/150 - 0s - loss: 0.0211 - val_loss: 0.0095
150/150 - 0s - loss: 0.0210 - val_loss: 0.0095
150/150 - 0s - loss: 0.0210 - val_loss: 0.0095
150/150 - 0s - loss: 0.0209 - val_loss: 0.0095
150/150 - 0s - loss: 0.0209 - val_loss: 0.0095
150/150 - 0s - loss: 0.0208 - val_loss: 0.0095
150/150 - 0s - loss: 0.0208 - val_loss: 0.0095
150/150 - 0s - loss: 0.0207 - val_loss: 0.0095
150/150 - 0s - loss: 0.0207 - val_loss: 0.0095
150/150 - 0s - loss: 0.0206 - val_loss: 0.0095
150/150 - 0s - loss: 0.0206 - val_loss: 0.0095
150/150 - 0s - loss: 0.0205 - val_loss: 0.0095
150/150 - 0s - loss: 0.0205 - val_loss: 0.0095
150/150 - 0s - loss: 0.0204 - val_loss: 0.0095
150/150 - 0s - loss: 0.0204 - val_loss: 0.0094
150/150 - 0s - loss: 0.0204 - val_loss: 0.0094
150/150 - 0s - loss: 0.0203 - val_loss: 0.0094
150/150 - 0s - loss: 0.0203 - val_loss: 0.0094
150/150 - 0s - loss: 0.0202 - val_loss: 0.0094
150/150 - 0s - loss: 0.0202 - val_loss: 0.0094
150/150 - 0s - loss: 0.0201 - val_loss: 0.0094
150/150 - 0s - loss: 0.0201 - val_loss: 0.0094
150/150 - 0s - loss: 0.0201 - val_loss: 0.0094
150/150 - 0s - loss: 0.0200 - val_loss: 0.0094
150/150 - 0s - loss: 0.0200 - val_loss: 0.0094
150/150 - 0s - loss: 0.0200 - val_loss: 0.0094
150/150 - 0s - loss: 0.0199 - val_loss: 0.0094
150/150 - 0s - loss: 0.0199 - val_loss: 0.0094
150/150 - 0s - loss: 0.0198 - val_loss: 0.0094
150/150 - 0s - loss: 0.0198 - val_loss: 0.0094
150/150 - 0s - loss: 0.0198 - val_loss: 0.0094
150/150 - 0s - loss: 0.0197 - val_loss: 0.0094
150/150 - 0s - loss: 0.0197 - val_loss: 0.0094
150/150 - 0s - loss: 0.0197 - val_loss: 0.0094
150/150 - 0s - loss: 0.0196 - val_loss: 0.0094
150/150 - 0s - loss: 0.0196 - val_loss: 0.0094
150/150 - 0s - loss: 0.0196 - val_loss: 0.0094
150/150 - 0s - loss: 0.0195 - val_loss: 0.0094
150/150 - 0s - loss: 0.0195 - val_loss: 0.0094
150/150 - 0s - loss: 0.0195 - val_loss: 0.0094
150/150 - 0s - loss: 0.0194 - val_loss: 0.0094
150/150 - 0s - loss: 0.0194 - val_loss: 0.0094
150/150 - 0s - loss: 0.0194 - val_loss: 0.0094
150/150 - 0s - loss: 0.0193 - val_loss: 0.0094
150/150 - 0s - loss: 0.0193 - val_loss: 0.0094
150/150 - 0s - loss: 0.0193 - val_loss: 0.0094
150/150 - 0s - loss: 0.0193 - val_loss: 0.0094
150/150 - 0s - loss: 0.0192 - val_loss: 0.0094
150/150 - 0s - loss: 0.0192 - val_loss: 0.0094
150/150 - 0s - loss: 0.0192 - val_loss: 0.0094
150/150 - 0s - loss: 0.0192 - val_loss: 0.0094
150/150 - 0s - loss: 0.0191 - val_loss: 0.0094
150/150 - 0s - loss: 0.0191 - val_loss: 0.0094
150/150 - 0s - loss: 0.0191 - val_loss: 0.0094
150/150 - 0s - loss: 0.0191 - val_loss: 0.0094
150/150 - 0s - loss: 0.0190 - val_loss: 0.0094
150/150 - 0s - loss: 0.0190 - val_loss: 0.0094
150/150 - 0s - loss: 0.0190 - val_loss: 0.0094
150/150 - 0s - loss: 0.0190 - val_loss: 0.0094
150/150 - 0s - loss: 0.0189 - val_loss: 0.0094
150/150 - 0s - loss: 0.0189 - val_loss: 0.0094
150/150 - 0s - loss: 0.0189 - val_loss: 0.0094
150/150 - 0s - loss: 0.0189 - val_loss: 0.0094
150/150 - 0s - loss: 0.0189 - val_loss: 0.0094
150/150 - 0s - loss: 0.0188 - val_loss: 0.0094
150/150 - 0s - loss: 0.0188 - val_loss: 0.0094
150/150 - 0s - loss: 0.0188 - val_loss: 0.0094
150/150 - 0s - loss: 0.0188 - val_loss: 0.0094
150/150 - 0s - loss: 0.0188 - val_loss: 0.0094
150/150 - 0s - loss: 0.0187 - val_loss: 0.0094
150/150 - 0s - loss: 0.0187 - val_loss: 0.0094
150/150 - 0s - loss: 0.0187 - val_loss: 0.0094
150/150 - 0s - loss: 0.0187 - val_loss: 0.0094
150/150 - 0s - loss: 0.0187 - val_loss: 0.0094
150/150 - 0s - loss: 0.0186 - val_loss: 0.0094
150/150 - 0s - loss: 0.0186 - val_loss: 0.0094
150/150 - 0s - loss: 0.0186 - val_loss: 0.0094
150/150 - 0s - loss: 0.0186 - val_loss: 0.0094
150/150 - 0s - loss: 0.0186 - val_loss: 0.0094
150/150 - 0s - loss: 0.0186 - val_loss: 0.0094
150/150 - 0s - loss: 0.0186 - val_loss: 0.0095
150/150 - 0s - loss: 0.0185 - val_loss: 0.0095
150/150 - 0s - loss: 0.0185 - val_loss: 0.0095
150/150 - 0s - loss: 0.0185 - val_loss: 0.0095
150/150 - 0s - loss: 0.0185 - val_loss: 0.0095
150/150 - 0s - loss: 0.0185 - val_loss: 0.0095
150/150 - 0s - loss: 0.0185 - val_loss: 0.0095
150/150 - 0s - loss: 0.0185 - val_loss: 0.0095
150/150 - 0s - loss: 0.0184 - val_loss: 0.0095
150/150 - 0s - loss: 0.0184 - val_loss: 0.0095
150/150 - 0s - loss: 0.0184 - val_loss: 0.0095
150/150 - 0s - loss: 0.0184 - val_loss: 0.0095
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_82_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 117, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 168, in train
    y_pred = self.predict(X_test,batch_size=batch_size)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 177, in predict
    X1 = X1.reshape(1, 1, len(X1))
ValueError: cannot reshape array of size 2 into shape (1,1,1)
>>> X_test
array([[[0.82549655, 0.72904277]],

       [[0.6410156 , 0.6733934 ]],

       [[0.5970076 , 0.6733934 ]],

       [[0.61120456, 0.4708836 ]],

       [[0.61120456, 0.5766312 ]],

       [[0.61120456, 0.6882042 ]],

       [[0.7191336 , 0.76269007]],

       [[0.60985374, 0.76269007]],

       [[0.64101565, 0.76269007]],

       [[0.72953767, 0.7712647 ]],

       [[0.7525097 , 0.7712647 ]],

       [[0.7525097 , 0.7712647 ]],

       [[0.7525097 , 0.6839733 ]],

       [[0.5514426 , 0.6839733 ]],

       [[0.7008486 , 0.6839733 ]],

       [[0.7008486 , 0.55956024]],

       [[0.58467615, 0.7750007 ]],

       [[0.58467615, 0.7750007 ]],

       [[0.58467615, 0.7336524 ]],

       [[0.58467615, 0.7336524 ]],

       [[0.55144256, 0.459666  ]],

       [[0.49963963, 0.459666  ]],

       [[0.49963963, 0.5640913 ]],

       [[0.49355727, 0.5640913 ]],

       [[0.41022736, 0.5640913 ]],

       [[0.35257933, 0.76130736]],

       [[0.40344423, 0.76130736]],

       [[0.36414245, 0.76130736]],

       [[0.31619734, 0.7699896 ]],

       [[0.31619734, 0.7699896 ]],

       [[0.31619734, 0.7699896 ]],

       [[0.35787925, 0.66340494]],

       [[0.44243342, 0.7138798 ]],

       [[0.27594748, 0.72422445]],

       [[0.5252995 , 0.72809637]],

       [[0.45485806, 0.72809637]],

       [[0.45485806, 0.73274636]],

       [[0.45485806, 0.73274636]],

       [[0.45485806, 0.73274636]],

       [[0.45485806, 0.7222344 ]],

       [[0.45485806, 0.64640975]],

       [[0.3882091 , 0.6616451 ]],

       [[0.6674908 , 0.68397325]],

       [[0.6674908 , 0.4471263 ]],

       [[0.7811212 , 0.4471263 ]],

       [[0.7811212 , 0.4471263 ]],

       [[0.7811212 , 0.5913395 ]],

       [[0.7811212 , 0.5913395 ]],

       [[0.7706434 , 0.5067066 ]],

       [[0.7431066 , 0.6868125 ]]], dtype=float32)
>>> X_test.shape
(50, 1, 2)
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 130, in <module>
    y_pred = tK.predict(X_test,batch_size=batch_size)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 177, in predict
    X1 = X1.reshape(1, 1, len(X1))
ValueError: cannot reshape array of size 2 into shape (1,1,1)
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/deep_lernia/deep_lernia/train_longShort.py", line 177, in <module>
    X1 = X1.reshape(1, 1, len(X1))
ValueError: cannot reshape array of size 2 into shape (1,1,1)
>>> X1
array([[0.82549655, 0.72904277]], dtype=float32)
>>> X_test.shape
(50, 1, 2)
>>> X1.shape
(1, 2)
>>> X1.shape
(1, 2, 1)
>>> yhat = tK.model.predict(X1, batch_size=batch_size)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 88, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 1268, in predict
    tmp_batch_outputs = predict_function(iterator)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 618, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2419, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2774, in _maybe_define_function
    return self._define_function_with_shape_relaxation(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2705, in _define_function_with_shape_relaxation
    graph_function = self._create_graph_function(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2657, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:1147 predict_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:1122 predict_step  **
        return self(x, training=False)
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__
        input_spec.assert_input_compatibility(self.input_spec, inputs,
    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/input_spec.py:224 assert_input_compatibility
        raise ValueError('Input ' + str(input_index) +

    ValueError: Input 0 is incompatible with layer sequential_82: expected shape=(1, None, 2), found shape=[1, 2, 1]

>>> X1.shape
(1, 2, 1)
>>> X1.shape
(1, 1, 2)
>>> yhat = tK.model.predict(X1, batch_size=batch_size)
>>> yhat
array([[0.74314415, 0.7423121 ]], dtype=float32)
>>> X1
array([[[0.7431066, 0.6868125]]], dtype=float32)
>>> (200, 2)
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_83_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_83_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_83_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
150/150 - 0s - loss: 0.4017 - val_loss: 0.1973
150/150 - 0s - loss: 0.1299 - val_loss: 0.0209
150/150 - 0s - loss: 0.0657 - val_loss: 0.0126
150/150 - 0s - loss: 0.0620 - val_loss: 0.0113
150/150 - 0s - loss: 0.0587 - val_loss: 0.0102
150/150 - 0s - loss: 0.0558 - val_loss: 0.0094
150/150 - 0s - loss: 0.0534 - val_loss: 0.0089
150/150 - 0s - loss: 0.0513 - val_loss: 0.0084
150/150 - 0s - loss: 0.0494 - val_loss: 0.0082
150/150 - 0s - loss: 0.0478 - val_loss: 0.0079
150/150 - 0s - loss: 0.0462 - val_loss: 0.0078
150/150 - 0s - loss: 0.0447 - val_loss: 0.0076
150/150 - 0s - loss: 0.0433 - val_loss: 0.0075
150/150 - 0s - loss: 0.0420 - val_loss: 0.0075
150/150 - 0s - loss: 0.0407 - val_loss: 0.0074
150/150 - 0s - loss: 0.0395 - val_loss: 0.0073
150/150 - 0s - loss: 0.0384 - val_loss: 0.0073
150/150 - 0s - loss: 0.0373 - val_loss: 0.0072
150/150 - 0s - loss: 0.0363 - val_loss: 0.0072
150/150 - 0s - loss: 0.0354 - val_loss: 0.0072
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_83_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 118, in <module>
    tK.plotPrediction()
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 160, in plotPrediction
    X = self.reshape(self.X,mode=reshape)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 225, in reshape
    X, y = self.prepareLongShort(X,self.y,n_in=1)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 117, in prepareLongShort
    return pre, post[0].values
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0
>>> kpi
{'cor': 0.8197233991589754, 'rel_err': 0.1182162840784529, 'mut_info': 3.1942768977141434, 'dif': -0.007048012892797692}
>>> X
                         room_jitter  vehicle_jitter  ...  ram_usage  cpu_usage
deci                                                  ...                      
2020-07-02 08:04:53.800     1.609438        2.351375  ...   7.524115  68.403366
2020-07-02 08:05:40.300     1.609438        2.351375  ...   7.524115  68.403366
2020-07-02 08:05:57.700     1.686399        2.174752  ...   7.524115  68.403366
2020-07-02 08:05:59.200     1.686399        2.054124  ...   7.525381  68.403366
2020-07-02 08:05:59.400     1.686399        1.131403  ...   7.525381  68.403366
...                              ...             ...  ...        ...        ...
2020-07-02 08:51:19.600     2.411440        2.379546  ...   7.729978  66.334991
2020-07-02 08:52:28.300     2.766319        2.379546  ...   7.749253  66.334991
2020-07-02 08:53:23.800     2.766319        2.379546  ...   7.749253  66.334991
2020-07-02 08:53:47.900     2.766319        2.379546  ...   7.749253  66.334991
2020-07-02 08:54:03.400     2.766319        2.379546  ...   7.749253  66.334991

[29546 rows x 11 columns]
>>> X1
                         room_jitter  vehicle_jitter
deci                                                
2020-07-02 08:04:53.800     1.609438        2.351375
2020-07-02 08:05:40.300     1.609438        2.351375
2020-07-02 08:05:57.700     1.686399        2.174752
2020-07-02 08:05:59.200     1.686399        2.054124
2020-07-02 08:05:59.400     1.686399        1.131403
...                              ...             ...
2020-07-02 08:15:09.200     2.397895        1.131403
2020-07-02 08:15:11.900     2.397895        1.131403
2020-07-02 08:15:14.000     2.370244        0.336474
2020-07-02 08:15:44.600     2.297573        2.028148
2020-07-02 08:16:46.400     1.983298        1.960095

[200 rows x 2 columns]
>>> tK.reshape(X1)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 225, in reshape
    X, y = self.prepareLongShort(X,self.y,n_in=1)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 117, in prepareLongShort
    return pre, post[0].values
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0
>>> (200, 2)
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_84_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_84_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_84_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
150/150 - 1s - loss: 0.1950 - val_loss: 0.0246
150/150 - 0s - loss: 0.1160 - val_loss: 0.0220
150/150 - 0s - loss: 0.1052 - val_loss: 0.0201
150/150 - 0s - loss: 0.0964 - val_loss: 0.0186
150/150 - 0s - loss: 0.0890 - val_loss: 0.0173
150/150 - 0s - loss: 0.0826 - val_loss: 0.0161
150/150 - 0s - loss: 0.0772 - val_loss: 0.0150
150/150 - 0s - loss: 0.0727 - val_loss: 0.0141
150/150 - 0s - loss: 0.0690 - val_loss: 0.0133
150/150 - 0s - loss: 0.0659 - val_loss: 0.0126
150/150 - 0s - loss: 0.0633 - val_loss: 0.0119
150/150 - 0s - loss: 0.0610 - val_loss: 0.0114
150/150 - 0s - loss: 0.0590 - val_loss: 0.0108
150/150 - 0s - loss: 0.0572 - val_loss: 0.0103
150/150 - 0s - loss: 0.0554 - val_loss: 0.0099
150/150 - 0s - loss: 0.0538 - val_loss: 0.0095
150/150 - 0s - loss: 0.0522 - val_loss: 0.0091
150/150 - 0s - loss: 0.0506 - val_loss: 0.0087
150/150 - 0s - loss: 0.0492 - val_loss: 0.0084
150/150 - 0s - loss: 0.0477 - val_loss: 0.0081
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_84_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 118, in <module>
    tK.plotPrediction()
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 160, in plotPrediction
    X = self.reshape(self.X,mode=reshape)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 225, in reshape
    X, y = self.prepareLongShort(X,n_in=1)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 117, in prepareLongShort
    return pre, post[0].values
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0
>>> tK.X
     room_jitter  vehicle_jitter
0       0.482357        0.721225
1       0.482357        0.721225
2       0.511519        0.702421
3       0.511519        0.689578
4       0.511519        0.591340
..           ...             ...
195     0.781121        0.591340
196     0.781121        0.591340
197     0.770643        0.506707
198     0.743107        0.686813
199     0.624021        0.679567

[200 rows x 2 columns]
>>> tK.X.shape
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'tk' is not defined
>>> tK.X.shape
(200, 2)
>>> tK.reshape(tK.X)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 225, in reshape
    X, y = self.prepareLongShort(X,n_in=1)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 117, in prepareLongShort
    return pre, post[0].values
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0
>>> X2, y2 = tK.reshape(tk.X)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 225, in reshape
    X, y = self.prepareLongShort(X,n_in=1)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 117, in prepareLongShort
    return pre, post[0].values
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0
>>> X2, y2 = tK.reshape(tK.X)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'tk' is not defined
>>> X2, y2 = tK.prepareLongShort(tK.X)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 225, in reshape
    X, y = self.prepareLongShort(X,n_in=1)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 117, in prepareLongShort
    return pre, post[0].values
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0
>>> X2, y2 = tK.prepareLongShort(tK.X)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 117, in prepareLongShort
    return pre, post[0].values
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 0
>>> post = pd.DataFrame(tK.X)
>>> post
     room_jitter  vehicle_jitter
0       0.482357        0.721225
1       0.482357        0.721225
2       0.511519        0.702421
3       0.511519        0.689578
4       0.511519        0.591340
..           ...             ...
195     0.781121        0.591340
196     0.781121        0.591340
197     0.770643        0.506707
198     0.743107        0.686813
199     0.624021        0.679567

[200 rows x 2 columns]
>>> pre
     room_jitter  vehicle_jitter  room_jitter  vehicle_jitter
0       0.000000        0.000000     0.000000        0.000000
1       0.482357        0.721225     0.000000        0.000000
2       0.482357        0.721225     0.482357        0.721225
3       0.511519        0.702421     0.482357        0.721225
4       0.511519        0.689578     0.511519        0.702421
..           ...             ...          ...             ...
195     0.781121        0.447126     0.781121        0.447126
196     0.781121        0.591340     0.781121        0.447126
197     0.781121        0.591340     0.781121        0.591340
198     0.770643        0.506707     0.781121        0.591340
199     0.743107        0.686813     0.770643        0.506707

[200 rows x 4 columns]
>>> post
     room_jitter  vehicle_jitter
0       0.482357        0.721225
1       0.482357        0.721225
2       0.511519        0.702421
3       0.511519        0.689578
4       0.511519        0.591340
..           ...             ...
195     0.781121        0.591340
196     0.781121        0.591340
197     0.770643        0.506707
198     0.743107        0.686813
199     0.624021        0.679567

[200 rows x 2 columns]
>>> pre
     room_jitter  vehicle_jitter  room_jitter  vehicle_jitter
0       0.000000        0.000000     0.000000        0.000000
1       0.482357        0.721225     0.000000        0.000000
2       0.482357        0.721225     0.482357        0.721225
3       0.511519        0.702421     0.482357        0.721225
4       0.511519        0.689578     0.511519        0.702421
..           ...             ...          ...             ...
195     0.781121        0.447126     0.781121        0.447126
196     0.781121        0.591340     0.781121        0.447126
197     0.781121        0.591340     0.781121        0.591340
198     0.770643        0.506707     0.781121        0.591340
199     0.743107        0.686813     0.770643        0.506707

[200 rows x 4 columns]
>>> post
     room_jitter  vehicle_jitter
0       0.482357        0.721225
1       0.482357        0.721225
2       0.511519        0.702421
3       0.511519        0.689578
4       0.511519        0.591340
..           ...             ...
195     0.781121        0.591340
196     0.781121        0.591340
197     0.770643        0.506707
198     0.743107        0.686813
199     0.624021        0.679567

[200 rows x 2 columns]
>>> (200, 2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 117, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 159, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 151, in splitSet
    y_train = y_train.reshape(len(y_train),-1)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py", line 5274, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'DataFrame' object has no attribute 'reshape'
>>> (200, 2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 117, in <module>
    kpi = tK.train(batch_size=1,nb_epoch=20)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 159, in train
    X_train, X_test, y_train, y_test = self.splitSet(self.X,self.y,shuffle=shuffle)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 149, in splitSet
    X_train = X_train.values.reshape(X_train.shape[0], -1, X_train.shape[1])
AttributeError: 'numpy.ndarray' object has no attribute 'values'
>>> (200, 2)
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_87_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_87_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_87_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
150/150 - 1s - loss: 0.2618 - val_loss: 0.0314
150/150 - 0s - loss: 0.1286 - val_loss: 0.0193
150/150 - 0s - loss: 0.1210 - val_loss: 0.0177
150/150 - 0s - loss: 0.1138 - val_loss: 0.0163
150/150 - 0s - loss: 0.1072 - val_loss: 0.0150
150/150 - 0s - loss: 0.1013 - val_loss: 0.0139
150/150 - 0s - loss: 0.0959 - val_loss: 0.0130
150/150 - 0s - loss: 0.0912 - val_loss: 0.0122
150/150 - 0s - loss: 0.0871 - val_loss: 0.0115
150/150 - 0s - loss: 0.0835 - val_loss: 0.0109
150/150 - 0s - loss: 0.0803 - val_loss: 0.0104
150/150 - 0s - loss: 0.0774 - val_loss: 0.0100
150/150 - 0s - loss: 0.0748 - val_loss: 0.0096
150/150 - 0s - loss: 0.0724 - val_loss: 0.0092
150/150 - 0s - loss: 0.0702 - val_loss: 0.0090
150/150 - 0s - loss: 0.0681 - val_loss: 0.0087
150/150 - 0s - loss: 0.0661 - val_loss: 0.0085
150/150 - 0s - loss: 0.0643 - val_loss: 0.0083
150/150 - 0s - loss: 0.0626 - val_loss: 0.0081
150/150 - 0s - loss: 0.0610 - val_loss: 0.0080
WARNING:tensorflow:Model was constructed with shape (1, 2, 2) for input Tensor("lstm_87_input:0", shape=(1, 2, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_resample.py", line 118, in <module>
    tK.plotPrediction()
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 160, in plotPrediction
    X = self.reshape(self.X,mode=reshape)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 226, in reshape
    X_test = X.values.reshape((X.shape[0], 1, X.shape[1]))
AttributeError: 'numpy.ndarray' object has no attribute 'values'
>>> (200, 2)
150/150 - 0s - loss: 0.2629 - val_loss: 0.0844
150/150 - 0s - loss: 0.1249 - val_loss: 0.0335
150/150 - 0s - loss: 0.1069 - val_loss: 0.0195
150/150 - 0s - loss: 0.1012 - val_loss: 0.0153
150/150 - 0s - loss: 0.0982 - val_loss: 0.0138
150/150 - 0s - loss: 0.0956 - val_loss: 0.0130
150/150 - 0s - loss: 0.0932 - val_loss: 0.0125
150/150 - 0s - loss: 0.0908 - val_loss: 0.0121
150/150 - 0s - loss: 0.0884 - val_loss: 0.0117
150/150 - 0s - loss: 0.0860 - val_loss: 0.0113
150/150 - 0s - loss: 0.0835 - val_loss: 0.0110
150/150 - 0s - loss: 0.0810 - val_loss: 0.0106
150/150 - 0s - loss: 0.0784 - val_loss: 0.0103
150/150 - 0s - loss: 0.0759 - val_loss: 0.0100
150/150 - 0s - loss: 0.0733 - val_loss: 0.0097
150/150 - 0s - loss: 0.0709 - val_loss: 0.0095
150/150 - 0s - loss: 0.0685 - val_loss: 0.0092
150/150 - 0s - loss: 0.0663 - val_loss: 0.0090
150/150 - 0s - loss: 0.0642 - val_loss: 0.0088
150/150 - 0s - loss: 0.0622 - val_loss: 0.0087
>>> y1
                         room_jitter  vehicle_jitter  ...  ram_usage  cpu_usage
deci                                                  ...                      
2020-07-02 08:04:53.800     1.609438        2.351375  ...   7.524115  68.403366
2020-07-02 08:05:40.300     1.609438        2.351375  ...   7.524115  68.403366
2020-07-02 08:05:57.700     1.686399        2.174752  ...   7.524115  68.403366
2020-07-02 08:05:59.200     1.686399        2.054124  ...   7.525381  68.403366
2020-07-02 08:05:59.400     1.686399        1.131403  ...   7.525381  68.403366
...                              ...             ...  ...        ...        ...
2020-07-02 08:15:09.200     2.397895        1.131403  ...   7.563508  68.243240
2020-07-02 08:15:11.900     2.397895        1.131403  ...   7.575470  68.243240
2020-07-02 08:15:14.000     2.370244        0.336474  ...   7.575470  68.243240
2020-07-02 08:15:44.600     2.297573        2.028148  ...   7.581198  68.243240
2020-07-02 08:16:46.400     1.983298        1.960095  ...   7.581198  68.243240

[200 rows x 11 columns]
>>> (200, 11)
150/150 - 0s - loss: 0.3470 - val_loss: 0.2870
150/150 - 0s - loss: 0.2496 - val_loss: 0.2758
150/150 - 0s - loss: 0.2029 - val_loss: 0.2717
150/150 - 0s - loss: 0.1691 - val_loss: 0.2704
150/150 - 0s - loss: 0.1442 - val_loss: 0.2698
150/150 - 0s - loss: 0.1273 - val_loss: 0.2694
150/150 - 0s - loss: 0.1167 - val_loss: 0.2690
150/150 - 0s - loss: 0.1101 - val_loss: 0.2683
150/150 - 0s - loss: 0.1058 - val_loss: 0.2671
150/150 - 0s - loss: 0.1028 - val_loss: 0.2658
150/150 - 0s - loss: 0.1005 - val_loss: 0.2647
150/150 - 0s - loss: 0.0987 - val_loss: 0.2640
150/150 - 0s - loss: 0.0972 - val_loss: 0.2639
150/150 - 0s - loss: 0.0959 - val_loss: 0.2645
150/150 - 0s - loss: 0.0947 - val_loss: 0.2657
150/150 - 0s - loss: 0.0936 - val_loss: 0.2673
150/150 - 0s - loss: 0.0925 - val_loss: 0.2693
150/150 - 0s - loss: 0.0916 - val_loss: 0.2713
150/150 - 0s - loss: 0.0906 - val_loss: 0.2734
150/150 - 0s - loss: 0.0898 - val_loss: 0.2753
>>> 22159/22159 - 25s - loss: 0.0312 - val_loss: 0.0212
22159/22159 - 24s - loss: 0.0204 - val_loss: 0.0210
22159/22159 - 24s - loss: 0.0203 - val_loss: 0.0208
22159/22159 - 24s - loss: 0.0202 - val_loss: 0.0207
22159/22159 - 24s - loss: 0.0201 - val_loss: 0.0206
22159/22159 - 24s - loss: 0.0201 - val_loss: 0.0205
22159/22159 - 24s - loss: 0.0201 - val_loss: 0.0205
22159/22159 - 24s - loss: 0.0201 - val_loss: 0.0204
22159/22159 - 24s - loss: 0.0200 - val_loss: 0.0204
22159/22159 - 24s - loss: 0.0200 - val_loss: 0.0203
22159/22159 - 24s - loss: 0.0200 - val_loss: 0.0203
22159/22159 - 24s - loss: 0.0200 - val_loss: 0.0203
22159/22159 - 26s - loss: 0.0200 - val_loss: 0.0202
22159/22159 - 24s - loss: 0.0200 - val_loss: 0.0202
22159/22159 - 24s - loss: 0.0200 - val_loss: 0.0202
22159/22159 - 24s - loss: 0.0200 - val_loss: 0.0202
22159/22159 - 24s - loss: 0.0200 - val_loss: 0.0201
22159/22159 - 24s - loss: 0.0200 - val_loss: 0.0201
22159/22159 - 24s - loss: 0.0200 - val_loss: 0.0201
22159/22159 - 24s - loss: 0.0200 - val_loss: 0.0201
>>> 

>>> ----------------------execute-query-folder---------------------
QUEUED
RUNNING
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7aae6830-9693-419c-b933-b4a8f3a805b0.csv
>>> ----------------------execute-query-folder---------------------
QUEUED
RUNNING
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/57e96f42-52a2-4b8e-b2a9-1b428aa65b8f.csv
>>> query
"with ts_range as (\nselect distinct ts\nfrom ree_cloud_data_{{ environment }}.telemetry\nwhere \n    dt = date_format(date_trunc('day', from_unixtime({{snapshot}}/1000)), '%Y-%m-%d')\n    and session_id = '{{ session_id }}'\n), timebuckets as (\nselect\n\ttimestamp_ms,\n\tfloor(to_unixtime(timestamp_ms)*10)/10 as timebucket,\n\tradar_track.object_distance_m as object_distance_m,\n\n\tlatency_camera.latency_ms as camera_latency_ms,\n    \t(joystick_latency.joystick_latency_ns/1000000) as command_latency_ms,\n    \te2e_latency.latency_ms as e2e_latency_ms\nfrom ree_cloud_data_{{ environment }}.telemetry\nwhere\n    ts in (select ts from ts_range)\n    and session_id = '{{ session_id }}'\n    -- and (latency_camera is not null or joystick_latency is not null or e2e_latency is not null)\n    and abs({{snapshot}}/1000.0 - to_unixtime(timestamp_ms)) < {{time_horizon_sec}}\norder by timestamp_ms\n)\nselect \n    max(camera_latency_ms) as max_camera_latency_ms,\n    max(command_latency_ms) as max_command_latency_ms,\n    max(e2e_latency_ms) as max_e2e_latency_ms,\n    {{snapshot}},\n    timebucket,\n    timebucket*1000 - {{snapshot}} as ts\n    -- from_unixtime(timebucket)  as ts\n    -- date_add('millisecond', (timebucket%100)*10 ,from_unixtime(timebucket/100))  as ts\nfrom timebuckets\ngroup by timebucket\norder by timebucket\n"
>>> Sammy has 5 balloons.
>>> with ts_range as (
select distinct ts
from ree_cloud_data_{ environment }.telemetry
where 
    dt = date_format(date_trunc('day', from_unixtime({snapshot}/1000)), '%Y-%m-%d')
    and session_id = '{ session_id }'
), timebuckets as (
select
	timestamp_ms,
	floor(to_unixtime(timestamp_ms)*10)/10 as timebucket,
	radar_track.object_distance_m as object_distance_m,

	latency_camera.latency_ms as camera_latency_ms,
    	(joystick_latency.joystick_latency_ns/1000000) as command_latency_ms,
    	e2e_latency.latency_ms as e2e_latency_ms
from ree_cloud_data_{ environment }.telemetry
where
    ts in (select ts from ts_range)
    and session_id = '{ session_id }'
    -- and (latency_camera is not null or joystick_latency is not null or e2e_latency is not null)
    and abs({snapshot}/1000.0 - to_unixtime(timestamp_ms)) < {time_horizon_sec}
order by timestamp_ms
)
select 
    max(camera_latency_ms) as max_camera_latency_ms,
    max(command_latency_ms) as max_command_latency_ms,
    max(e2e_latency_ms) as max_e2e_latency_ms,
    {snapshot},
    timebucket,
    timebucket*1000 - {snapshot} as ts
    -- from_unixtime(timebucket)  as ts
    -- date_add('millisecond', (timebucket%100)*10 ,from_unixtime(timebucket/100))  as ts
from timebuckets
group by timebucket
order by timebucket

>>> with ts_range as (
select distinct ts
from ree_cloud_data_{ environment }.telemetry
where 
    dt = date_format(date_trunc('day', from_unixtime({snapshot}/1000)), '%Y-%m-%d')
    and session_id = '{ session_id }'
), timebuckets as (
select
	timestamp_ms,
	floor(to_unixtime(timestamp_ms)*10)/10 as timebucket,
	radar_track.object_distance_m as object_distance_m,

	latency_camera.latency_ms as camera_latency_ms,
    	(joystick_latency.joystick_latency_ns/1000000) as command_latency_ms,
    	e2e_latency.latency_ms as e2e_latency_ms
from ree_cloud_data_{ environment }.telemetry
where
    ts in (select ts from ts_range)
    and session_id = '{ session_id }'
    -- and (latency_camera is not null or joystick_latency is not null or e2e_latency is not null)
    and abs({snapshot}/1000.0 - to_unixtime(timestamp_ms)) < {time_horizon_sec}
order by timestamp_ms
)
select 
    max(camera_latency_ms) as max_camera_latency_ms,
    max(command_latency_ms) as max_command_latency_ms,
    max(e2e_latency_ms) as max_e2e_latency_ms,
    {snapshot},
    timebucket,
    timebucket*1000 - {snapshot} as ts
    -- from_unixtime(timebucket)  as ts
    -- date_add('millisecond', (timebucket%100)*10 ,from_unixtime(timebucket/100))  as ts
from timebuckets
group by timebucket
order by timebucket

>>> with ts_range as (
select distinct ts
from ree_cloud_data_{ environment }.telemetry
where 
    dt = date_format(date_trunc('day', from_unixtime({snapshot}/1000)), '%Y-%m-%d')
    and session_id = '{ session_id }'
), timebuckets as (
select
	timestamp_ms,
	floor(to_unixtime(timestamp_ms)*10)/10 as timebucket,
	radar_track.object_distance_m as object_distance_m,

	latency_camera.latency_ms as camera_latency_ms,
    	(joystick_latency.joystick_latency_ns/1000000) as command_latency_ms,
    	e2e_latency.latency_ms as e2e_latency_ms
from ree_cloud_data_{ environment }.telemetry
where
    ts in (select ts from ts_range)
    and session_id = '{ session_id }'
    -- and (latency_camera is not null or joystick_latency is not null or e2e_latency is not null)
    and abs({snapshot}/1000.0 - to_unixtime(timestamp_ms)) < {time_horizon_sec}
order by timestamp_ms
)
select 
    max(camera_latency_ms) as max_camera_latency_ms,
    max(command_latency_ms) as max_command_latency_ms,
    max(e2e_latency_ms) as max_e2e_latency_ms,
    {snapshot},
    timebucket,
    timebucket*1000 - {snapshot} as ts
    -- from_unixtime(timebucket)  as ts
    -- date_add('millisecond', (timebucket%100)*10 ,from_unixtime(timebucket/100))  as ts
from timebuckets
group by timebucket
order by timebucket

>>> with ts_range as (
select distinct ts
from ree_cloud_data_{ environment }.telemetry
where 
    dt = date_format(date_trunc('day', from_unixtime({snapshot}/1000)), '%Y-%m-%d')
    and session_id = '{ session_id }'
), timebuckets as (
select
	timestamp_ms,
	floor(to_unixtime(timestamp_ms)*10)/10 as timebucket,
	radar_track.object_distance_m as object_distance_m,

	latency_camera.latency_ms as camera_latency_ms,
    	(joystick_latency.joystick_latency_ns/1000000) as command_latency_ms,
    	e2e_latency.latency_ms as e2e_latency_ms
from ree_cloud_data_{ environment }.telemetry
where
    ts in (select ts from ts_range)
    and session_id = '{ session_id }'
    -- and (latency_camera is not null or joystick_latency is not null or e2e_latency is not null)
    and abs({snapshot}/1000.0 - to_unixtime(timestamp_ms)) < {time_horizon_sec}
order by timestamp_ms
)
select 
    max(camera_latency_ms) as max_camera_latency_ms,
    max(command_latency_ms) as max_command_latency_ms,
    max(e2e_latency_ms) as max_e2e_latency_ms,
    {snapshot},
    timebucket,
    timebucket*1000 - {snapshot} as ts
    -- from_unixtime(timebucket)  as ts
    -- date_add('millisecond', (timebucket%100)*10 ,from_unixtime(timebucket/100))  as ts
from timebuckets
group by timebucket
order by timebucket

>>> timebucket*1000 - ciccio as ts
>>> timebucket*1000 - {snapshot} as ts
>>> timebucket*1000 - ciccio as ts
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 59, in <module>
    print("timebucket*1000 - {snapshot} as ts".format(5))
KeyError: ' snapshot '
>>> 
>>> with ts_range as (
select distinct ts
from ree_cloud_data_{ environment }.telemetry
where 
    dt = date_format(date_trunc('day', from_unixtime({snapshot}/1000)), '%Y-%m-%d')
    and session_id = '{ session_id }'
), timebuckets as (
select
	timestamp_ms,
	floor(to_unixtime(timestamp_ms)*10)/10 as timebucket,
	radar_track.object_distance_m as object_distance_m,

	latency_camera.latency_ms as camera_latency_ms,
    	(joystick_latency.joystick_latency_ns/1000000) as command_latency_ms,
    	e2e_latency.latency_ms as e2e_latency_ms
from ree_cloud_data_{ environment }.telemetry
where
    ts in (select ts from ts_range)
    and session_id = '{ session_id }'
    -- and (latency_camera is not null or joystick_latency is not null or e2e_latency is not null)
    and abs({snapshot}/1000.0 - to_unixtime(timestamp_ms)) < {time_horizon_sec}
order by timestamp_ms
)
select 
    max(camera_latency_ms) as max_camera_latency_ms,
    max(command_latency_ms) as max_command_latency_ms,
    max(e2e_latency_ms) as max_e2e_latency_ms,
    {snapshot},
    timebucket,
    timebucket*1000 - {snapshot} as ts
    -- from_unixtime(timebucket)  as ts
    -- date_add('millisecond', (timebucket%100)*10 ,from_unixtime(timebucket/100))  as ts
from timebuckets
group by timebucket
order by timebucket

>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 

>>> >>> 
>>> 
>>> 
>>> with ts_range as (
select distinct ts
from ree_cloud_data_ciccio.telemetry
where 
    dt = date_format(date_trunc('day', from_unixtime({snapshot}/1000)), '%Y-%m-%d')
    and session_id = ciccia
), timebuckets as (
select
	timestamp_ms,
	floor(to_unixtime(timestamp_ms)*10)/10 as timebucket,
	radar_track.object_distance_m as object_distance_m,

	latency_camera.latency_ms as camera_latency_ms,
    	(joystick_latency.joystick_latency_ns/1000000) as command_latency_ms,
    	e2e_latency.latency_ms as e2e_latency_ms
from ree_cloud_data_{ environment }.telemetry
where
    ts in (select ts from ts_range)
    and session_id = '{ session_id }'
    -- and (latency_camera is not null or joystick_latency is not null or e2e_latency is not null)
    and abs({snapshot}/1000.0 - to_unixtime(timestamp_ms)) < {time_horizon_sec}
order by timestamp_ms
)
select 
    max(camera_latency_ms) as max_camera_latency_ms,
    max(command_latency_ms) as max_command_latency_ms,
    max(e2e_latency_ms) as max_e2e_latency_ms,
    {snapshot},
    timebucket,
    timebucket*1000 - {snapshot} as ts
    -- from_unixtime(timebucket)  as ts
    -- date_add('millisecond', (timebucket%100)*10 ,from_unixtime(timebucket/100))  as ts
from timebuckets
group by timebucket
order by timebucket

>>> with ts_range as (
select distinct ts
from ree_cloud_data_ciccio.telemetry
where 
    dt = date_format(date_trunc('day', from_unixtime(pasticcio/1000)), '%Y-%m-%d')
    and session_id = 'ciccia'
), timebuckets as (
select
	timestamp_ms,
	floor(to_unixtime(timestamp_ms)*10)/10 as timebucket,
	radar_track.object_distance_m as object_distance_m,

	latency_camera.latency_ms as camera_latency_ms,
    	(joystick_latency.joystick_latency_ns/1000000) as command_latency_ms,
    	e2e_latency.latency_ms as e2e_latency_ms
from ree_cloud_data_{ environment }.telemetry
where
    ts in (select ts from ts_range)
    and session_id = 'ciccia'
    -- and (latency_camera is not null or joystick_latency is not null or e2e_latency is not null)
    and abs(pasticcio/1000.0 - to_unixtime(timestamp_ms)) < {time_horizon_sec}
order by timestamp_ms
)
select 
    max(camera_latency_ms) as max_camera_latency_ms,
    max(command_latency_ms) as max_command_latency_ms,
    max(e2e_latency_ms) as max_e2e_latency_ms,
    pasticcio,
    timebucket,
    timebucket*1000 - pasticcio as ts
    -- from_unixtime(timebucket)  as ts
    -- date_add('millisecond', (timebucket%100)*10 ,from_unixtime(timebucket/100))  as ts
from timebuckets
group by timebucket
order by timebucket

>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 62, in <module>
    
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 448, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 880, in __init__
    self._make_engine(self.engine)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 1114, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 1891, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 374, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File /home/sabeiro/lav/rem/raw/incident_list.csv does not exist: '/home/sabeiro/lav/rem/raw/incident_list.csv'
>>> incL
                               session_id  ...                                              _col3
0    d7a647a5-83a1-45f6-b974-2f7cb8867625  ...                         [FAULT_CAMERA_E2E_LATENCY]
1    3ff2265d-4ddb-40cc-bb50-1bdcda9e90aa  ...                            [FAULT_CONTROL_LATENCY]
2    a71efb5b-21fe-4e2a-9c17-77b294b308ee  ...  [FAULT_CAMERA_E2E_LATENCY, FAULT_CAMERA_E2E_LA...
3    b682b88f-7aea-4fc7-a9c0-abc34d22fbc5  ...  [FAULT_CAMERA_E2E_LATENCY, FAULT_CAMERA_E2E_LA...
4    b682b88f-7aea-4fc7-a9c0-abc34d22fbc5  ...                         [FAULT_CAMERA_E2E_LATENCY]
..                                    ...  ...                                                ...
239  383cb841-cb63-4dab-af62-b65a6ef40280  ...  [FAULT_CAMERA_E2E_LATENCY, FAULT_CAMERA_E2E_LA...
240  383cb841-cb63-4dab-af62-b65a6ef40280  ...  [FAULT_CAMERA_E2E_LATENCY, FAULT_CAMERA_E2E_LA...
241  af8d77fe-2913-4cfc-a29d-5d992b7a2f1f  ...                         [FAULT_CAMERA_E2E_LATENCY]
242  162da9ab-d5d8-4241-8d93-3754900d8ceb  ...  [FAULT_CAMERA_E2E_LATENCY, FAULT_CAMERA_E2E_LA...
243  06687b9b-42f7-4130-b3b0-087cffb9df4c  ...  [FAULT_CAMERA_E2E_LATENCY, FAULT_CAMERA_E2E_LA...

[244 rows x 4 columns]
>>> snapshot
1593424805987
>>> session_id
'06687b9b-42f7-4130-b3b0-087cffb9df4c'
>>> with ts_range as (
select distinct ts
from ree_cloud_data_prod.telemetry
where 
    dt = date_format(date_trunc('day', from_unixtime(1593424805987/1000)), '%Y-%m-%d')
    and session_id = '06687b9b-42f7-4130-b3b0-087cffb9df4c'
), timebuckets as (
select
	timestamp_ms,
	floor(to_unixtime(timestamp_ms)*10)/10 as timebucket,
	radar_track.object_distance_m as object_distance_m,

	latency_camera.latency_ms as camera_latency_ms,
    	(joystick_latency.joystick_latency_ns/1000000) as command_latency_ms,
    	e2e_latency.latency_ms as e2e_latency_ms
from ree_cloud_data_{ environment }.telemetry
where
    ts in (select ts from ts_range)
    and session_id = '06687b9b-42f7-4130-b3b0-087cffb9df4c'
    -- and (latency_camera is not null or joystick_latency is not null or e2e_latency is not null)
    and abs(1593424805987/1000.0 - to_unixtime(timestamp_ms)) < {time_horizon_sec}
order by timestamp_ms
)
select 
    max(camera_latency_ms) as max_camera_latency_ms,
    max(command_latency_ms) as max_command_latency_ms,
    max(e2e_latency_ms) as max_e2e_latency_ms,
    1593424805987,
    timebucket,
    timebucket*1000 - 1593424805987 as ts
    -- from_unixtime(timebucket)  as ts
    -- date_add('millisecond', (timebucket%100)*10 ,from_unixtime(timebucket/100))  as ts
from timebuckets
group by timebucket
order by timebucket

>>> fileN
'/home/sabeiro/lav/rem/raw/incindent_1593424805987'
>>> QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c066883e-b1b1-4fec-ba33-5bbf368b55db.csv
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 69, in <module>
    download_query(query,fileN)
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 27, in download_query
    location = a_u.exec_athena(athena,query,params)
  File "/home/sabeiro/lav//src/sawmill/sawmill/aws_utils.py", line 18, in exec_athena
    q_id = athena.start_query_execution(
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 316, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 635, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.InvalidRequestException: An error occurred (InvalidRequestException) when calling the StartQueryExecution operation: line 18:2: extraneous input 'latency_camera' expecting {',', ')', 'FROM', 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'UNION', 'EXCEPT', 'INTERSECT'}
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 69, in <module>
    download_query(query,fileN)
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 27, in download_query
    location = a_u.exec_athena(athena,query,params)
  File "/home/sabeiro/lav//src/sawmill/sawmill/aws_utils.py", line 18, in exec_athena
    q_id = athena.start_query_execution(
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 316, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 635, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.InvalidRequestException: An error occurred (InvalidRequestException) when calling the StartQueryExecution operation: Queries of this type are not supported
>>> query
"with ts_range as (\nselect distinct ts\nfrom ree_cloud_data_prod.telemetry\nwhere \n    dt = date_format(date_trunc('day', from_unixtime(1593424805987/1000)), '%Y-%m-%d')\n    and session_id = '06687b9b-42f7-4130-b3b0-087cffb9df4c'\n), timebuckets as (\nselect\n\ttimestamp_ms,\n\tfloor(to_unixtime(timestamp_ms)*10)/10 as timebucket,\n\tradar_track.object_distance_m as object_distance,\n\tsteering_wheel_state,\n\tsum(case when steering_wheel_state is null then 0 else 1 end) over (order by timestamp_ms) as steering_partition,\n\tbrake_pressure,\n\tsum(case when brake_pressure is null then 0 else 1 end) over (order by timestamp_ms) as brake_partition,\n\n\n\tlatency_camera.latency_ms as camera_latency_ms,\n    \t(joystick_latency.joystick_latency_ns/1000000) as command_latency_ms,\n    \te2e_latency.latency_ms as e2e_latency_ms\nfrom ree_cloud_data_prod.telemetry\nwhere\n    ts in (select ts from ts_range)\n    and session_id = '06687b9b-42f7-4130-b3b0-087cffb9df4c'\n    -- and (latency_camera is not null or joystick_latency is not null or e2e_latency is not null)\n    and abs(1593424805987/1000.0 - to_unixtime(timestamp_ms)) < 20\norder by timestamp_ms\n)\nselect \n    timebucket,\n    timebucket*1000 - 1593424805987 as ts,\n\n    avg(object_distance) as object_distance,\n    avg(steering_partition) as steering_partition,\n    avg(brake_pressure) as break_pressure,\n    avg(brake_partition) as break_partition,\n    \n\n    max(camera_latency_ms) as camera_latency,\n    max(command_latency_ms) as joystick_latency,\n    max(e2e_latency_ms) as e2e_latency,    \n    -- from_unixtime(timebucket)  as ts\n    -- date_add('millisecond', (timebucket%100)*10 ,from_unixtime(timebucket/100))  as ts\nfrom timebuckets\ngroup by timebucket\norder by timebucket\n"
>>> print(query)
with ts_range as (
select distinct ts
from ree_cloud_data_prod.telemetry
where 
    dt = date_format(date_trunc('day', from_unixtime(1593424805987/1000)), '%Y-%m-%d')
    and session_id = '06687b9b-42f7-4130-b3b0-087cffb9df4c'
), timebuckets as (
select
	timestamp_ms,
	floor(to_unixtime(timestamp_ms)*10)/10 as timebucket,
	radar_track.object_distance_m as object_distance,
	steering_wheel_state,
	sum(case when steering_wheel_state is null then 0 else 1 end) over (order by timestamp_ms) as steering_partition,
	brake_pressure,
	sum(case when brake_pressure is null then 0 else 1 end) over (order by timestamp_ms) as brake_partition,


	latency_camera.latency_ms as camera_latency_ms,
    	(joystick_latency.joystick_latency_ns/1000000) as command_latency_ms,
    	e2e_latency.latency_ms as e2e_latency_ms
from ree_cloud_data_prod.telemetry
where
    ts in (select ts from ts_range)
    and session_id = '06687b9b-42f7-4130-b3b0-087cffb9df4c'
    -- and (latency_camera is not null or joystick_latency is not null or e2e_latency is not null)
    and abs(1593424805987/1000.0 - to_unixtime(timestamp_ms)) < 20
order by timestamp_ms
)
select 
    timebucket,
    timebucket*1000 - 1593424805987 as ts,

    avg(object_distance) as object_distance,
    avg(steering_partition) as steering_partition,
    avg(brake_pressure) as break_pressure,
    avg(brake_partition) as break_partition,
    

    max(camera_latency_ms) as camera_latency,
    max(command_latency_ms) as joystick_latency,
    max(e2e_latency_ms) as e2e_latency,    
    -- from_unixtime(timebucket)  as ts
    -- date_add('millisecond', (timebucket%100)*10 ,from_unixtime(timebucket/100))  as ts
from timebuckets
group by timebucket
order by timebucket

>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 69, in <module>
    download_query(query,fileN)
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 27, in download_query
    location = a_u.exec_athena(athena,query,params)
  File "/home/sabeiro/lav//src/sawmill/sawmill/aws_utils.py", line 18, in exec_athena
    q_id = athena.start_query_execution(
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 316, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 635, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.InvalidRequestException: An error occurred (InvalidRequestException) when calling the StartQueryExecution operation: Queries of this type are not supported
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 69, in <module>
    download_query(query,fileN)
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 27, in download_query
    location = a_u.exec_athena(athena,query,params)
  File "/home/sabeiro/lav//src/sawmill/sawmill/aws_utils.py", line 18, in exec_athena
    q_id = athena.start_query_execution(
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 316, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 635, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.InvalidRequestException: An error occurred (InvalidRequestException) when calling the StartQueryExecution operation: Queries of this type are not supported
>>> QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f8592247-9e19-4908-85f1-94a785c12da6.csv
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 69, in <module>
    download_query(query,fileN)
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 27, in download_query
    location = a_u.exec_athena(athena,query,params)
  File "/home/sabeiro/lav//src/sawmill/sawmill/aws_utils.py", line 18, in exec_athena
    q_id = athena.start_query_execution(
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 316, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 635, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.InvalidRequestException: An error occurred (InvalidRequestException) when calling the StartQueryExecution operation: Queries of this type are not supported
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 69, in <module>
    download_query(query,fileN)
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 27, in download_query
    location = a_u.exec_athena(athena,query,params)
  File "/home/sabeiro/lav//src/sawmill/sawmill/aws_utils.py", line 18, in exec_athena
    q_id = athena.start_query_execution(
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 316, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 635, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.InvalidRequestException: An error occurred (InvalidRequestException) when calling the StartQueryExecution operation: Queries of this type are not supported
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 69, in <module>
    download_query(query,fileN)
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 27, in download_query
    location = a_u.exec_athena(athena,query,params)
  File "/home/sabeiro/lav//src/sawmill/sawmill/aws_utils.py", line 18, in exec_athena
    q_id = athena.start_query_execution(
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 316, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/botocore/client.py", line 635, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.InvalidRequestException: An error occurred (InvalidRequestException) when calling the StartQueryExecution operation: Queries of this type are not supported
>>> QUEUED
QUEUED
QUEUED
QUEUED
QUEUED
QUEUED
QUEUED
QUEUED
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/23663253-1854-41f5-a60d-e379d8e66257.csv
>>> QUEUED
FAILED
SYNTAX_ERROR: line 33:5: Unexpected parameters (row("brake_pressure" real)) for function avg. Expected: avg(double) , avg(bigint) , avg(real) , avg(interval day to second) , avg(interval year to month) , avg(decimal(p,s)) 
>>> QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/97e568b5-243f-43db-9f62-474d60793b38.csv
>>> QUEUED
FAILED
SYNTAX_ERROR: line 35:9: Column 'steering_wheel_state' cannot be resolved
>>> -------------------------execute-incident-list---------------------------
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4ce11dad-bcfc-4ffd-b308-4aa56146024c.csv
>>> QUEUED
FAILED
SYNTAX_ERROR: line 33:9: Column 'brake_pressure.breake_pressure' cannot be resolved
>>> QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8aaff330-fec6-4338-bf9d-9e1b760a532a.csv
>>> QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/04392b36-89b6-4c95-8c8b-7487ee973516.csv
>>> QUEUED
FAILED
SYNTAX_ERROR: line 43:9: Column 'yaw_rate' cannot be resolved
>>> QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d7bbe88f-a291-4907-ba12-1e40f3b0d654.csv
>>> QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/79b3a28f-1527-4089-b4c0-d8a04f477d2e.csv
>>> 
>>> 
>>> FAILED
SYNTAX_ERROR: line 56:9: Column 'vehicle_ping_ms' cannot be resolved
>>> 
>>> RUNNING
FAILED
SYNTAX_ERROR: line 57:9: Column 'rtp_loss' cannot be resolved
>>> QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5d3eec64-fac0-4df5-ae23-fdbc241f276c.csv
>>> QUEUED
RUNNING


RUNNING
SUCCEEDED
s3://ree-cloud-athena/c1ab1fad-28e4-484e-9bd5-564c0e2aab3f.csv
>>> RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d83777f3-8151-4b3a-8348-ad8313430757.csv
>>> QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/87bca7ce-bce0-4e5c-b0e8-e1b5f05f66af.csv
>>> -------------------------execute-incident-list---------------------------
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/db18d0f6-448b-48e5-b2cc-39d9981d6c7a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b87922c3-7b2c-48b2-8f49-fa082ed7fbd5.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f2925c50-8991-4325-b226-ce255e025b9e.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e9f1ebd9-432c-4d23-b13a-a2a7358da965.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/02893626-a382-4bb9-9504-d49c7279f7d5.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e0a693f8-2774-4f71-b980-2bab3c66a837.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/67c54700-de4d-441f-964e-b59186ca4a6b.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f6fc7cd3-6769-4259-8182-d09e41f310a5.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/34057d73-ce99-4d4a-a7c0-d04e253eafd2.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/eb48f0c0-afc2-4178-997c-2ebdf96d3e5a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/65d7b85d-c5ab-4c6f-b536-08df5e53b7ef.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/fe506872-2d01-4e0c-ac2c-7f64261c99d1.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/837155c8-0e6d-4f5f-9e62-b8cd177b166d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4a9a630a-d521-4c13-8ed8-0c82db64aee0.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0304f75c-66f7-4f7f-991a-08c12234050f.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/07bd07b3-d260-47d5-8d70-5b9dc51d198c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f9fedee1-a9c5-4074-a6a0-b7e851665673.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b08360a7-68b0-4d3d-bf28-78132bed8929.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/979b1062-ba3b-461d-927e-035aaa64b761.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3c1f4f3e-0b1e-4b82-8c58-31b883693387.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/835d0bb2-a2ea-4912-8a53-490c6bcc3949.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5631efcc-ee24-4fa4-8c4b-1de29f8e3f67.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0a8ea0e1-49b7-4b9d-a3b8-2b811c1e9c52.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2db74191-6642-42ba-963c-adf7126b17cc.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/67f7a186-a8b1-4d61-b44b-326f549f0667.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d0f1adb4-7e19-443d-846f-657e884e39b1.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5aa9316b-7bb2-4c40-83bb-fb95534c03d6.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d5c9dbda-3d21-433b-a252-f2380863322f.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/eced5fda-6d91-493f-ac66-3329f8a73488.csv
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/fe543c81-b4a0-4859-af72-049ba3d0e4f1.csv
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4e32b3f5-bef5-4c46-a587-83b2b925b412.csv
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7c520ab7-0925-46b7-a4c8-f987fe113c5a.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/254d2fd4-5566-41cb-946e-6cbd86650fd7.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d1cdbd50-f7a4-4176-ba3a-3d1c87f08573.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b4dc1f2f-7355-43e9-92ee-afb2e349fd0b.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/74a43144-187f-4408-8c0a-b9b30ea09b90.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a08debfc-941e-4a33-ab86-0961ffc4f0aa.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a15d8048-c47b-4c50-b9d4-eb705bf92448.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4a36d6a0-bd23-4f56-884d-6fe7afecf557.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0bd6e13d-d54d-4ebd-80c0-89609d0a6e62.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2d372a60-9e90-412d-acc4-ee6ca362ee9c.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/387e9f17-e699-421b-80d1-6ab89e41ff50.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/080f577a-9e99-45e7-b655-32bdd39856c5.csv
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d8e4c913-d2de-4351-bc83-a10a6d96af2d.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/25dd4fbe-5274-4d54-9132-f7fd8a42faa3.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/63e0b21f-7918-4053-83f8-adc9fde5c50d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5c8dece5-03fe-4793-b9e0-633c8705c765.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4794649d-308f-421d-8c1e-e125eeb4a801.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4232f8d6-928b-4bb2-a54a-f630f5b43f53.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/dc0ec188-9237-489e-8ddc-d05017f47590.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/547ff020-4902-4b6b-a1f4-37ceff118cc9.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ea338327-6c78-4f56-954e-cce6d2714955.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4557e318-3dbf-466f-bbda-1a0194305a69.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d30cda51-1a01-451b-a593-386d77998f72.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5d3d043d-8d6b-48bf-9ec5-1004e9a422c8.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b466f8eb-262a-4e23-8889-f267a85387c7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f525bb0d-56f0-4d3a-a00f-86ed8644daae.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/24bb1ae5-56b8-4f55-9bd4-3d835fcd40db.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f56014fd-68ff-4ca8-b7b6-5ec7456961f6.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/82127175-5d64-4e27-86ab-5f6ce705c0e8.csv
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/137cad63-a842-4b4e-a76c-e9a59eeb108c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0ad56357-722e-4d7f-a9fd-91cfd3aa2c6d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b3fd77e0-9fe7-45e1-b3bb-1b964187b778.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f83582ed-f813-42b3-97bc-10936b9a0a57.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/156d6d07-8c2b-434b-977a-5d10ac79f9fb.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e5c24cf1-57de-4c7d-9a2d-f94dec49c0fb.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/26276d46-e3cf-4d81-ab2a-25d1d9ec7600.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/25e70312-bad0-4526-9b07-8934cfb4470f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5b1d12c9-c4f9-4a99-8de7-dfc6e8a6d4b3.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d941e362-1246-4fe0-a989-1d0aa06a3273.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ef153fed-395f-4543-a174-212998cf62e9.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ff593fbd-8602-429f-b2e1-29934091eb86.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/be33a24b-a48f-4fb5-b8ed-c49fd7830802.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8fa38e74-4bc7-4fee-ba99-00b24ecfffbd.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/90e9603c-19f4-4092-abb9-00a0bfb21b81.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f38a9b65-01ad-4fdb-9bb8-c3566910e2cd.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/cffa82bd-52c4-4f35-9097-08cd48b20ccb.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ddf84566-6a67-4f14-80e3-88c92dd3777b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4354c659-9a05-41ef-a38c-b2eb2fe0b814.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5323fb7e-2352-49e7-b94c-d353d81613bd.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/79df8ab7-996b-4ee6-bb43-54d9e14a273a.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/77d1b893-288e-4c45-ada1-4fb6ddc996b7.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6158cf16-b541-440c-8a8d-5dfae9cffb80.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/99603bfd-f763-46c4-bbe5-43df3d8a5de6.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/da64967a-31e8-499f-b404-328049905afe.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2709074d-3f56-4f5d-805f-2992949c3f2d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d7b3d0a1-85cf-40e3-8be6-cc4b15c296d2.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d1a1132f-78f5-4807-a3a8-d5d32e05ccea.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/cb9f9a2e-ab3c-42bd-8b49-a746e1e30602.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2b450bc9-28ff-48ed-b2c2-f47be4d6ada1.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/86b76b2d-ca4b-47c6-837c-9f66c138cf1f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4816b261-f559-42c4-b964-4fe1d6827c83.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d578f336-8398-40ed-b018-75801b2dd0ae.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ac8de1eb-704c-47bd-8bef-8ca4f6cabbbf.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/301af1e8-ec7d-44ab-8735-6c6560e1fa89.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/354f7f4c-03bf-4454-8a97-157658666273.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/faf8b7ff-89bf-4724-b2cc-bb56f1e756a5.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6e326f70-415b-454a-91ee-6663aa25a4bb.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a1a70578-870b-42de-a6da-12ccf29b6297.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/60e5f50d-492b-4210-8ca7-77b525c4d952.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b35216fc-2378-42c0-8a4b-3e492235e052.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8d7816f9-8fe6-45ef-a83f-8d28c3354bd5.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e1306456-8914-4b50-b069-fda14c383495.csv
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3bd0f327-6fb9-475c-8371-f84bc442b86c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/68d467a0-9bce-49fa-84f6-4139a363d3c9.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/05f98f0c-f120-4f59-8a34-0654b036052f.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/cdcb378e-934d-451c-854d-c77ef43c9876.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/fc1613dc-5627-4aa8-a57b-a9b9bb40283a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6114737f-c016-4189-ba96-a6ffaa556794.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d582b41e-02af-4f61-97a2-d6dc3e29f0c0.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d746e9d7-aab8-445c-b9ac-a3e32533855d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/caa54464-c460-481c-9916-402db3db1c45.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e8ef28ef-7737-428d-b3bc-f4b831f3bc7b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3ade90e8-0bc7-4180-aa38-dbcf9276976c.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/93c7fe4c-6987-4aa1-acff-e6dd0de26f8e.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2c1dfabb-aecf-4996-b707-fb5880a6d819.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/88b02c24-dcf6-45a2-b9b0-613e4ac0227f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/65cdf11e-50ca-478b-98ed-24048b351735.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/45b14bcf-fb2b-475a-802e-196838f3c123.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/bd35fead-b7e8-479a-9b4f-40bc11de6d96.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8069ee7f-17ba-433a-84a5-442322280b62.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/68de9ff1-c9ee-47e4-97a3-c98837b2838e.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b50d6edc-5a2b-4e3c-930a-790a94be73df.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c0c67f5b-d1a0-4cf6-b5aa-e564b4360d52.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/05efee9e-9d75-42cc-8218-ac13cb133623.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e2e388ae-fb7c-4116-9631-5056598b94b6.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f54ca5b6-2300-4a96-9885-8e9de9c225ba.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/afab4b8a-c831-4875-a44b-d31a9e0ec811.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/488445d3-84fc-47f0-bc14-f01f7e0c6404.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5d05ca23-9d92-463b-835c-fc4958abd7a5.csv
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/acd191de-825d-4e46-8493-bc24d23d158b.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/29b17194-90d1-4719-bd83-3e1cfd90be7b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/344ae12f-51d1-46a9-9e1d-078bb09d71d7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2c2d18b5-0a36-4e3d-8fff-8d38dd21eabd.csv
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4592e709-ada7-469f-bdb5-cfb3cb1321d0.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3d027010-d68a-4781-841e-0dd16a33576c.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/67e92ad2-f81d-44f4-ad4f-7da51ae64236.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ad8fa7e0-d54a-47f3-996d-efc9c1a315f6.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5fde90eb-61a1-486f-b589-a52bff62e234.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ceb2c493-f0de-4303-8d98-ed899bd9a749.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d3919df8-5cc3-4b5d-b2d5-b0d74993b564.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/727ecbda-7c51-49a4-bc1f-24af312448b5.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/812e07f0-158d-4bf8-a195-32a99b8772e4.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6a3c2c0e-07f5-4a58-9502-aaa73e608bc0.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e54f4217-739e-4dd3-87e8-a49f2f4fcd04.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/cebab9a6-b266-4b92-ba7c-04c472bd0cc5.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/eecd934a-8d87-40f0-bc76-b301fd55ea5d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/dc7dd53b-6ca9-41f5-8ddb-805f09598740.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8957151d-fa65-4b05-84b7-94242817ddc2.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d1d5018f-e4bf-4e37-82ce-d46332aa2f52.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c3ef330f-34ee-4187-8b2f-4454cdd79131.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/420d10bb-c635-478d-a49d-5d90a924ff75.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/bcb18ad5-d7eb-4826-92d2-2c5be3de61b5.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/779bf190-4cc8-49c7-9e71-d1d200a45814.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a880a7a7-535c-48d1-a4b5-cbe1320f3487.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4c292d15-2d1f-4d7b-8f66-dcf68c2bc3f8.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6db7b1b1-2ef5-4cd1-8316-0134409b36c7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1d4b6c8a-1c83-4808-be1c-20549808b810.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/910d832f-f87b-466c-8404-2a212d422766.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/aafdff22-9075-4293-b690-ad7a77053c07.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/930f6304-26f6-4630-bc2d-28728d899b01.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2ce23a75-a89e-48e7-b575-d001ed0ff39a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a8e189e7-8e75-434f-8498-54128b4df824.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b169bcb5-ef84-4529-b890-d0e7d6188518.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b97ed568-67de-4748-b4c3-f1222bb076aa.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/624c2088-e156-48a4-8f4a-0211a1ce48e4.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4c15e861-c7df-4c2b-9700-6018bd564193.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/456b3fc9-5b56-4667-92e0-3cf942c28539.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8c4fe807-3928-428f-9ca6-079fee97ee00.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/aa799382-be96-49f3-a6bd-1c8186bc585c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d77681ca-6214-4fa6-91e4-df3188d8e52c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/10456a9b-3bf2-42f7-b08b-e2e367e87b07.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ec0d4317-9e69-47fc-9b4f-64e70ed0c8aa.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/21a68501-4898-4440-8de9-23bb5dcdfcd8.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f309fdea-3c86-4921-b8e4-db52ec8422ff.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/55a24347-0374-48bb-b38d-67bee4cff0e9.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6d2c3da1-2ab2-48ef-bd8d-2a268d2d3c91.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c44fd260-9c91-434c-b7c1-f82691641920.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d76bb4a9-52ea-4bf5-8325-80bb67aaab60.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7cf5a889-87c5-498c-a1d1-ae5457aa60a9.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ae9c308e-0f3c-4b52-a003-62520cbb52d9.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/233a8114-fce5-47ff-9da2-77273b37f319.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5e445988-f0a3-4087-8963-cb2e95d89dfd.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ccbd3fa6-5faf-4dda-9b48-d9c6926d57a6.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/338f02e4-daa3-482b-84e8-51d24344169a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8f350e22-06f3-4da4-bd53-f3571045a5be.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5073ad72-fadd-45d6-b7dd-e4997e9eb962.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f42b8689-8053-41ee-ba92-64c109702507.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8a8df47b-b225-456c-8e65-d7828fed8233.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d9f927c7-9b4d-4832-838d-b4cb4f7060b7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/fdafc4e8-4d1c-4d7d-b0b5-3fa95566f58b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1ed0cfe4-f73e-44e4-8276-3b603d5a9e74.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7936cfa4-3d94-4141-bf67-0ad995759c0b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d5480199-c48a-4586-a2bd-f12c5905b531.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c5c4dfe2-b512-4178-80a8-29de1d520ae4.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/86d7e0fc-ace8-47aa-82b2-54f20c6fbbdb.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/50a20d5b-2ef4-4f8c-95e5-1d515d49fe16.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/697b3182-632d-4d01-83f8-32d36032069b.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3fc8cd49-8050-464b-bcc3-f076e399611a.csv
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ac924aa1-6466-40c7-842d-27564292365d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c5b2e984-9347-426d-b008-6789f8586063.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f241de34-c823-4f16-9340-5db5e4994246.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2d9e8513-7183-4eff-89b3-714c2b831879.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a2294c1b-acc5-485c-b43d-f1b92fcacfe5.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b89227ac-9750-4650-8b83-4d1a39eebb1c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1a9ccc42-2af2-4b63-9298-221e755fa12f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/091d8957-dc59-4afd-aece-144ba7a2026e.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4b3f8d12-bd92-4604-a27e-56b7ff26d7f8.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b5afe2b9-4b37-4be1-a17b-fb5ef9002c90.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/931b29f7-215d-4af4-adc7-9ed615aeb0c2.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8c91e5a5-8415-4526-98a6-c9627b3e7f90.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3ed82bb9-0842-48cc-897b-15288a837e71.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1ee5b635-235f-421e-b810-b548205aada6.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/272c56ff-cb84-4b81-a58b-5ac593e88609.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/aac2de9c-5cd3-4bdd-b9e8-cf144434389e.csv
QUEUED
QUEUED
QUEUED
SUCCEEDED
s3://ree-cloud-athena/9e747549-d5b4-4045-ad57-0478b09bda61.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6758319f-ecd7-4854-8ad7-a2c9114b5e0d.csv
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b9604c9d-c8f0-42c9-961b-10dd86f1d2e2.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4ca09be4-553f-4e38-ab4b-611ab7b4c3dd.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2553f14a-d25a-4bae-bce6-a488f46f808a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ff166b1c-79a4-4d10-a2c6-713ed107f481.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/cdeb19d8-889d-4ec1-9515-dea692c598aa.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e474841e-09b1-489f-b6da-355b95466bb0.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b8aed78d-499c-43c1-bd9a-07fabc7a51da.csv
QUEUED
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8d7addfc-e826-4a60-82fc-adc933dc1561.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6dc832d8-a28e-4001-b5f4-e4bea03dd0e7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b240847b-d202-4274-a234-2c4b2f2e6acb.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5d1ad789-4847-46c6-a01d-36a4cb3ca11c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5f2bb7fd-1160-4d9d-bf3a-b91c207f46c0.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/342cb01b-eb31-4975-b450-f889bdf5c999.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1e6b10f9-0a1a-4930-b3bd-007d9f47b90f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/9ffac770-a8c4-40f4-9d03-72b424aeff9e.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c4838aba-a870-438d-bacb-0d217849ee17.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/65a038bf-4e10-4c64-af90-15593fb80f5c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/dbaf281d-925b-4ad8-a394-8a94b421e8fa.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b0b49756-7910-43f3-ad91-322f578bc9b6.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2e51572f-c480-4adc-9549-530b6fe3e852.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/bb6b0750-6cd3-4505-8cae-b6bab98f3ffc.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5bbb2f3c-0751-41d1-86cc-c0a94a8d017a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/fb3d1aeb-73e5-442c-9294-5465afb2a99b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1a84528c-788d-45bd-aed1-02d4122e17c5.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3ac4280c-7673-4853-84b9-a3d35021f987.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a7a706b4-59d1-45ef-b79a-6565329daae5.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/867a1754-642f-483c-9936-fc253c97c36e.csv
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 23, in <module>
    fL = os.listDir(projDir)
AttributeError: module 'os' has no attribute 'listDir'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 23, in <module>
    fL = os.listDir(projDir)
AttributeError: module 'os' has no attribute 'dirList'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 25, in <module>
    feat = pd.read_csv(projDir + f).ffill()
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 448, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 880, in __init__
    self._make_engine(self.engine)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 1114, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 1891, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 374, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File /home/sabeiro/lav//rem/raw/incidentincindent_1596459206367.csv does not exist: '/home/sabeiro/lav//rem/raw/incidentincindent_1596459206367.csv'
>>> feat
       timebucket       ts  object_distance  ...  camera_latency  joystick_latency  e2e_latency_ms
0    1.596459e+09 -20067.0              NaN  ...           118.0               NaN             NaN
1    1.596459e+09 -19967.0              NaN  ...           139.0               0.0             NaN
2    1.596459e+09 -19867.0              NaN  ...           141.0               0.0             NaN
3    1.596459e+09 -19767.0              NaN  ...           138.0               0.0             NaN
4    1.596459e+09 -19667.0              NaN  ...           121.0               0.0             NaN
..            ...      ...              ...  ...             ...               ...             ...
396  1.596459e+09  19533.0              NaN  ...           128.0               0.0             NaN
397  1.596459e+09  19633.0              NaN  ...           133.0               0.0             NaN
398  1.596459e+09  19733.0              NaN  ...           141.0               0.0             NaN
399  1.596459e+09  19833.0              NaN  ...           146.0               0.0             NaN
400  1.596459e+09  19933.0              NaN  ...           142.0               0.0             NaN

[401 rows x 38 columns]
>>> p = pd.read_csv()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: parser_f() missing 1 required positional argument: 'filepath_or_buffer'
>>> feat
       timebucket       ts  object_distance  ...  camera_latency  joystick_latency  e2e_latency_ms
0    1.596459e+09 -20067.0              NaN  ...           118.0               0.0             NaN
1    1.596459e+09 -19967.0              NaN  ...           139.0               0.0             NaN
2    1.596459e+09 -19867.0              NaN  ...           141.0               0.0             NaN
3    1.596459e+09 -19767.0              NaN  ...           138.0               0.0             NaN
4    1.596459e+09 -19667.0              NaN  ...           121.0               0.0             NaN
..            ...      ...              ...  ...             ...               ...             ...
396  1.596459e+09  19533.0              NaN  ...           128.0               0.0             NaN
397  1.596459e+09  19633.0              NaN  ...           133.0               0.0             NaN
398  1.596459e+09  19733.0              NaN  ...           141.0               0.0             NaN
399  1.596459e+09  19833.0              NaN  ...           146.0               0.0             NaN
400  1.596459e+09  19933.0              NaN  ...           142.0               0.0             NaN

[401 rows x 38 columns]
>>> f
'incindent_1596459206367.csv'
>>> feat
       timebucket       ts  object_distance  ...  camera_latency  joystick_latency  e2e_latency_ms
0    1.595393e+09 -20056.0        14.800000  ...             162                29             181
1    1.595393e+09 -19956.0        14.850000  ...             166                30             188
2    1.595393e+09 -19856.0        15.000000  ...             166                30             190
3    1.595393e+09 -19756.0        15.133333  ...             144                30             185
4    1.595393e+09 -19656.0        15.250000  ...             209                29             184
..            ...      ...              ...  ...             ...               ...             ...
396  1.595393e+09  19544.0              NaN  ...             181                30             200
397  1.595393e+09  19644.0              NaN  ...             161                30             200
398  1.595393e+09  19744.0              NaN  ...             190                30             200
399  1.595393e+09  19844.0              NaN  ...             157                30             196
400  1.595393e+09  19944.0              NaN  ...             146                30             180

[401 rows x 38 columns]
>>> feat
       timebucket       ts  object_distance  ...  camera_latency  joystick_latency  e2e_latency_ms
0    1.595393e+09 -20056.0        14.800000  ...             162                29             181
1    1.595393e+09 -19956.0        14.850000  ...             166                30             188
2    1.595393e+09 -19856.0        15.000000  ...             166                30             190
3    1.595393e+09 -19756.0        15.133333  ...             144                30             185
4    1.595393e+09 -19656.0        15.250000  ...             209                29             184
..            ...      ...              ...  ...             ...               ...             ...
396  1.595393e+09  19544.0        12.300000  ...             181                30             200
397  1.595393e+09  19644.0        12.300000  ...             161                30             200
398  1.595393e+09  19744.0        12.300000  ...             190                30             200
399  1.595393e+09  19844.0        12.300000  ...             157                30             196
400  1.595393e+09  19944.0        12.300000  ...             146                30             180

[401 rows x 38 columns]
>>> feat
       timebucket       ts  object_distance  ...  camera_latency  joystick_latency  e2e_latency_ms
0    1.595393e+09 -20056.0        14.800000  ...             162                29             181
1    1.595393e+09 -19956.0        14.850000  ...             166                30             188
2    1.595393e+09 -19856.0        15.000000  ...             166                30             190
3    1.595393e+09 -19756.0        15.133333  ...             144                30             185
4    1.595393e+09 -19656.0        15.250000  ...             209                29             184
..            ...      ...              ...  ...             ...               ...             ...
396  1.595393e+09  19544.0              NaN  ...             181                30             200
397  1.595393e+09  19644.0              NaN  ...             161                30             200
398  1.595393e+09  19744.0              NaN  ...             190                30             200
399  1.595393e+09  19844.0              NaN  ...             157                30             196
400  1.595393e+09  19944.0              NaN  ...             146                30             180

[401 rows x 38 columns]
>>> feat.columns
Index(['timebucket', 'ts', 'object_distance', 'brake_pressure',
       'brake_partition', 'steering_partition', 'force_lat', 'force_lon',
       'yaw_rate', 'steering_wheel_deg', 'steering_angle_deg', 'td_brake',
       'td_throttle', 'wheel_speed', 'vehicle_ping', 'rtp_lost', 'rtp_late',
       'steering_interval', 'modem0_rtt', 'modem1_rtt', 'modem2_rtt',
       'modem3_rtt', 'modem0_rx', 'modem1_rx', 'modem2_rx', 'modem3_rx',
       'modem0_tx', 'modem1_tx', 'modem2_tx', 'modem3_tx', 'camera_jitter',
       'room_ram', 'room_cpu', 'vehicle_ram', 'vehicle_cpu', 'camera_latency',
       'joystick_latency', 'e2e_latency_ms'],
      dtype='object')
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'deci'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 28, in <module>
    yL = ['camera_latency', 'joystick_latency', 'e2e_latency']
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'deci'
>>> feat
                       timebucket       ts  ...  joystick_latency  e2e_latency_ms
timebucket                                  ...                                  
2020-07-22 06:38:00  1.595393e+09 -20056.0  ...                29             181
2020-07-22 06:38:00  1.595393e+09 -19956.0  ...                30             188
2020-07-22 06:38:00  1.595393e+09 -19856.0  ...                30             190
2020-07-22 06:38:00  1.595393e+09 -19756.0  ...                30             185
2020-07-22 06:38:00  1.595393e+09 -19656.0  ...                29             184
...                           ...      ...  ...               ...             ...
2020-07-22 06:38:40  1.595393e+09  19544.0  ...                30             200
2020-07-22 06:38:40  1.595393e+09  19644.0  ...                30             200
2020-07-22 06:38:40  1.595393e+09  19744.0  ...                30             200
2020-07-22 06:38:40  1.595393e+09  19844.0  ...                30             196
2020-07-22 06:38:40  1.595393e+09  19944.0  ...                30             180

[401 rows x 38 columns]
>>> feat.columns
Index(['timebucket', 'ts', 'object_distance', 'brake_pressure',
       'brake_partition', 'steering_partition', 'force_lat', 'force_lon',
       'yaw_rate', 'steering_wheel_deg', 'steering_angle_deg', 'td_brake',
       'td_throttle', 'wheel_speed', 'vehicle_ping', 'rtp_lost', 'rtp_late',
       'steering_interval', 'modem0_rtt', 'modem1_rtt', 'modem2_rtt',
       'modem3_rtt', 'modem0_rx', 'modem1_rx', 'modem2_rx', 'modem3_rx',
       'modem0_tx', 'modem1_tx', 'modem2_tx', 'modem3_tx', 'camera_jitter',
       'room_ram', 'room_cpu', 'vehicle_ram', 'vehicle_cpu', 'camera_latency',
       'joystick_latency', 'e2e_latency_ms'],
      dtype='object')
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 38, in <module>
    X = s_s.interpMissingMatrix(feat[xL])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: "['joystick_freq', 'cpu_usage', 'force_lateral', 'vehicle_jitter', 'radar_speed', 'ram_usage', 'room_jitter', 'force_longitudinal'] not in index"
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 41, in <module>
    t_v.plotTimeSeries(feat[tL])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: "['joystick_freq', 'e2e_latency', 'cpu_usage', 'force_lateral', 'vehicle_jitter', 'radar_speed', 'ram_usage', 'room_jitter', 'force_longitudinal'] not in index"
>>> f
'incindent_1595392700456.csv'
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'timebucket'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 28, in <module>
    feat.index = feat['timebucket'].apply(lambda x: datetime.datetime.fromtimestamp(int(x)))
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'timebucket'
>>> feat
Empty DataFrame
Columns: [Unnamed: 0, sabeiro, malindro, 11.08.2020 15:08, file:///home/sabeiro/.config/libreoffice/4;]
Index: []
>>> len(feat)
0
>>> 
>>> featL
                       timebucket       ts  ...  joystick_latency  e2e_latency_ms
timebucket                                  ...                                  
2020-08-03 14:53:06  1.596459e+09 -20067.0  ...               NaN             NaN
2020-08-03 14:53:06  1.596459e+09 -19967.0  ...               0.0             NaN
2020-08-03 14:53:06  1.596459e+09 -19867.0  ...               0.0             NaN
2020-08-03 14:53:06  1.596459e+09 -19767.0  ...               0.0             NaN
2020-08-03 14:53:06  1.596459e+09 -19667.0  ...               0.0             NaN
...                           ...      ...  ...               ...             ...
2020-07-22 06:38:40  1.595393e+09  19544.0  ...              30.0           200.0
2020-07-22 06:38:40  1.595393e+09  19644.0  ...              30.0           200.0
2020-07-22 06:38:40  1.595393e+09  19744.0  ...              30.0           200.0
2020-07-22 06:38:40  1.595393e+09  19844.0  ...              30.0           196.0
2020-07-22 06:38:40  1.595393e+09  19944.0  ...              30.0           180.0

[97087 rows x 38 columns]
>>> X
                     object_distance  brake_pressure  ...  vehicle_ram  vehicle_cpu
timebucket                                            ...                          
2020-07-22 06:38:00        14.800000             0.0  ...     6.948030     67.23550
2020-07-22 06:38:00        14.850000             0.0  ...     6.948030     67.23550
2020-07-22 06:38:00        15.000000             0.0  ...     6.948030     67.23550
2020-07-22 06:38:00        15.133333             0.0  ...     6.948030     67.23550
2020-07-22 06:38:00        15.250000             0.0  ...     6.948030     67.23550
...                              ...             ...  ...          ...          ...
2020-07-22 06:38:40        12.300000             0.0  ...     6.951125     66.61074
2020-07-22 06:38:40        12.300000             0.0  ...     6.951038     66.61074
2020-07-22 06:38:40        12.300000             0.0  ...     6.950951     66.61074
2020-07-22 06:38:40        12.300000             0.0  ...     6.950951     66.61074
2020-07-22 06:38:40        12.300000             0.0  ...     6.950951     66.61074

[401 rows x 33 columns]
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 56, in <module>
    for t in featL.columns: featL.loc[:,t] = t_r.normPercentile(featL[t],perc=[1,99])    
  File "/home/sabeiro/lav//src/lernia/lernia/train_reshape.py", line 78, in normPercentile
    tmin, norm = np.percentile(y1,[1,99])
  File "<__array_function__ internals>", line 5, in percentile
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 3732, in percentile
    return _quantile_unchecked(
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 3851, in _quantile_unchecked
    r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 3429, in _ureduce
    r = func(a, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 3967, in _quantile_ureduce_func
    x1 = take(ap, indices_below, axis=axis) * weights_below
  File "<__array_function__ internals>", line 5, in take
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py", line 191, in take
    return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py", line 58, in _wrapfunc
    return bound(*args, **kwds)
IndexError: cannot do a non-empty take from an empty axes.
>>> feat
                     object_distance  brake_pressure  ...  vehicle_ram  vehicle_cpu
timebucket                                            ...                          
2020-07-22 06:38:00         0.236855             0.0  ...     0.305997     0.656401
2020-07-22 06:38:00         0.237841             0.0  ...     0.305997     0.656401
2020-07-22 06:38:00         0.240802             0.0  ...     0.305997     0.656401
2020-07-22 06:38:00         0.243434             0.0  ...     0.305997     0.656401
2020-07-22 06:38:00         0.245737             0.0  ...     0.305997     0.656401
...                              ...             ...  ...          ...          ...
2020-07-22 06:38:40         0.187510             0.0  ...     0.914226     0.545912
2020-07-22 06:38:40         0.187510             0.0  ...     0.897119     0.545912
2020-07-22 06:38:40         0.187510             0.0  ...     0.880012     0.545912
2020-07-22 06:38:40         0.187510             0.0  ...     0.880012     0.545912
2020-07-22 06:38:40         0.187510             0.0  ...     0.880012     0.545912

[401 rows x 33 columns]
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 56, in <module>
    for t in featL.columns: featL.loc[:,t] = t_r.normPercentile(featL[t],perc=[1,99])    
  File "/home/sabeiro/lav//src/lernia/lernia/train_reshape.py", line 78, in normPercentile
    tmin, norm = np.percentile(y1,[1,99])
  File "<__array_function__ internals>", line 5, in percentile
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 3732, in percentile
    return _quantile_unchecked(
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 3851, in _quantile_unchecked
    r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 3429, in _ureduce
    r = func(a, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 3967, in _quantile_ureduce_func
    x1 = take(ap, indices_below, axis=axis) * weights_below
  File "<__array_function__ internals>", line 5, in take
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py", line 191, in take
    return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py", line 58, in _wrapfunc
    return bound(*args, **kwds)
IndexError: cannot do a non-empty take from an empty axes.
>>> feat.summary()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py", line 5274, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'DataFrame' object has no attribute 'summary'
>>> feat[mL].describe()
       object_distance  brake_pressure  brake_partition  ...    room_cpu  vehicle_ram  vehicle_cpu
count       397.000000      397.000000       393.000000  ...  393.000000   393.000000   393.000000
mean          0.476562        0.140332         0.500157  ...    0.399269     0.481336     0.493084
std           0.335899        0.258965         0.287638  ...    0.183512     0.221742     0.227232
min           0.000000        0.000000         0.000000  ...    0.000000     0.000000     0.000000
25%           0.131026        0.000000         0.255400  ...    0.277095     0.323588     0.318544
50%           0.570741        0.000000         0.494282  ...    0.407037     0.463913     0.512048
75%           0.802695        0.177419         0.745870  ...    0.490172     0.649274     0.656401
max           1.000000        1.000000         1.000000  ...    1.000000     1.000000     1.000000

[8 rows x 33 columns]
>>> 
>>> mL
['modem0_rtt', 'modem1_rtt', 'modem2_rtt', 'modem3_rtt', 'modem0_rx', 'modem1_rx', 'modem2_rx', 'modem3_rx', 'modem0_tx', 'modem1_tx', 'modem2_tx', 'modem3_tx']
>>> feat[mL].describe()
       modem0_rtt  modem1_rtt  modem2_rtt  modem3_rtt  ...  modem0_tx  modem1_tx  modem2_tx  modem3_tx
count         0.0  397.000000  393.000000  397.000000  ...        0.0        0.0        0.0        0.0
mean          NaN    0.439426    0.412261    0.499195  ...        NaN        NaN        NaN        NaN
std           NaN    0.355577    0.223326    0.227842  ...        NaN        NaN        NaN        NaN
min           NaN    0.000000    0.000000    0.000000  ...        NaN        NaN        NaN        NaN
25%           NaN    0.117267    0.234568    0.404134  ...        NaN        NaN        NaN        NaN
50%           NaN    0.326917    0.382716    0.539694  ...        NaN        NaN        NaN        NaN
75%           NaN    0.816551    0.592593    0.589575  ...        NaN        NaN        NaN        NaN
max           NaN    1.000000    1.000000    1.000000  ...        NaN        NaN        NaN        NaN

[8 rows x 12 columns]
>>> feat
                     object_distance  brake_pressure  ...  vehicle_ram  vehicle_cpu
timebucket                                            ...                          
2020-07-22 06:38:00         0.230991             0.0  ...     0.305997     0.656401
2020-07-22 06:38:00         0.231954             0.0  ...     0.305997     0.656401
2020-07-22 06:38:00         0.234841             0.0  ...     0.305997     0.656401
2020-07-22 06:38:00         0.237408             0.0  ...     0.305997     0.656401
2020-07-22 06:38:00         0.239654             0.0  ...     0.305997     0.656401
...                              ...             ...  ...          ...          ...
2020-07-22 06:38:40         0.182868             0.0  ...     0.914226     0.545912
2020-07-22 06:38:40         0.182868             0.0  ...     0.897119     0.545912
2020-07-22 06:38:40         0.182868             0.0  ...     0.880012     0.545912
2020-07-22 06:38:40         0.182868             0.0  ...     0.880012     0.545912
2020-07-22 06:38:40         0.182868             0.0  ...     0.880012     0.545912

[401 rows x 33 columns]
>>> X
                     object_distance  brake_pressure  ...  vehicle_ram  vehicle_cpu
timebucket                                            ...                          
2020-07-22 06:38:00        14.800000             0.0  ...     6.948030     67.23550
2020-07-22 06:38:00        14.850000             0.0  ...     6.948030     67.23550
2020-07-22 06:38:00        15.000000             0.0  ...     6.948030     67.23550
2020-07-22 06:38:00        15.133333             0.0  ...     6.948030     67.23550
2020-07-22 06:38:00        15.250000             0.0  ...     6.948030     67.23550
...                              ...             ...  ...          ...          ...
2020-07-22 06:38:40        12.300000             0.0  ...     6.951125     66.61074
2020-07-22 06:38:40        12.300000             0.0  ...     6.951038     66.61074
2020-07-22 06:38:40        12.300000             0.0  ...     6.950951     66.61074
2020-07-22 06:38:40        12.300000             0.0  ...     6.950951     66.61074
2020-07-22 06:38:40        12.300000             0.0  ...     6.950951     66.61074

[401 rows x 33 columns]
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'camera_latency'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 39, in <module>
    feat.loc[:,'b_latency'], _ = t_r.binOutlier(feat['camera_latency'],nBin=4)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'camera_latency'
>>> featL
                       timebucket       ts  ...  joystick_latency  e2e_latency_ms
timebucket                                  ...                                  
2020-08-03 14:53:06  1.596459e+09 -20067.0  ...               NaN             NaN
2020-08-03 14:53:06  1.596459e+09 -19967.0  ...               0.0             NaN
2020-08-03 14:53:06  1.596459e+09 -19867.0  ...               0.0             NaN
2020-08-03 14:53:06  1.596459e+09 -19767.0  ...               0.0             NaN
2020-08-03 14:53:06  1.596459e+09 -19667.0  ...               0.0             NaN
...                           ...      ...  ...               ...             ...
2020-07-22 06:38:40  1.595393e+09  19544.0  ...              30.0           200.0
2020-07-22 06:38:40  1.595393e+09  19644.0  ...              30.0           200.0
2020-07-22 06:38:40  1.595393e+09  19744.0  ...              30.0           200.0
2020-07-22 06:38:40  1.595393e+09  19844.0  ...              30.0           196.0
2020-07-22 06:38:40  1.595393e+09  19944.0  ...              30.0           180.0

[97087 rows x 38 columns]
>>> featL.columns
Index(['timebucket', 'ts', 'object_distance', 'brake_pressure',
       'brake_partition', 'steering_partition', 'force_lat', 'force_lon',
       'yaw_rate', 'steering_wheel_deg', 'steering_angle_deg', 'td_brake',
       'td_throttle', 'wheel_speed', 'vehicle_ping', 'rtp_lost', 'rtp_late',
       'steering_interval', 'modem0_rtt', 'modem1_rtt', 'modem2_rtt',
       'modem3_rtt', 'modem0_rx', 'modem1_rx', 'modem2_rx', 'modem3_rx',
       'modem0_tx', 'modem1_tx', 'modem2_tx', 'modem3_tx', 'camera_jitter',
       'room_ram', 'room_cpu', 'vehicle_ram', 'vehicle_cpu', 'camera_latency',
       'joystick_latency', 'e2e_latency_ms'],
      dtype='object')
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'camera_latency'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 39, in <module>
    feat.loc[:,'b_latency'], _ = t_r.binOutlier(feat['camera_latency'],nBin=4)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'camera_latency'
>>> featL
>>>                        timebucket       ts  ...  e2e_latency_ms  b_latency
timebucket                                  ...                           
2020-08-03 14:53:06  1.596459e+09 -20067.0  ...             NaN          0
2020-08-03 14:53:06  1.596459e+09 -19967.0  ...             NaN          1
2020-08-03 14:53:06  1.596459e+09 -19867.0  ...             NaN          1
2020-08-03 14:53:06  1.596459e+09 -19767.0  ...             NaN          1
2020-08-03 14:53:06  1.596459e+09 -19667.0  ...             NaN          0
...                           ...      ...  ...             ...        ...
2020-07-22 06:38:40  1.595393e+09  19544.0  ...           200.0          1
2020-07-22 06:38:40  1.595393e+09  19644.0  ...           200.0          1
2020-07-22 06:38:40  1.595393e+09  19744.0  ...           200.0          2
2020-07-22 06:38:40  1.595393e+09  19844.0  ...           196.0          1
2020-07-22 06:38:40  1.595393e+09  19944.0  ...           180.0          1

[97087 rows x 39 columns]
>>> featL[mL].describe()
         modem0_rtt    modem1_rtt   modem2_rtt  ...     modem1_tx     modem2_tx     modem3_tx
count   9643.000000   9643.000000  9643.000000  ...  9.145000e+03  9.643000e+03  9.614000e+03
mean     389.098932    546.375091   147.528155  ...           inf           inf           inf
std     1346.052594    955.772158   354.706990  ...           NaN           NaN           NaN
min        0.000000      0.000000     0.000000  ... -3.237131e+06 -3.237130e+06 -3.237129e+06
25%        0.000000     54.000000    51.000000  ...  0.000000e+00  0.000000e+00  0.000000e+00
50%       65.000000    150.000000    57.000000  ...  1.568000e+03  0.000000e+00  0.000000e+00
75%      152.500000    750.500000    65.000000  ...  8.974701e+05  8.857271e+05  8.898973e+05
max    31581.000000  14164.000000  4856.000000  ...           inf           inf           inf

[8 rows x 12 columns]
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 46, in <module>
    t_v.plotTimeSeries(feat[tL])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: "['camera_latency', 'joystick_latency', 'e2e_latency_ms'] not in index"
>>> plt.plot(y)
[<matplotlib.lines.Line2D object at 0x7efe02adda60>]
>>> plt.show()
>>> plt.plot(y)
[<matplotlib.lines.Line2D object at 0x7efe0c730880>]
>>> plt.show()
>>> plt.plot(y)
>>> [<matplotlib.lines.Line2D object at 0x7efe023da130>]
>>> plt.show()
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'td_trottle'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 39, in <module>
    featL.loc[:,i] = np.log(1e-6 + np.abs(featL[i]))
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'td_trottle'
>>> 
>>> 


>>> 
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 86, in <module>
    sns.pairplot(feat[sL+['b_latency']],hue='class',diag_kind="kde",markers="+",plot_kws=dict(s=50,edgecolor="b", linewidth=1),diag_kws=dict(shade=True))
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: "['b_latency'] not in index"
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'class'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 86, in <module>
    sns.pairplot(featL[sL+['b_latency']],hue='class',diag_kind="kde",markers="+",plot_kws=dict(s=50,edgecolor="b", linewidth=1),diag_kws=dict(shade=True))
  File "/usr/local/lib/python3.8/dist-packages/seaborn/axisgrid.py", line 2084, in pairplot
    grid = PairGrid(data, vars=vars, x_vars=x_vars, y_vars=y_vars, hue=hue,
  File "/usr/local/lib/python3.8/dist-packages/seaborn/axisgrid.py", line 1325, in __init__
    hue_names = utils.categorical_order(data[hue], hue_order)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'class'
>>> /usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 90, in <module>
    g = sns.PairGrid(feat[tL1+['b_latency']],hue="class")
NameError: name 'tL1' is not defined
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 90, in <module>
    g = sns.PairGrid(feat[tL1+['b_latency']],hue="class")
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: "['b_latency'] not in index"
>>> /usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/statsmodels/nonparametric/kernels.py:125: RuntimeWarning: divide by zero encountered in true_divide
  return (1. / np.sqrt(2 * np.pi)) * np.exp(-(Xi - x)**2 / (h**2 * 2.))
/usr/local/lib/python3.8/dist-packages/statsmodels/nonparametric/kernels.py:125: RuntimeWarning: invalid value encountered in true_divide
  return (1. / np.sqrt(2 * np.pi)) * np.exp(-(Xi - x)**2 / (h**2 * 2.))
/usr/local/lib/python3.8/dist-packages/statsmodels/nonparametric/_kernel_base.py:514: RuntimeWarning: invalid value encountered in true_divide
  dens = Kval.prod(axis=1) / np.prod(bw[iscontinuous])
/usr/local/lib/python3.8/dist-packages/matplotlib/contour.py:1494: UserWarning: Warning: converting a masked element to nan.
  self.zmax = float(z.max())
/usr/local/lib/python3.8/dist-packages/matplotlib/contour.py:1495: UserWarning: Warning: converting a masked element to nan.
  self.zmin = float(z.min())
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:434: UserWarning: No contour levels were found within the data range.
  cset = contour_func(xx, yy, z, n_levels, **kwargs)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:434: UserWarning: The following kwargs were not used by contour: 'lw'
  cset = contour_func(xx, yy, z, n_levels, **kwargs)
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'e2e_latency'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 95, in <module>
    plt.plot(feat['camera_latency'],feat['e2e_latency'],'o')
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'e2e_latency'
>>> 
>>> 

>>> 
>>> feat
                       timebucket       ts  ...  joystick_latency  e2e_latency_ms
timebucket                                  ...                                  
2020-07-22 06:38:00  1.595393e+09 -20056.0  ...                29             181
2020-07-22 06:38:00  1.595393e+09 -19956.0  ...                30             188
2020-07-22 06:38:00  1.595393e+09 -19856.0  ...                30             190
2020-07-22 06:38:00  1.595393e+09 -19756.0  ...                30             185
2020-07-22 06:38:00  1.595393e+09 -19656.0  ...                29             184
...                           ...      ...  ...               ...             ...
2020-07-22 06:38:40  1.595393e+09  19544.0  ...                30             200
2020-07-22 06:38:40  1.595393e+09  19644.0  ...                30             200
2020-07-22 06:38:40  1.595393e+09  19744.0  ...                30             200
2020-07-22 06:38:40  1.595393e+09  19844.0  ...                30             196
2020-07-22 06:38:40  1.595393e+09  19944.0  ...                30             180

[401 rows x 38 columns]
>>> feat.describe()
         timebucket            ts  object_distance  ...  camera_latency  joystick_latency  e2e_latency_ms
count  4.010000e+02    401.000000       401.000000  ...      401.000000        401.000000      401.000000
mean   1.595393e+09    -56.000000        27.645677  ...      165.987531         31.187032      196.586035
std    1.159030e+01  11590.297667        17.775327  ...       42.239879          8.957256       41.303368
min    1.595393e+09 -20056.000000         2.800000  ...      123.000000         28.000000      163.000000
25%    1.595393e+09 -10056.000000         8.800000  ...      153.000000         30.000000      184.000000
50%    1.595393e+09    -56.000000        32.950000  ...      160.000000         30.000000      190.000000
75%    1.595393e+09   9944.000000        44.700000  ...      168.000000         30.000000      198.000000
max    1.595393e+09  19944.000000        55.900000  ...      643.000000        130.000000      676.000000

[8 rows x 38 columns]
>>> featL.describe()
         timebucket            ts  object_distance  ...  joystick_latency  e2e_latency_ms     b_latency
count  9.708700e+04  97087.000000     60160.000000  ...      95304.000000    84373.000000  97087.000000
mean   1.595508e+09    -97.946687         0.666233  ...          0.910216        0.455807      1.974198
std    9.165540e+05  11545.708428         0.229664  ...          0.210834        0.141062      1.013649
min    1.593425e+09 -20096.000000         0.000000  ...          0.000000        0.000000     -1.000000
25%    1.594976e+09 -10077.000000         0.515564  ...          0.941464        0.377883      2.000000
50%    1.595435e+09   -120.000000         0.675634  ...          0.960995        0.453082      2.000000
75%    1.596444e+09   9867.500000         0.850847  ...          0.974397        0.528918      3.000000
max    1.597124e+09  19998.000000         1.000000  ...          1.000000        0.999786      4.000000

[8 rows x 39 columns]
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 30
    feat.loc[:,"modem_rtt"] = feat.apply(lambda x: np.mean(x['modem0_rtt', 'modem1_rtt', 'modem2_rtt','modem3_rtt'],axis=1)
                                                                                                                          ^
SyntaxError: unexpected EOF while parsing
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 30, in <module>
    feat = feat.ffill()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 6878, in apply
    return op.get_result()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py", line 186, in get_result
    return self.apply_standard()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py", line 295, in apply_standard
    result = libreduction.compute_reduction(
  File "pandas/_libs/reduction.pyx", line 620, in pandas._libs.reduction.compute_reduction
  File "pandas/_libs/reduction.pyx", line 128, in pandas._libs.reduction.Reducer.get_result
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 30, in <lambda>
    feat = feat.ffill()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/series.py", line 910, in __getitem__
    return self._get_with(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/series.py", line 923, in _get_with
    return self._get_values_tuple(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/series.py", line 974, in _get_values_tuple
    raise ValueError("Can only tuple-index with a MultiIndex")
ValueError: Can only tuple-index with a MultiIndex
>>> mL
['modem0_rtt', 'modem1_rtt', 'modem2_rtt', 'modem3_rtt', 'modem0_rx', 'modem1_rx', 'modem2_rx', 'modem3_rx', 'modem0_tx', 'modem1_tx', 'modem2_tx', 'modem3_tx', 'modem_rtt']
>>> mL
[]
>>> featL['modem_tx'].describe()
timebucket
2020-08-03 14:53:06   NaN
2020-08-03 14:53:06   NaN
2020-08-03 14:53:06   NaN
2020-08-03 14:53:06   NaN
2020-08-03 14:53:06   NaN
                       ..
2020-07-22 06:38:40   NaN
2020-07-22 06:38:40   NaN
2020-07-22 06:38:40   NaN
2020-07-22 06:38:40   NaN
2020-07-22 06:38:40   NaN
Name: modem_tx, Length: 97087, dtype: float64
>>> feat['modem0_tx'].describe()
count    59196.0
mean         0.0
std          0.0
min          0.0
25%          0.0
50%          0.0
75%          0.0
max          0.0
Name: modem_tx, dtype: float64
>>> feat['modem1_tx'].describe()
count    0.0
mean     NaN
std      NaN
min      NaN
25%      NaN
50%      NaN
75%      NaN
max      NaN
Name: modem0_tx, dtype: float64
>>> feat['modem2_tx'].describe()
count    40.0
mean      0.0
std       0.0
min       0.0
25%       0.0
50%       0.0
75%       0.0
max       0.0
Name: modem1_tx, dtype: float64
>>> feat['modem3_tx'].describe()
count    40.0
mean      0.0
std       0.0
min       0.0
25%       0.0
50%       0.0
75%       0.0
max       0.0
Name: modem2_tx, dtype: float64
>>> feat['modem3_tx'].describe()
count    40.0
mean      0.0
std       0.0
min       0.0
25%       0.0
50%       0.0
75%       0.0
max       0.0
Name: modem3_tx, dtype: float64
>>> /home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:31: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_rtt"] = feat.apply(lambda x: np.mean(x['modem0_rtt'] + x['modem1_rtt'] + x['modem2_rtt'] + x['modem3_rtt']),axis=1)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:32: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_tx"] = feat.apply(lambda x: np.mean(x['modem0_tx'] + x['modem1_tx'] + x['modem2_tx'] + x['modem3_tx']),axis=1)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:33: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_rx"] = feat.apply(lambda x: np.mean(x['modem0_rx'] + x['modem1_rx'] + x['modem2_rx'] + x['modem3_rx']),axis=1)
>>> featL['modem_rx'].describe()
timebucket
2020-08-03 14:53:06   NaN
2020-08-03 14:53:06   NaN
2020-08-03 14:53:06   NaN
2020-08-03 14:53:06   NaN
2020-08-03 14:53:06   NaN
                       ..
2020-07-22 06:38:40   NaN
2020-07-22 06:38:40   NaN
2020-07-22 06:38:40   NaN
2020-07-22 06:38:40   NaN
2020-07-22 06:38:40   NaN
Name: modem_rx, Length: 97087, dtype: float64
>>> featL['modem_rx'].describe()
count    6.203200e+04
mean              inf
std               NaN
min     -1.022564e+06
25%      2.483178e+03
50%      2.535782e+05
75%      2.816609e+05
max               inf
Name: modem_rx, dtype: float64
>>> feat['room_ram']
timebucket
2020-07-22 06:38:00          NaN
2020-07-22 06:38:00          NaN
2020-07-22 06:38:00          NaN
2020-07-22 06:38:00          NaN
2020-07-22 06:38:00          NaN
                         ...    
2020-07-22 06:38:40    17.948812
2020-07-22 06:38:40    17.948812
2020-07-22 06:38:40    17.948812
2020-07-22 06:38:40    17.948812
2020-07-22 06:38:40    17.963219
Name: room_ram, Length: 401, dtype: float64
>>> feat['room_ram']
0      17.720228
1      17.720228
2      17.720228
3      17.720228
4      17.720228
         ...    
396    17.948812
397    17.948812
398    17.948812
399    17.948812
400    17.963219
Name: room_ram, Length: 401, dtype: float64
>>> /home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:34: RuntimeWarning: Mean of empty slice
  feat.drop(columns=mL,inplace=True)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:35: RuntimeWarning: Mean of empty slice
  feat = feat.ffill()
>>> -------------------------execute-incident-list---------------------------
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 65, in <module>
    query = open(projDir + "queries/incident/" + 'incident_factors.sql', 'r').read()
FileNotFoundError: [Errno 2] No such file or directory: '/home/sabeiro/lav//rem/raw/incident/queries/incident/incident_factors.sql'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 65, in <module>
    query = open(projDir + "queries/incident/" + 'incident_factors_sec.sql', 'r').read()
FileNotFoundError: [Errno 2] No such file or directory: '/home/sabeiro/lav//rem/raw/incident/queries/incident/incident_factors_sec.sql'
>>> -------------------------execute-incident-list---------------------------
QUEUED
FAILED
SYNTAX_ERROR: line 9:2: 'timestamp_ms' must be an aggregate expression or appear in GROUP BY clause
QUEUED
FAILED
SYNTAX_ERROR: line 9:2: 'timestamp_ms' must be an aggregate expression or appear in GROUP BY clause
QUEUED
FAILED
SYNTAX_ERROR: line 9:2: 'timestamp_ms' must be an aggregate expression or appear in GROUP BY clause
QUEUED
FAILED
SYNTAX_ERROR: line 9:2: 'timestamp_ms' must be an aggregate expression or appear in GROUP BY clause
QUEUED
FAILED
SYNTAX_ERROR: line 9:2: 'timestamp_ms' must be an aggregate expression or appear in GROUP BY clause
QUEUED
FAILED
SYNTAX_ERROR: line 9:2: 'timestamp_ms' must be an aggregate expression or appear in GROUP BY clause
QUEUED
FAILED
SYNTAX_ERROR: line 9:2: 'timestamp_ms' must be an aggregate expression or appear in GROUP BY clause
QUEUED
FAILED
SYNTAX_ERROR: line 9:2: 'timestamp_ms' must be an aggregate expression or appear in GROUP BY clause
QUEUED
FAILED  C-c C-cTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 68, in <module>
    download_query(query,fileN)
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 27, in download_query
    location = a_u.exec_athena(athena,query,params)
  File "/home/sabeiro/lav//src/sawmill/sawmill/aws_utils.py", line 38, in exec_athena
    time.sleep(2)
KeyboardInterrupt
>>> QUEUED
FAILED
SYNTAX_ERROR: line 65:9: Column 'brake_pressure_partition' cannot be resolved
>>> QUEUED
FAILED
SYNTAX_ERROR: line 72:9: Column 'td_brake' cannot be resolved
>>> QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c1486a1f-2fb6-4898-ae0a-6db435a9e0d8.csv
>>> QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6163f4d6-bd9c-4e51-82d0-f193b8017d63.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4860d0ba-2de9-41ce-bf77-4920158a170c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/996f3a7d-68da-4f7f-a018-9ab97dda291e.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/9366dd4f-6cff-49c6-abbd-0df83475e44c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e99a66a8-852f-4ff0-93b3-01b2f44f14d4.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f483840c-b543-4065-a4c0-7832601de17d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/94771fa3-301e-4063-9ed5-5f3802e7b0b4.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/237a1973-453c-444d-afab-6fde7418ecc2.csv
QUEUED
SUCCEEDED
s3://ree-cloud-athena/202744e6-cc70-4587-a05a-7e1acb3f53b0.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7aa857a5-d93e-4e23-97cb-d9ba783259b3.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/769f7855-b2ad-4dfd-a1a4-38a6cebbd8ee.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/06523d1e-1112-4297-a271-1fd079608afa.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/975d5e47-24bd-4a8c-bfe7-3f1b35ea2765.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7cd26583-bf6a-4a61-a690-648b6d09e376.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/bd8e69f4-f9e7-4ad6-b2f1-6ca9e8afce33.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/9ad86b16-b517-407e-923d-03566b57b79e.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/44999f3d-69a4-4300-902a-0f901e7d535d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1b2048c0-dc79-4689-9e6c-ae1e7eb6a41a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/99c6929b-5c85-43c2-9ff0-75d12c53866e.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8c6f5970-18ad-4e49-9cdc-50392563e15d.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d159d337-d903-474f-be7c-4a6dce2339a6.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2ac44766-de52-4958-bf7f-40687bd5e293.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/77b7b244-e098-4bf3-8fe8-1d2ee904c3be.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/dfb17e25-eb22-4f35-bcbc-6dda23450c7e.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/cf5aa658-e929-4a01-bb93-8eb42328a31a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b4a589de-3bb9-4ff3-84e1-f5dcee06b5cf.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d8af730e-af31-4805-862a-f956c0fe6863.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/cde4bb72-01a6-43ae-9e7d-ae097bd173b5.csv
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4189a405-9cb9-4bd1-b4e5-731dc50e5f5f.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/9efa51cb-4911-435d-af56-b178cd447c95.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/962338d6-83b1-403b-9779-428befeaa8ab.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8ee4fa16-ebbe-4680-aabf-71b77ed92eed.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/22ce6d3c-6ed6-4bb0-bcb2-35694e5b45ed.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0c121ee9-0dfd-47b8-82e0-ae0ebf45bace.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/38b858d1-ad10-407a-9a5d-c8103960a547.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/692e95c4-40f8-4e85-9b9b-8c7f0da1c689.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/12d04f88-b21c-4e9f-9016-735188d17c22.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8b001c18-d126-4c4a-a226-979784d7c06c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2a32a518-cfc2-4369-b41d-60b7064a4971.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/20b088ca-2770-4d61-85a4-af2a85155ff6.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4ca33236-1c3b-43f4-af51-b0e041c8f5fe.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ee4cceaa-421f-4271-989d-e204de3e8460.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1e3374b1-81c1-471f-a1b4-3e8bace1848d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0d257d08-f440-432c-8a88-0943861fba54.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7097e9e5-1cb0-493e-b1bb-ed6238f0345d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4a30d976-44b6-485b-8dee-d9c58337784a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/77799984-ce69-4f2c-a964-7b9dd2f03763.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0683098c-1a9f-4c6c-95e8-66c0785bef26.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/07990089-fc4b-4157-ab0a-457285c80670.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/90472469-4c78-4743-a3ad-61786dff2fda.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/497d75fe-4951-427e-973e-121322ce1975.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a8b3e086-c083-465d-9031-a2221873c759.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/9a6eea98-78f4-40d3-b27e-55064c659fe2.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a40790ba-437b-4b40-93d6-3412fc662c4f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ab778e25-5cdb-443a-9f8f-6b03482c2831.csv
QUEUED
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7e2346b5-c248-418a-aa35-df8ca0c134fd.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/04483965-3a1f-4f82-bca6-48b035d401b3.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/267e827f-008f-443f-85a0-c295c301dc23.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f677faf1-ee37-4287-a0b9-fa8216ce51c0.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ed122059-9a28-4eb2-a161-2bac3da0a99d.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d8c7a592-27b2-4755-a801-673b99c77131.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/88ba5dab-0ae7-4b9b-b4b7-09607e428222.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a997586b-5a9d-461d-afee-da9eeb549bf6.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/89dbd5e8-2f35-4d0e-83c1-409197da6889.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6bb1d21f-7b5b-4b70-8e8b-52f8339f4bf0.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3dea1007-34a3-4d68-810e-390da4d4cbef.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/81a47ab5-feba-4756-a97e-3490078d2efe.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4bf3a952-a6b1-4aab-9651-95fa9497d2c7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/148c4f57-640d-42c8-81aa-f91f38a917cc.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/62e8ba24-c05d-46ed-ac93-c254cd2bf6da.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1391e2b1-2fc0-4462-960b-11f4a88f7f4c.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f309e61d-14dc-4166-aa19-1670b523729a.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/eb633bfb-75f0-4885-8e6f-73485f56bfbb.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ef4ca51f-2dae-488c-a348-bb173338f86b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/eeca7c56-00e3-477d-b787-de6d1ce5a4c9.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d1cebce6-858f-4c54-851b-c5e5cbfa039b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/de2c2cd7-726a-4739-ace7-f2bf77042b71.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/43248b51-3cb3-49d7-97f9-7161a5a09939.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/acb5734b-3f79-450d-8fe4-b36103487df9.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/193723f3-55a1-4a7f-bc1d-0df105e2ab9b.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7934ba01-9359-4739-9fac-deaceef6f8db.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a2d18fcd-1a03-4633-9ba3-e82b76f3b53b.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0b6d55be-0517-4d24-bfb4-d3336375cd6e.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ef7d543d-f1ff-407e-8324-8686903376d0.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/326ac0c4-ad24-4468-8044-948174c8b602.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8f8e22c4-8cd0-4ada-85c0-f9168166f47f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/605af0b3-2051-4757-846e-2c2ff380e57f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8f2b1876-b2b8-4b0d-9931-d05ec7db1cb8.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1965e872-ec09-4f89-836f-00a541bc50fd.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a5639c2a-5410-4de1-b552-fe4ea731f5c6.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8840400b-7ac6-4b0f-abf6-3ed9e9505fc9.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/9ba5e1d4-3438-4ec9-a706-cce9bd0b0461.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/bae95d5b-9eaf-4a2d-8d97-8c39e891fb22.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1e0fd093-d379-45c1-9713-6ba1cf1f65e1.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/78601b6e-ef86-43b5-8de6-47a511cfe785.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/fa3091cf-42ce-4173-8301-b5c21edee7cc.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0977f98b-dbf3-42c3-be12-9390fcb12dba.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f2039de3-3fe5-4450-a896-4a7a86f250bf.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c93d29e5-7f75-4117-b541-62b2b766bdf8.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/89115ede-2a06-48a7-b281-1865b5fbab2c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5d6ea1e1-b209-40de-8b05-e6e355a4b300.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0c70e3b8-a91e-4c6b-aaf0-c432b306821d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/be6e5d18-071c-4419-94b9-343c9f60b21c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b9820653-888a-477a-a439-9698561a9436.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0e01a110-3d2e-40db-8bb7-2244b8cec3b8.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/51002ec0-e7fa-4c27-b9f2-8baf522e079a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8d63c4db-7f82-40c1-983e-e03ff9e06873.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a223ad53-971d-43e9-90d8-25a4fe81f1cf.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/fe095161-9b31-4e67-8450-26d5ea3576bc.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/9303e2e2-f069-4460-888d-8370d561f0e6.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/65270888-7a91-4d71-b2f7-05177235223f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/701bbf78-39d6-4ae6-9abd-630573e045e2.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/95fae76d-551c-416e-b3c7-51e72f2e3304.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/90e395f1-5bf6-4f23-8e5a-34454fd359f4.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f1208c14-5606-4358-86b2-b2f44e97bcaf.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e7dfafe9-032e-44a6-8c75-ec840b2d4fbd.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1cc4205a-68f3-4c42-af3a-d32bf765c89a.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/bac247ca-ee66-47d0-bfc8-6577338f9c73.csv
RUNNING
SUCCEEDED
s3://ree-cloud-athena/da86aef3-5a5d-4789-83b9-cf7302191356.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5b07ac82-d4c2-4fe7-a3d0-ceeb26041f04.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8d41b42a-7ae3-497e-b2e5-0b574da7708e.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/45c43718-9287-441c-b629-0ac807d40c1b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1a7ff021-0a1d-48d1-a6cb-a25e027b6537.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a3f1364b-331a-4762-9580-c831d88c0b95.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/cbaae919-a800-4e55-bfa3-6e65885a1818.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b94ae56a-0248-4c91-b705-97838b331539.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/66efc12d-d475-4b4d-9cb7-ef986ff30203.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a795abde-b53b-4486-9465-7449fc92281a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ecabd4f8-f0e8-4967-8815-4ff272e47cc8.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d3f14f00-a71b-450c-b377-4dfe58a50f83.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a3a77a3c-e8ca-4ed0-866e-140bf11608ba.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0817e1a7-7926-464a-923c-ba53468173ed.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3c10092b-7961-4366-a5fc-24ea09c6e49f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ae04ea6d-e3c5-42b8-a8aa-670fea82e7d4.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2c5cec9e-71cd-464c-8018-b697f93c2662.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/32ec1d13-3ef4-43b3-8ac5-412c87d05bbc.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c8b664da-cc69-4e51-a5dd-23698726c995.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a7cfbb80-ede8-43c6-975a-c8db82c7ab83.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ca0e54af-77d7-4af7-8202-419605f3072f.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f57ed198-d91f-4bce-91f3-8d4bef9dfb96.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/780ccbab-d7d3-4313-aea8-27e2468e4b75.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c80747f1-7bd0-4b77-8ba0-a4e52710b9c2.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5fa55b91-37ff-4229-872b-96ca573e85c5.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/76694bd5-b1d4-4c3b-8143-063aa7683e12.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/dc7a2b1e-6102-4141-b1a2-4782c10eeeaa.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d5a0bbb7-4bd6-4f01-91b4-7e5e4108c98a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/686046f2-64bd-441f-8d8a-7967ca2aef43.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7e400fd3-3d73-4154-884b-98ca295ee05c.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5120c188-c8ab-4b42-9273-7d214245a60d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/298a6d70-54d4-44ce-aa19-ba82da8784cf.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f6badec2-4ef2-4170-a7a9-d12141824444.csv
QUEUED
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/503e4bdf-928f-4f03-b393-586203850713.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/dc23b552-71af-4da0-9e2c-c67b2a0e4e89.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/099a0a39-fce7-49df-b99f-f5beea5bea33.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d8e0106f-240f-4c65-a7c7-5255a52ef027.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/71357e95-3652-4ad4-aadf-7fcc512ae281.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/63887a1d-c8d3-4105-837d-35e9580a8e4f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/52001f60-cac6-4fbe-ab80-914dbc8b8a68.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6f6a7d19-23b7-4d89-8e82-2bb0aeed886a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e4b847ae-ca91-4bab-a84e-123a4389e2bd.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/74b8a959-87e9-4532-835b-2e855ad268ee.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/196b6a40-9a1c-41f9-a096-740b2b759871.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/afe3f847-b12a-437e-b32a-908d22bf79d2.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d7284c26-4686-4482-b396-f61b794c110a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/67d8fb96-94f1-4029-a4a4-d63d81d90485.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/978eccfc-87a3-4aba-90a5-5c0908b14a3e.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/689966a1-0161-44f2-b487-1851c7fd162a.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2538a65b-9f85-4021-8387-cc39fa2621c7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1675fcc9-cad9-4190-8f2e-4a6467add908.csv
QUEUED
QUEUED
QUEUED
QUEUED
QUEUED
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0001af3b-4394-4b85-8dc0-07b8b883dcf4.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4992ffbd-ff47-4c48-800c-ea032b0f3046.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/347a881f-1981-4512-87dc-6adfbc417b66.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/50b0dbb1-40c8-4472-97d8-9cfd7f095c97.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/59a5c050-2282-4f62-a0aa-5a83b57d5738.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/77d91ac5-00ca-451f-ba8c-076e2a820d59.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3951323c-0fa0-403f-a07b-8b63863445f2.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c8a49884-327e-42f9-815f-8a809640e090.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5db8669a-1388-49f6-bde1-f40b3c58001b.csv
QUEUED
s3://ree-cloud-athena/5db8669a-1388-49f6-bde1-f40b3c58001b.csv  C-c C-cTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 68, in <module>
    download_query(query,fileN)
  File "/home/sabeiro/lav/rem/src/feature_exp/src/db_feature.py", line 27, in download_query
    location = a_u.exec_athena(athena,query,params)
  File "/home/sabeiro/lav//src/sawmill/sawmill/aws_utils.py", line 38, in exec_athena
    time.sleep(2)
KeyboardInterrupt
>>> -------------------------execute-incident-list---------------------------
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1b792c66-476c-438e-a37b-e4037cc1f2f7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/29dbb9b6-9dea-48cc-bbeb-5341df6f7b6d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/457f393c-cabe-4252-ad94-9e548f3ba9ff.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1b6a65bf-c257-4adf-bb0a-35bcf17887e3.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/99daa596-6b5c-448c-bb2d-b72eda3ed3fb.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f09cb2c2-3f36-419f-bc62-ea83cf5ee36f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/df465a3b-e486-4a0f-811a-c23b15e575b7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/09436d45-db5b-4cc9-aefa-949f50cb84c5.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0b413f3b-5853-450a-94d4-ac75483a2429.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e40a4565-e658-4f8e-a585-392697363081.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/cf6790d5-d001-455b-80aa-ab32b0b9d878.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ba7dba47-66cb-4b2f-aa18-c8ccbeee495e.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/89c517c7-d913-4ee4-b4f2-dae3ebe96b2c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2e227b57-0f11-420d-a9ea-68674e798e92.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/33fd3f26-dad5-4ac4-ba1f-6831a01f8314.csv
QUEUED
QUEUED
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/29740302-2645-4dfe-b8ab-691307608df2.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/10f1f505-5da4-4744-b579-99dd9ba1a74d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6e470166-dec9-4ade-9f5a-1c9d70e2bb96.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/415d7aa1-f522-48d9-a07c-2635082dd99b.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4868eee4-7498-45dd-b517-9fb247532b2c.csv
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/9d456720-aa5e-46fa-b382-0cb5b7016469.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/dcfa89e2-fd27-484c-a854-f026dc496ddb.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a4bdee11-6251-42b9-856b-d778fbbdf0e5.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/87051df0-cc3c-40a4-85f6-0ee9fcd4532d.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/9f639458-2bc5-46c8-9a26-6c0c8a5c83db.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/031d684c-bee4-4b85-a420-f4113331445b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/378aab87-b7e4-4f3a-b147-1df6a3342ca3.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ba7372e2-38bc-4fac-8a6a-a7d8927d4551.csv
QUEUED
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f971fed5-f8d5-4851-a3e5-e518c7f22274.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f7406584-b4af-41b5-8557-d423322ed52e.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e70d7e80-408f-40af-a609-fba6b042cc79.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ff94273b-4cc8-4173-a13e-09f7038dc89d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2d5613ee-6cdd-42ca-a9a5-756872df0519.csv
QUEUED
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c04ee039-0cc0-4606-ad92-415006ed1017.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3b83d606-d071-4717-9a8c-9fd0d62029fd.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/731d5859-ca9a-4385-b9d5-046da8f94418.csv
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/402c1073-ec5a-4bec-ab4d-f99c221793f3.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/dc6c2484-e977-444f-83d3-a1475790b594.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5ab05f16-83a1-4c47-b246-fe8a613c7b28.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/04455f9b-2130-46a4-9101-122bce84d963.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f11fbe81-f40f-4eeb-95c0-be7dfb3d8908.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/50e9196a-6f9d-4be0-b83e-0e67869a6892.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ba530648-0070-445f-a3cd-bdd8373c14bc.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ef97d274-75ec-47d8-a19c-4f3de71f2707.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7a6b27c6-2878-49fc-82af-d7c6b0289218.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/cd4e9662-c7f5-4a7d-a5a3-6241edfdf4c5.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/45f9a581-7671-4650-891f-3250c4f1fe58.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a22838cb-eb6a-42c5-979c-6e800a11c830.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c90e1c29-2689-408f-a828-5f80804d8caa.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d02d1581-a16d-4571-aef2-33c2b558d710.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2bc46252-1ec3-4b3f-a361-0ef5ffe4e56d.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/13d82870-9c6a-4610-9fb1-3417882fd92b.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6a9238a3-10e2-43ac-a6e6-550def79083d.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/044ab671-67d0-410f-8126-a440120324cb.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5517b77c-1cf9-4b46-a213-6ed2837a12e6.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/064579f5-7135-4c88-90e1-689a2269520e.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/72d1c2e2-e4a5-47ac-aa6c-6bb68d908266.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d9c13db6-2cf2-4608-a350-759e15b0f3e0.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/788bd2d1-2bf5-4399-b984-ecaeca2721b5.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e2a68a3f-08b1-483c-9399-593a37380b57.csv
QUEUED
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2391a324-0813-481f-b005-726085dae8f3.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2cef9a9b-448b-4f52-80c8-0005d793fc0f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/03c6977b-6956-4c28-8e54-b7f61ce457a9.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ce51fb84-9251-40ac-92ba-61c9cbb5a048.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c201ac9a-ae42-40f1-bf81-2695c9f4f119.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d1bc3be4-d381-404a-994f-9b3fceba1dc7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0aeb2c02-58ab-4b8a-937a-114be3ec5432.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c1bf376c-40a1-4772-a8db-382ec32f267d.csv
QUEUED
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/13dfcd70-7fdc-46bd-94a2-21c05e28ca7c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e9337253-2f18-40e1-a87d-2a237ca776ca.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ff162834-272a-48c2-bc47-c8a35f65607e.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1855fbb9-8435-447b-9308-3cbee3bd59e3.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/004ff741-8044-40ac-ba22-2b00203b85e9.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/29fe53ec-f106-4ce0-8fad-2e8262d8b5f0.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1b078173-5ebd-4e5e-ba1b-5d111f1568bc.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c7846133-4e3f-414c-9540-d0c4f1f51525.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/261028cb-f64e-44fe-8271-7a3200cb3a30.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/68f8e441-bc86-4144-885f-46f390c1a9a1.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4f0bea3d-88d5-4054-939b-0084f74c3387.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/99a1d7ea-c9a3-461b-97e6-9354077c2929.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1688f4ca-c0a9-4d74-8309-6ec327a6ba68.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7dd300b5-3d93-4ebf-976e-1419e8449302.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/dc79f3d5-3e0a-45fa-81fb-01266af1b540.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/fc43d0b6-99a4-47a5-a47c-9d3c45fe688a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8f521b17-590b-4ceb-8814-517c467d0423.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3c6ebb0c-069c-4eff-86b1-4dc174aed3e7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2b733403-af66-46f3-a721-efa8a84a4bb1.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3aa76435-2314-40e8-a497-03d1d16ffc29.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8b62aa0b-3e7e-4459-872c-4f5b0e764eed.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/cbab69ba-50d5-42bc-86c8-ecc100ccf85b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5ca4aa24-fca2-4f2e-83f2-187416117a67.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f1d49160-525b-4f39-9b26-c91b736c2842.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1639f1ca-7225-4eaf-aae2-0ed7433675b1.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/52bca011-7f08-4184-8de0-543c5b6bcce9.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1160da8a-2c0b-4b65-af39-f50958d1a364.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/cc355e45-d313-42b1-ad47-a5a0e7255de7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/57ede986-8b63-477c-8e9c-5a9ccf7d657b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b4afe5d4-a631-4ba2-a37b-e0ce3554e6f0.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/29189682-3ce6-4bbf-b3b6-f49b38b02ea5.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d71dc17a-6cec-497e-a1c7-781eff189299.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7e3e7403-c8ed-40d0-832e-484ec2bbaaf1.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/40b16faf-3fd9-40b6-9337-b6979d5e91a3.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0f14cea3-2e45-4b93-96cb-1b2c92d17e1e.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7e7ab6d3-2dfb-40df-98f2-ada502840f27.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/fa2f50d9-d4de-4beb-9d63-7cf0afeb5ed2.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6360af72-6c54-4322-9a39-b272b29f1ac4.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/9e993805-7506-4f4d-b869-706f91636c4d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/eb7ac569-a76d-42d2-b2c2-b3f57357321f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e11965ff-f791-47f5-b456-302790d4e0e8.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c2cba5af-db67-4d50-ba37-ab1ad5e8a808.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/73602cc2-f238-4d11-8596-b9fee578bb50.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/37f343dc-89c9-42cf-881b-4c35790cbea1.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4fa8ce84-5ceb-4a25-92b2-b9e8579cf094.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ac29f1ed-a2d2-43e8-b2ed-cc2fa0c44ba9.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c02bcab3-bd59-4cf3-87bd-ca492bbb8dda.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3554ce1b-122d-418c-b07f-e0b5cdc43549.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d2d2d045-4d54-4301-a136-31995dcfb62c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/85037d42-eec9-4faa-b1e1-25365e11226d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6e7b7798-9b66-4045-ab37-28211cc66a1b.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/86cdf8f1-bb4e-4ddc-a1b1-622f58249bfd.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/38dc0d9c-82e3-4aff-b84c-55449e0284fd.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0956cba8-bf01-4da0-8ba9-5141bc3ce444.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/8bd89801-17e0-4edb-a793-54f4e2afd8e9.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e833bfe0-9928-46e9-8632-61d3a2f330ac.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/bb7f5ce4-229b-4f59-9d09-87704139dd24.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c2d16875-eb22-4e32-8fce-54e57dd16d2f.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b2c78327-3c96-4809-869a-2cefdadefead.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d8d46441-4e2a-42a1-8997-f855564689ed.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7bf34dd1-da84-4703-bd46-bb8851b8b1ca.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7bbd91c3-1140-4c2c-bfb7-170ad44071ab.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/46c257dc-0562-486b-b8f2-52ce2c5d3571.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2151be07-127f-4363-aa68-0924e5be4423.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e5673b45-3f67-4044-bd16-0b0fbb70410c.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/019503c0-4e0a-4921-b168-d0bc384ef0a2.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/69643ba9-720d-43f2-bc63-748ab5ac6c2d.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7e650eb3-56f3-4c0d-b986-91c3898ad583.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/6833198a-144a-4dae-a7d8-1f67bb9ae8a5.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/dc3bf5fa-29b2-447a-bfa6-b457da175082.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d34c7dca-e313-4cee-88f4-19202d7c6af9.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/fc59bd26-703a-4d1f-a191-20ce265760fb.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ffde6e0f-41e3-43f9-b935-593f8a699a71.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b50989f2-bead-4df8-82cb-529e0141ba5d.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a29713d1-2bdb-4533-984e-b1df04280c7f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a12748c3-fbaa-4390-a4f5-487471d68772.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/84da5e07-8bdb-4b24-a436-88317428b027.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e89897a8-84b3-47b1-adba-9d558db3fafb.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ede5e660-c5f1-4b17-b43a-56cb9fac8cde.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/63caec2c-3c4a-4372-8093-af925799d053.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/08492b24-7009-42c9-a397-794725414430.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/56caf7e5-53e6-4605-8228-816eb4ba5f5d.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/181df1c2-9f41-4a1f-8f31-4d17f78ee464.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4b81b444-8afe-4b54-be86-85d309d16c19.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/19efb233-bd04-405f-98a2-148e4bf58add.csv
QUEUED


RUNNING
SUCCEEDED
s3://ree-cloud-athena/58a674a2-45f0-4f38-afeb-c46e0d660c4a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/11e0f337-aa68-474f-b128-bb503f095956.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/36640157-7ea5-4b27-be3c-96fe3e1243a4.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5f10a257-a2e9-444a-8596-04cef4db32e0.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/88b1c33d-b5ae-42d5-ab70-5642fe8dce28.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/89c1bcb5-72cc-4350-a758-8d6ff9216810.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a27c3487-c545-49af-8d81-b9f26a4ea1c4.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/072d7ea2-fe40-476d-84aa-33c58f6e77bd.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3d629164-570c-4e83-b501-f490d87205b3.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0abc7c30-09d8-488e-8c87-14379aba1157.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4201d88f-eb1f-4265-b8ce-e00f394d7526.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/642a6bd8-6bd9-4622-8337-518d6f73f29f.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d4b708c8-852e-4f17-a36a-42172846361a.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0ab0f0bc-5aea-4d0d-997a-1b9f8f89b725.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1259e7d3-5b1a-4be7-831b-3a96629bd44a.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/15d48cfc-2bec-45da-9f09-d7dcba803c5d.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/332bb219-908f-486c-a934-864ea1377816.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/423609fe-d148-44b6-813e-5383d7399ceb.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b056e6d3-fbd9-427d-849e-cdd21e9443de.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ffbe2bc1-828c-46b7-bc4b-887cf4f3809b.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/36e9793a-a72a-42ca-8af7-248a0ae94e62.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/bda7de26-3754-4eb8-ad20-6814bc20a707.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/37659299-4641-4abe-8a14-2386cdbbf248.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b8a86098-5b8c-4e92-9305-809ec77e7587.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3b728c1d-6292-49c8-a3ac-c2ae0396d37f.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/dc8abb95-6ca8-4318-95f6-9f2bf3e3ebc0.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/92bb4343-4051-4995-bda0-9b30752a0958.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d2148469-f692-4eb7-91de-cbfea5f9de08.csv
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/9c6d451e-f15e-4a3a-8713-cb13dc273b88.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/74fa3b6d-162c-4b47-bf84-e35c3cd8d3ba.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/50af8670-e857-435e-a850-628eaefcc5d7.csv
QUEUED
QUEUED
QUEUED
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/863a21f1-c884-41f1-bcc1-0ce9d9394b13.csv
QUEUED
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2b84e857-dbc7-48f1-b6d6-434f9022da9b.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3b2b2faa-ddd6-479a-afbe-82a5ab481291.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c55d9bab-7409-4cb3-a32c-a2092b3b6e45.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/3d5e7ea7-b63b-4e22-a158-9e8082921f3c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/fe54b3c9-8013-406c-a465-243ef3a38ce8.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/72c759ca-79a4-4328-833d-537a79d202fc.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/a32f6f90-8c12-480a-8306-a417f9a66afb.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e8c39231-fd71-49b1-b9b4-b5c3467f2792.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5aa49e10-aa4e-4e6c-a800-5175ba7cc06e.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/993f229f-2557-4958-9dbc-a5b0aea618b1.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7b45177e-4578-4f20-95f1-70eba2f2a800.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c56ea6a4-b932-4507-935a-fcadfefbf769.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ffe25f25-3ad4-4ed0-a164-0b33256aada0.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f1de3dfc-5fd7-43d1-8174-477caf49091b.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b39da36b-5e12-4ec2-850e-9e37ba1722bd.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/704a76a7-a1cf-4096-b79c-3052610feff8.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/07077d47-6e48-4b43-998a-6d12925cb56f.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/edb8fb7b-107c-4b2e-9a94-72acdc904b32.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/671c16f2-4cf8-4726-8c96-18127447addb.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/52ac6f76-1240-4ecc-ae83-3260c67ab4b6.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/d555de89-cfad-49d2-b65c-96ba3d820a56.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e16a4c08-4f41-4094-a5d2-0d5787569b89.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2a32bb6c-b8e1-4737-8694-ca0996fbe282.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/7fd4a08f-c863-4910-bcc7-9351af7c0d15.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4ed37155-4d6b-461b-9d16-b197d2b30048.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/c18523dc-78b9-42f1-8b5c-4288be306270.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/1d196efa-4186-4779-a03d-50f7114d09ec.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2c500d5d-46f4-49a0-959d-39d63d5aa549.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/37029c7c-de25-459e-8c3a-eaa0b3716489.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/9a0b9aac-9b05-435d-946f-f333243efdf4.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/365a7a77-c132-46bc-b9ed-58d575a1ce98.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e1aa59e4-7a4e-49a6-b7bc-1439457486b4.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/32a83623-d77c-4dc9-8f2d-e4b831469857.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/62abed75-79e0-48b7-be5c-6b7c1c5bdde8.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/30e55ce8-19ea-40aa-b97c-960797e2e258.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/838387c4-56e2-4c7f-a243-7a158449b405.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/07cbb1c8-787b-4f2d-8b33-b94f20d7e6b2.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/bab7c214-9c62-48bc-ac74-1b6afa352d19.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/43301173-fba2-40d4-916d-b7ff153d5932.csv
RUNNING
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/ede2e18b-79ce-45e3-8157-a3f8ca19f508.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/2a826f65-5526-4d4b-87ed-f9bad1a2fef9.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/346fbc08-0bc1-44b3-a13c-6712e99d59d3.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/b5d7ec0a-ec97-40f1-b20f-3eda284ded7c.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/0ef2dd60-e3ec-45be-ab83-bb12fa1d333b.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5771b2ee-19c1-4dc0-b470-09c40e3770e7.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/bbf10020-56c8-4ddc-89d0-8993368947f0.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5a496d38-7956-429e-a854-2989e66dd74b.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/5988a66e-1706-4e47-9aa4-b57e85737e45.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/65158c85-2448-4a83-8d47-fbb03dd0d755.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/601f794c-2edd-4722-92e4-2395e1a250d6.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/34cc49de-5ef0-4ae1-a2d3-bf0ab6df1dcb.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/11eaea8c-4087-406d-a7ea-2bf4a52a8439.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/55c30203-990f-45c9-9abf-b7f56a81a8c4.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/4868ac5e-b324-4a13-8019-2dae127fe1b9.csv
QUEUED
SUCCEEDED
s3://ree-cloud-athena/4e854588-0915-4dfd-b8e3-4c75f5e84ba2.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/848fd003-47ab-4b83-ad96-23a9c2479a81.csv
QUEUED
RUNNING
RUNNING
SUCCEEDED
s3://ree-cloud-athena/f2901c0d-4ef5-4b69-b757-78e51e8b50fd.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/e660d0b8-5c5a-4158-a3d5-27b2d4f250b7.csv
QUEUED
RUNNING
SUCCEEDED
s3://ree-cloud-athena/97e9f29f-4381-4772-a8d4-4a9e182f446a.csv
>>> 
>>> 
>>> /home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:37: RuntimeWarning: Mean of empty slice
  featL = pd.concat(featL)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:38: RuntimeWarning: Mean of empty slice
  
>>> 
>>> featL
                       timebucket       ts  ...  joystick_latency  e2e_latency
timebucket                                  ...                               
2020-08-03 14:52:27  1.596459e+09 -59367.0  ...               0.0          NaN
2020-08-03 14:52:28  1.596459e+09 -58367.0  ...               0.0          NaN
2020-08-03 14:52:29  1.596459e+09 -57367.0  ...               0.0          NaN
2020-08-03 14:52:30  1.596459e+09 -56367.0  ...               0.0          NaN
2020-08-03 14:52:31  1.596459e+09 -55367.0  ...               0.0          NaN
...                           ...      ...  ...               ...          ...
2020-07-22 06:39:16  1.595393e+09  55544.0  ...              32.0        208.0
2020-07-22 06:39:17  1.595393e+09  56544.0  ...              32.0        191.0
2020-07-22 06:39:18  1.595393e+09  57544.0  ...              51.0        209.0
2020-07-22 06:39:19  1.595393e+09  58544.0  ...              33.0        213.0
2020-07-22 06:39:20  1.595393e+09  59544.0  ...              32.0        282.0

[28459 rows x 33 columns]
>>> featL.describe()
         timebucket            ts  object_distance  ...  camera_latency  joystick_latency   e2e_latency
count  2.845900e+04  28459.000000     26899.000000  ...    28459.000000      28459.000000  25663.000000
mean   1.595495e+09   -652.399803        48.037731  ...      306.224252         37.915633    364.961969
std    9.098490e+05  34337.529588        51.788829  ...     1960.055852         28.560440   2065.889317
min    1.593425e+09 -59989.000000         1.400000  ...      109.000000          0.000000    163.000000
25%    1.594976e+09 -30247.500000         9.026087  ...      163.000000         31.000000    202.000000
50%    1.595435e+09   -921.000000        23.725000  ...      174.000000         37.000000    214.000000
75%    1.596438e+09  28724.000000        66.100006  ...      188.000000         42.500000    230.000000
max    1.597124e+09  59986.000000       204.600000  ...    60407.000000       2267.000000  60395.000000

[8 rows x 33 columns]
>>> feat
                       timebucket       ts  ...  joystick_latency  e2e_latency
timebucket                                  ...                               
2020-07-22 06:37:21  1.595393e+09 -59456.0  ...                33          210
2020-07-22 06:37:22  1.595393e+09 -58456.0  ...                51          207
2020-07-22 06:37:23  1.595393e+09 -57456.0  ...                34          200
2020-07-22 06:37:24  1.595393e+09 -56456.0  ...                32          205
2020-07-22 06:37:25  1.595393e+09 -55456.0  ...                35          195
...                           ...      ...  ...               ...          ...
2020-07-22 06:39:16  1.595393e+09  55544.0  ...                32          208
2020-07-22 06:39:17  1.595393e+09  56544.0  ...                32          191
2020-07-22 06:39:18  1.595393e+09  57544.0  ...                51          209
2020-07-22 06:39:19  1.595393e+09  58544.0  ...                33          213
2020-07-22 06:39:20  1.595393e+09  59544.0  ...                32          282

[120 rows x 33 columns]
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 57, in <module>
    X = s_s.interpMissingMatrix(featL[xL])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: "['steering_wheel_deg', 'modem_tx', 'brake_partition', 'steering_partition', 'modem_rx', 'modem_rtt', 'steering_angle_deg'] not in index"
>>> /home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:37: RuntimeWarning: Mean of empty slice
  featL = pd.concat(featL)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:38: RuntimeWarning: Mean of empty slice
  
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'brake_partition'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 47, in <module>
    for t in tL: featL.loc[:,t] = t_r.normPercentile(featL[t],perc=[5,95])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'brake_partition'
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'e2e_latency_ms'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 47, in <module>
    for t in tL: featL.loc[:,t] = t_r.normPercentile(featL[t],perc=[5,95])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'e2e_latency_ms'
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'steering_wheel_deg'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 49, in <module>
    featL.loc[:,i] = np.log(1e-6 + np.abs(featL[i]))
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'steering_wheel_deg'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 47, in <module>
    for t in tL: featL.loc[:,t] = t_r.normPercentile(featL[t],perc=[5,95])
  File "/home/sabeiro/lav//src/lernia/lernia/train_reshape.py", line 78, in normPercentile
    tmin, norm = np.percentile(y1,[1,99])
  File "<__array_function__ internals>", line 5, in percentile
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 3732, in percentile
    return _quantile_unchecked(
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 3851, in _quantile_unchecked
    r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 3429, in _ureduce
    r = func(a, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 3967, in _quantile_ureduce_func
    x1 = take(ap, indices_below, axis=axis) * weights_below
  File "<__array_function__ internals>", line 5, in take
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py", line 191, in take
    return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py", line 58, in _wrapfunc
    return bound(*args, **kwds)
IndexError: cannot do a non-empty take from an empty axes.
>>> 
>>> featL
                       timebucket       ts  object_distance  ...  modem_tx  modem_rx  b_latency
timebucket                                                   ...                               
2020-08-03 14:52:27  1.596459e+09 -59367.0         0.853707  ...       0.0       0.0          2
2020-08-03 14:52:28  1.596459e+09 -58367.0         0.853707  ...       0.0       0.0          0
2020-08-03 14:52:29  1.596459e+09 -57367.0         0.853707  ...       0.0       0.0          1
2020-08-03 14:52:30  1.596459e+09 -56367.0         0.853707  ...       0.0       0.0          4
2020-08-03 14:52:31  1.596459e+09 -55367.0         0.853707  ...       0.0       0.0          0
...                           ...      ...              ...  ...       ...       ...        ...
2020-07-22 06:39:16  1.595393e+09  55544.0         0.404749  ...       NaN       NaN          1
2020-07-22 06:39:17  1.595393e+09  56544.0         0.391136  ...       NaN       NaN          2
2020-07-22 06:39:18  1.595393e+09  57544.0         0.391136  ...       NaN       NaN          3
2020-07-22 06:39:19  1.595393e+09  58544.0         0.391136  ...       NaN       NaN          2
2020-07-22 06:39:20  1.595393e+09  59544.0         0.391136  ...       NaN       NaN          4

[28459 rows x 37 columns]
>>> featL['modem_tx\ 
  C-c C-c
Process Python interrupt
Python 3.8.2 (default, Jul 16 2020, 14:00:26) 
[GCC 9.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> python.el: native completion setup loaded
>>> /home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:37: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_tx"] = feat.apply(lambda x: np.nanmean(x['modem0_tx'] + x['modem1_tx'] + x['modem2_tx'] + x['modem3_tx']),axis=1)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:38: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_rx"] = feat.apply(lambda x: np.nanmean(x['modem0_rx'] + x['modem1_rx'] + x['modem2_rx'] + x['modem3_rx']),axis=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 209, in <module>
    scaler = MinMaxScaler(feature_range=(-1, 1))
NameError: name 'MinMaxScaler' is not defined
>>> featL
                       timebucket       ts  object_distance  ...  modem_tx  modem_rx  b_latency
timebucket                                                   ...                               
2020-08-03 14:52:27  1.596459e+09 -59367.0         0.853707  ...       0.0       0.0          2
2020-08-03 14:52:28  1.596459e+09 -58367.0         0.853707  ...       0.0       0.0          0
2020-08-03 14:52:29  1.596459e+09 -57367.0         0.853707  ...       0.0       0.0          1
2020-08-03 14:52:30  1.596459e+09 -56367.0         0.853707  ...       0.0       0.0          4
2020-08-03 14:52:31  1.596459e+09 -55367.0         0.853707  ...       0.0       0.0          0
...                           ...      ...              ...  ...       ...       ...        ...
2020-07-22 06:39:16  1.595393e+09  55544.0         0.404749  ...       NaN       NaN          1
2020-07-22 06:39:17  1.595393e+09  56544.0         0.391136  ...       NaN       NaN          2
2020-07-22 06:39:18  1.595393e+09  57544.0         0.391136  ...       NaN       NaN          3
2020-07-22 06:39:19  1.595393e+09  58544.0         0.391136  ...       NaN       NaN          2
2020-07-22 06:39:20  1.595393e+09  59544.0         0.391136  ...       NaN       NaN          4

[28459 rows x 37 columns]
>>> 
>>> --------------------histograms-and-outliers------------------
>>> featL[mL].describe()
                     modem_rtt  modem_rx  modem_tx
timebucket                                        
2020-08-03 14:52:27   0.165374       0.0       0.0
2020-08-03 14:52:28   0.190784       0.0       0.0
2020-08-03 14:52:29   0.190784       0.0       0.0
2020-08-03 14:52:30   0.206692       0.0       0.0
2020-08-03 14:52:31   0.219344       0.0       0.0
...                        ...       ...       ...
2020-07-22 06:39:16   0.303764       NaN       NaN
2020-07-22 06:39:17   0.302294       NaN       NaN
2020-07-22 06:39:18   0.302924       NaN       NaN
2020-07-22 06:39:19   0.302924       NaN       NaN
2020-07-22 06:39:20   0.076965       NaN       NaN

[28459 rows x 3 columns]
>>> featL[mL].describe()
          modem_rtt  modem_rx  modem_tx
count  27896.000000   20777.0   20743.0
mean       0.121833       0.0       0.0
std        0.141850       0.0       0.0
min        0.000000       0.0       0.0
25%        0.014490       0.0       0.0
50%        0.080850       0.0       0.0
75%        0.172304       0.0       0.0
max        0.999912       0.0       0.0
>>> featL[mL].describe()
          modem_rtt  modem_rx  modem_tx
count  27896.000000   20777.0   20743.0
mean       0.121833       0.0       0.0
std        0.141850       0.0       0.0
min        0.000000       0.0       0.0
25%        0.014490       0.0       0.0
50%        0.080850       0.0       0.0
75%        0.172304       0.0       0.0
max        0.999912       0.0       0.0
>>> mL
['modem_rtt', 'modem_rx', 'modem_tx']
>>> /home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:37: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_tx"] = feat.apply(lambda x: np.nanmean(x['modem0_tx'] + x['modem1_tx'] + x['modem2_tx'] + x['modem3_tx']),axis=1)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:38: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_rx"] = feat.apply(lambda x: np.nanmean(x['modem0_rx'] + x['modem1_rx'] + x['modem2_rx'] + x['modem3_rx']),axis=1)
>>> featL[mL].describe().values()
         modem0_rtt    modem1_rtt    modem2_rtt  ...     modem1_tx     modem2_tx     modem3_tx
count  28459.000000  28459.000000  28459.000000  ...  2.761900e+04  2.845900e+04  2.845900e+04
mean     321.352085    537.229453    118.500176  ...           inf           inf           inf
std     1200.673250   1080.933596    284.547941  ...           NaN           NaN           NaN
min        0.000000      0.000000      0.000000  ... -3.542121e+06 -3.542123e+06 -3.542130e+06
25%        0.000000     53.000000     51.000000  ...  0.000000e+00  0.000000e+00  0.000000e+00
50%       64.000000     93.000000     57.000000  ...  4.002667e+05  3.036361e+01  6.834754e+02
75%       87.000000    713.000000     64.000000  ...  9.110994e+05  9.002946e+05  9.046672e+05
max    31581.000000  19829.000000   4856.000000  ...           inf           inf           inf

[8 rows x 12 columns]
>>> featL[mL].describe().values
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'numpy.ndarray' object is not callable
>>> featL[mL].describe().values
array([[ 2.84590000e+04,  2.84590000e+04,  2.84590000e+04,
         2.84590000e+04,  2.18840000e+04,  2.76190000e+04,
         2.84590000e+04,  2.84590000e+04,  2.18840000e+04,
         2.76190000e+04,  2.84590000e+04,  2.84590000e+04],
       [ 3.21352085e+02,  5.37229453e+02,  1.18500176e+02,
         3.93988176e+02,  4.50809237e+04,             inf,
                    inf,             inf,  6.38787815e+05,
                    inf,             inf,             inf],
       [ 1.20067325e+03,  1.08093360e+03,  2.84547941e+02,
         7.11943855e+02,  3.89041039e+04,             nan,
                    nan,             nan,  4.94564846e+05,
                    nan,             nan,             nan],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00, -2.74863494e+05, -2.74863324e+05,
        -2.74863512e+05, -2.74864046e+05, -3.54212288e+06,
        -3.54212069e+06, -3.54212312e+06, -3.54213000e+06],
       [ 0.00000000e+00,  5.30000000e+01,  5.10000000e+01,
         3.00000000e+01,  1.27763665e+03,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  4.71366296e+05,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 6.40000000e+01,  9.30000000e+01,  5.70000000e+01,
         3.90000000e+01,  6.42311439e+04,  1.57808254e+04,
         1.13590689e+03,  1.30291177e+03,  8.71722267e+05,
         4.00266708e+05,  3.03636082e+01,  6.83475417e+02],
       [ 8.70000000e+01,  7.13000000e+02,  6.40000000e+01,
         6.61000000e+02,  7.02335146e+04,  6.81627251e+04,
         6.71857692e+04,  6.76445868e+04,  9.28818186e+05,
         9.11099400e+05,  9.00294584e+05,  9.04667205e+05],
       [ 3.15810000e+04,  1.98290000e+04,  4.85600000e+03,
         9.38700000e+03,  1.53831999e+05,             inf,
                    inf,             inf,  1.26733415e+06,
                    inf,             inf,             inf]])
>>> /home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:37: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_tx"] = feat.apply(lambda x: np.nanmean(x['modem0_tx'] + x['modem1_tx'] + x['modem2_tx'] + x['modem3_tx']),axis=1)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:38: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_rx"] = feat.apply(lambda x: np.nanmean(x['modem0_rx'] + x['modem1_rx'] + x['modem2_rx'] + x['modem3_rx']),axis=1)
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 36, in <module>
    feat.loc[:,"modem_rtt"] = feat.apply(lambda x: np.nanmean(x['modem0_rtt'],x['modem1_rtt'],x['modem2_rtt'],x['modem3_rtt']),axis=1)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 6878, in apply
    return op.get_result()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py", line 186, in get_result
    return self.apply_standard()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py", line 295, in apply_standard
    result = libreduction.compute_reduction(
  File "pandas/_libs/reduction.pyx", line 620, in pandas._libs.reduction.compute_reduction
  File "pandas/_libs/reduction.pyx", line 128, in pandas._libs.reduction.Reducer.get_result
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 36, in <lambda>
    feat.loc[:,"modem_rtt"] = feat.apply(lambda x: np.nanmean(x['modem0_rtt'],x['modem1_rtt'],x['modem2_rtt'],x['modem3_rtt']),axis=1)
  File "<__array_function__ internals>", line 5, in nanmean
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/nanfunctions.py", line 948, in nanmean
    cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims)
  File "<__array_function__ internals>", line 5, in sum
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py", line 2241, in sum
    return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py", line 83, in _wrapreduction
    return reduction(axis=axis, dtype=dtype, out=out, **passkwargs)
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py", line 47, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
TypeError: 'numpy.float64' object cannot be interpreted as an integer
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 4411, in get_value
    return libindex.get_value_at(s, key)
  File "pandas/_libs/index.pyx", line 44, in pandas._libs.index.get_value_at
  File "pandas/_libs/index.pyx", line 45, in pandas._libs.index.get_value_at
  File "pandas/_libs/util.pxd", line 98, in pandas._libs.util.get_value_at
  File "pandas/_libs/util.pxd", line 83, in pandas._libs.util.validate_indexer
TypeError: 'tuple' object cannot be interpreted as an integer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 36, in <module>
    feat.loc[:,"modem_rtt"] = feat.apply(lambda x: np.nanmean(x['modem0_rtt','modem1_rtt','modem2_rtt','modem3_rtt']),axis=1)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 6878, in apply
    return op.get_result()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py", line 186, in get_result
    return self.apply_standard()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py", line 295, in apply_standard
    result = libreduction.compute_reduction(
  File "pandas/_libs/reduction.pyx", line 620, in pandas._libs.reduction.compute_reduction
  File "pandas/_libs/reduction.pyx", line 128, in pandas._libs.reduction.Reducer.get_result
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 36, in <lambda>
    feat.loc[:,"modem_rtt"] = feat.apply(lambda x: np.nanmean(x['modem0_rtt','modem1_rtt','modem2_rtt','modem3_rtt']),axis=1)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/series.py", line 871, in __getitem__
    result = self.index.get_value(self, key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 4419, in get_value
    raise e1
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 4405, in get_value
    return self._engine.get_value(s, k, tz=getattr(series.dtype, "tz", None))
  File "pandas/_libs/index.pyx", line 80, in pandas._libs.index.IndexEngine.get_value
  File "pandas/_libs/index.pyx", line 90, in pandas._libs.index.IndexEngine.get_value
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: ('modem0_rtt', 'modem1_rtt', 'modem2_rtt', 'modem3_rtt')
>>> 
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 36
    feat.loc[:,"modem_rtt"] = np.nanmean(feat[['modem0_rtt','modem1_rtt','modem2_rtt','modem3_rtt']]
                                                                                                   ^
SyntaxError: unexpected EOF while parsing
>>> featL[['modem0_rx','modem1_rx','modem2_rx','modem3_rx']]
                     modem0_rx  modem1_rx  modem2_rx  modem3_rx
timebucket                                                     
2020-08-03 14:52:27        0.0        0.0        0.0        0.0
2020-08-03 14:52:28        0.0        0.0        0.0        0.0
2020-08-03 14:52:29        0.0        0.0        0.0        0.0
2020-08-03 14:52:30        0.0        0.0        0.0        0.0
2020-08-03 14:52:31        0.0        0.0        0.0        0.0
...                        ...        ...        ...        ...
2020-07-22 06:39:16        NaN        0.0        0.0        0.0
2020-07-22 06:39:17        NaN        0.0        0.0        0.0
2020-07-22 06:39:18        NaN        0.0        0.0        0.0
2020-07-22 06:39:19        NaN        0.0        0.0        0.0
2020-07-22 06:39:20        NaN        0.0        0.0        0.0

[28459 rows x 4 columns]
>>> np.nanmean(featL[['modem0_rx','modem1_rx','modem2_rx','modem3_rx']])
inf
>>> feat.apply(lambda x: np.nanmean(x[['modem0_rx','modem1_rx','modem2_rx','modem3_rx']),axis=1)
  File "<stdin>", line 1
    feat.apply(lambda x: np.nanmean(x[['modem0_rx','modem1_rx','modem2_rx','modem3_rx']),axis=1)
                                                                                       ^
SyntaxError: closing parenthesis ')' does not match opening parenthesis '['
>>> feat.apply(lambda x: np.nanmean(x[['modem0_rx','modem1_rx','modem2_rx','modem3_rx']]),axis=1)
timebucket
2020-07-22 06:37:21    0.0
2020-07-22 06:37:22    0.0
2020-07-22 06:37:23    0.0
2020-07-22 06:37:24    0.0
2020-07-22 06:37:25    0.0
                      ... 
2020-07-22 06:39:16    0.0
2020-07-22 06:39:17    0.0
2020-07-22 06:39:18    0.0
2020-07-22 06:39:19    0.0
2020-07-22 06:39:20    0.0
Length: 120, dtype: float64
>>> feat.dtypes
timebucket          float64
ts                  float64
object_distance     float64
brake_pressure      float64
force_lat           float64
force_lon           float64
yaw_rate            float64
steering_wheel      float64
steering_angle      float64
wheel_speed         float64
vehicle_ping        float64
rtp_lost            float64
rtp_late            float64
modem0_rtt          float64
modem1_rtt          float64
modem2_rtt          float64
modem3_rtt          float64
modem0_rx           float64
modem1_rx           float64
modem2_rx           float64
modem3_rx           float64
modem0_tx           float64
modem1_tx           float64
modem2_tx           float64
modem3_tx           float64
camera_jitter       float64
room_ram            float64
room_cpu            float64
vehicle_ram         float64
vehicle_cpu         float64
camera_latency        int64
joystick_latency      int64
e2e_latency           int64
dtype: object
>>> feat[mL].describe()
       modem0_rtt   modem1_rtt  modem2_rtt   modem3_rtt  ...  modem0_tx  modem1_tx  modem2_tx  modem3_tx
count       120.0   120.000000  120.000000   120.000000  ...        0.0      120.0      120.0      120.0
mean          0.0  1286.416667   50.266667   346.825000  ...        NaN        0.0        0.0        0.0
std           0.0  1022.155628    4.562789   394.602846  ...        NaN        0.0        0.0        0.0
min           0.0    39.000000   41.000000    21.000000  ...        NaN        0.0        0.0        0.0
25%           0.0   247.750000   47.000000    32.750000  ...        NaN        0.0        0.0        0.0
50%           0.0  1036.000000   50.000000    61.500000  ...        NaN        0.0        0.0        0.0
75%           0.0  2482.000000   53.000000   721.000000  ...        NaN        0.0        0.0        0.0
max           0.0  3261.000000   65.000000  1375.000000  ...        NaN        0.0        0.0        0.0

[8 rows x 12 columns]
>>> 
>>> 
>>> featL
                       timebucket       ts  object_distance  ...  modem_rtt  modem_tx  modem_rx
timebucket                                                   ...                               
2020-08-03 14:52:27  1.596459e+09 -59367.0        75.414280  ...    417.500       0.0       0.0
2020-08-03 14:52:28  1.596459e+09 -58367.0        75.414280  ...    478.000       0.0       0.0
2020-08-03 14:52:29  1.596459e+09 -57367.0        75.414280  ...    478.000       0.0       0.0
2020-08-03 14:52:30  1.596459e+09 -56367.0        75.414280  ...    515.875       0.0       0.0
2020-08-03 14:52:31  1.596459e+09 -55367.0        75.414280  ...    546.000       0.0       0.0
...                           ...      ...              ...  ...        ...       ...       ...
2020-07-22 06:39:16  1.595393e+09  55544.0         4.996429  ...    747.000       0.0       0.0
2020-07-22 06:39:17  1.595393e+09  56544.0         4.700000  ...    743.500       0.0       0.0
2020-07-22 06:39:18  1.595393e+09  57544.0         4.700000  ...    745.000       0.0       0.0
2020-07-22 06:39:19  1.595393e+09  58544.0         4.700000  ...    745.000       0.0       0.0
2020-07-22 06:39:20  1.595393e+09  59544.0         4.700000  ...    207.000       0.0       0.0

[28459 rows x 36 columns]
>>> /home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:36: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_rtt"] = feat.apply(lambda x: np.nanmean(x['modem0_rtt'] + x['modem1_rtt'] + x['modem2_rtt'] + x['modem3_rtt']),axis=1)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:37: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_tx"] = feat.apply(lambda x: np.nanmean(x['modem0_tx'] + x['modem1_tx'] + x['modem2_tx'] + x['modem3_tx']),axis=1)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:38: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_rx"] = feat.apply(lambda x: np.nanmean(x['modem0_rx'] + x['modem1_rx'] + x['modem2_rx'] + x['modem3_rx']),axis=1)
>>> X
                     object_distance  brake_pressure  force_lat  ...  room_cpu  vehicle_ram  vehicle_cpu
timebucket                                                       ...                                    
2020-08-03 14:52:27         0.875085        0.896326   0.790495  ...  0.129357     0.334332     0.720374
2020-08-03 14:52:28         0.875085        0.000000   0.775304  ...  0.105469     0.336589     0.781962
2020-08-03 14:52:29         0.875085        0.000000   0.743074  ...  0.160238     0.336847     0.887177
2020-08-03 14:52:30         0.875085        0.000000   0.789785  ...  0.175332     0.335272     0.942841
2020-08-03 14:52:31         0.875085        0.000000   0.786731  ...  0.113796     0.335028     0.678639
...                              ...             ...        ...  ...       ...          ...          ...
2020-07-22 06:39:16         0.490456        0.844612   0.676160  ...  0.079984     0.138531     0.910795
2020-07-22 06:39:17         0.478689        0.815945   0.683458  ...  0.060321     0.137651     0.708882
2020-07-22 06:39:18         0.478689        0.809732   0.685941  ...  0.077926     0.136038     0.768511
2020-07-22 06:39:19         0.478689        0.979265   0.685890  ...  0.170057     0.138324     0.858119
2020-07-22 06:39:20         0.478689        0.981898   0.686412  ...  0.041756     0.137806     0.858119

[28459 rows x 19 columns]
>>> /home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:36: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_rtt"] = feat.apply(lambda x: np.nanmean(x['modem0_rtt'] + x['modem1_rtt'] + x['modem2_rtt'] + x['modem3_rtt']),axis=1)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:37: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_tx"] = feat.apply(lambda x: np.nanmean(x['modem0_tx'] + x['modem1_tx'] + x['modem2_tx'] + x['modem3_tx']),axis=1)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:38: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_rx"] = feat.apply(lambda x: np.nanmean(x['modem0_rx'] + x['modem1_rx'] + x['modem2_rx'] + x['modem3_rx']),axis=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 39, in <module>
    if feat['modem_tx'] <= 0.: continue
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py", line 1478, in __nonzero__
    raise ValueError(
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
>>> 
>>> len(featD)
188
>>> featL[mL].describe()
         modem0_rtt    modem1_rtt    modem2_rtt  ...     modem1_tx     modem2_tx     modem3_tx
count  21242.000000  21242.000000  21242.000000  ...  2.084700e+04  2.124200e+04  2.114800e+04
mean     422.597048    522.024715    123.942873  ...           inf           inf           inf
std     1366.504516   1131.702299    303.102994  ...           NaN           NaN           NaN
min        0.000000      0.000000      0.000000  ... -3.542121e+06 -3.542123e+06 -3.542130e+06
25%       59.000000     52.000000     51.000000  ...  0.000000e+00  0.000000e+00  0.000000e+00
50%       72.000000     70.000000     57.000000  ...  8.518322e+05  8.380651e+05  8.425120e+05
75%      191.000000    682.000000     64.000000  ...  9.295065e+05  9.216431e+05  9.259386e+05
max    31581.000000  19829.000000   4856.000000  ...           inf           inf           inf

[8 rows x 12 columns]
>>> /home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:36: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_rtt"] = feat.apply(lambda x: np.nanmean(x['modem0_rtt'] + x['modem1_rtt'] + x['modem2_rtt'] + x['modem3_rtt']),axis=1)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:37: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_tx"] = feat.apply(lambda x: np.nanmean(x['modem0_tx'] + x['modem1_tx'] + x['modem2_tx'] + x['modem3_tx']),axis=1)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:38: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_rx"] = feat.apply(lambda x: np.nanmean(x['modem0_rx'] + x['modem1_rx'] + x['modem2_rx'] + x['modem3_rx']),axis=1)
934073.2562465966
0.0
980356.4047251451
0.0
0.0
0.0
1029306.8951338668
1010641.4055573589
919627.4197127365
1049144.6667342456
946481.4128867973
1084062.7867350087
inf
1043473.5696399603
0.0
1108061.369896904
955522.6048351778
1141526.206241183
0.0
973146.7046104029
1016511.3629817668
0.0
0.0
1011111.251691015
1062506.9220030983
917897.3252094175
995290.361102415
741981.8496519726
0.0
954495.977180284
992878.2203011769
993019.406385164
978344.8000291341
0.0
719578.0328886579
873156.9163567587
1026649.4373372179
0.0
1107218.347485764
1006699.8738266476
1112628.1101950735
0.0
1026649.4373372179  C-c C-cTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 37, in <module>
    feat.loc[:,"modem_tx"] = feat.apply(lambda x: np.nanmean(x[['modem0_tx','modem1_tx','modem2_tx','modem3_tx']]),axis=1)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 6878, in apply
    return op.get_result()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py", line 186, in get_result
    return self.apply_standard()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py", line 295, in apply_standard
    result = libreduction.compute_reduction(
  File "pandas/_libs/reduction.pyx", line 620, in pandas._libs.reduction.compute_reduction
  File "pandas/_libs/reduction.pyx", line 128, in pandas._libs.reduction.Reducer.get_result
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 37, in <lambda>
    feat.loc[:,"modem_tx"] = feat.apply(lambda x: np.nanmean(x[['modem0_tx','modem1_tx','modem2_tx','modem3_tx']]),axis=1)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/series.py", line 910, in __getitem__
    return self._get_with(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/series.py", line 939, in _get_with
    key_type = lib.infer_dtype(key, skipna=False)
  File "pandas/_libs/lib.pyx", line 1262, in pandas._libs.lib.infer_dtype
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/dtypes/cast.py", line 1458, in construct_1d_object_array_from_listlike
    def construct_1d_object_array_from_listlike(values):
KeyboardInterrupt
>>> 934073.2562465966
incindent_1596459206367.csv
0.0
incindent_1594283308275.csv
980356.4047251451
incindent_1596640266550.csv
0.0
incindent_1595431287070.csv
0.0
incindent_1595255406793.csv
0.0
incindent_1594198178254.csv
1029306.8951338668
incindent_1593765828813.csv
1010641.4055573589
incindent_1595935338376.csv
919627.4197127365
incindent_1595488131588.csv
1049144.6667342456
incindent_1593766835355.csv
946481.4128867973
incindent_1595493398961.csv
1084062.7867350087
incindent_1595406552278.csv
inf
incindent_1595432472889.csv
1043473.5696399603
incindent_1593687497131.csv
0.0
incindent_1595396352355.csv
1108061.369896904
incindent_1594191087191.csv
955522.6048351778
incindent_1595513667663.csv
1141526.206241183
incindent_1594206463882.csv
0.0
incindent_1596717751460.csv
973146.7046104029
incindent_1596625121955.csv
1016511.3629817668
incindent_1596640986229.csv
0.0  C-c C-cTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 36, in <module>
    feat.loc[:,"modem_rtt"] = feat.apply(lambda x: np.nanmean(x[['modem0_rtt','modem1_rtt','modem2_rtt','modem3_rtt']]),axis=1)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 6878, in apply
    return op.get_result()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py", line 186, in get_result
    return self.apply_standard()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py", line 284, in apply_standard
    values = self.values
  File "pandas/_libs/properties.pyx", line 34, in pandas._libs.properties.CachedProperty.__get__
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py", line 134, in values
    return self.obj.values
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py", line 5487, in values
    return self._data.as_array(transpose=self._AXIS_REVERSED)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py", line 830, in as_array
    arr = mgr._interleave()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py", line 839, in _interleave
    dtype = _interleaved_dtype(self.blocks)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py", line 1872, in _interleaved_dtype
    return find_common_type([b.dtype for b in blocks])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/dtypes/cast.py", line 1387, in find_common_type
    return np.find_common_type(types, [])
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/numerictypes.py", line 624, in find_common_type
    maxa = _can_coerce_all(array_types)
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/numerictypes.py", line 554, in _can_coerce_all
    numcoerce = len([x for x in dtypelist if newdtype >= x])
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/numerictypes.py", line 554, in <listcomp>
    numcoerce = len([x for x in dtypelist if newdtype >= x])
KeyboardInterrupt
>>> f = 'incindent_1595432472889.csv'
>>> feat['modem3_tx']
1      1.202451e+06
2               inf
3      8.789780e+05
4               inf
5               inf
           ...     
116    0.000000e+00
117    0.000000e+00
118    0.000000e+00
119    7.883201e+05
120    1.142210e+02
Name: modem3_tx, Length: 120, dtype: float64
>>> inf
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'inf' is not defined
>>> float(inf)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'inf' is not defined
>>> float('inf')
inf
>>> feat['modem3_tx']
1      1.202451e+06
2               NaN
3      8.789780e+05
4               NaN
5               NaN
           ...     
116    0.000000e+00
117    0.000000e+00
118    0.000000e+00
119    7.883201e+05
120    1.142210e+02
Name: modem3_tx, Length: 120, dtype: float64
>>> 934073.2562465966
incindent_1596459206367.csv
0.0
incindent_1594283308275.csv
980356.4047251451
incindent_1596640266550.csv
0.0
incindent_1595431287070.csv
0.0
incindent_1595255406793.csv
0.0
incindent_1594198178254.csv
1029306.8951338668
incindent_1593765828813.csv
1010641.4055573589
incindent_1595935338376.csv
919627.4197127365
incindent_1595488131588.csv
1049144.6667342456
incindent_1593766835355.csv
946481.4128867973
incindent_1595493398961.csv
1084062.7867350087
incindent_1595406552278.csv
1106835.1584717468
incindent_1595432472889.csv
1043473.5696399603
incindent_1593687497131.csv
0.0
incindent_1595396352355.csv
1108061.369896904
incindent_1594191087191.csv
955522.6048351778
incindent_1595513667663.csv
1141526.206241183
incindent_1594206463882.csv
0.0
incindent_1596717751460.csv
973146.7046104029
incindent_1596625121955.csv
1016511.3629817668
incindent_1596640986229.csv
0.0
incindent_1594647165307.csv
0.0
incindent_1595264515808.csv
1011111.251691015
incindent_1594974284525.csv
1062506.9220030983
incindent_1595265312692.csv
917897.3252094175
incindent_1595493124697.csv
995290.361102415
incindent_1596456976630.csv
741981.8496519726
incindent_1594116670217.csv
0.0
incindent_1594647163747.csv
954495.977180284
incindent_1596617608732.csv
992878.2203011769
incindent_1596705829381.csv
993019.406385164
incindent_1596458402349.csv
978344.8000291341
incindent_1595322327235.csv
0.0
incindent_1595581603041.csv
719578.0328886579
incindent_1594116669818.csv
873156.9163567587
incindent_1594115758877.csv
1026649.4373372179
incindent_1594918273607.csv
0.0
incindent_1595935161377.csv
1107218.347485764
incindent_1595407392455.csv
1006699.8738266476
incindent_1596444402887.csv
1112628.1101950735
incindent_1594199100160.csv
0.0
incindent_1594646368126.csv
977050.6683751917
incindent_1595494910440.csv
811690.5114275853
incindent_1594110208419.csv
988875.2027560179
incindent_1594809421094.csv
937055.8024497802
incindent_1595936492056.csv
844684.8764786351
incindent_1596708873124.csv
570740.4085676013
incindent_1594124270938.csv
0.0
incindent_1595255262356.csv
0.0
incindent_1596707999242.csv
950055.1966919375
incindent_1596640271690.csv
0.0
incindent_1595441860070.csv
905929.7504858437
incindent_1595494910398.csv
780787.1606362872
incindent_1593686595834.csv
896549.6600539881
incindent_1595493005358.csv
896255.9965790553
incindent_1595580788261.csv
713626.9352926193
incindent_1594109458837.csv
1036524.6479551838
incindent_1595317802843.csv
990511.8825338891
incindent_1594024279448.csv
1041009.3165935151
incindent_1594974603528.csv
1070510.375711147
incindent_1594907310567.csv
1219498.06082792
incindent_1596790241889.csv
1105750.3399795885
incindent_1595494452078.csv
1031958.725228037
incindent_1595321694877.csv
780787.1606362872
incindent_1593686563918.csv
955425.6847232174
incindent_1596733221653.csv
1013288.56414693
incindent_1596709552963.csv
0.0
incindent_1595941359497.csv
1054225.916672661
incindent_1593424805987.csv
0.0
incindent_1595264950068.csv
1019615.019444396
incindent_1595410165780.csv
1028860.8310660319
incindent_1595514212998.csv
0.0
incindent_1595409214624.csv
1066936.1505426418
incindent_1596708267182.csv
940531.8962229027
incindent_1596709043307.csv
966791.0848261964
incindent_1595497635518.csv
961784.9536174583
incindent_1595432531691.csv
1024350.3438938418
incindent_1595317843284.csv
941602.4689999338
incindent_1595425632591.csv
1051153.122256797
incindent_1595394753656.csv
1016954.2328952185
incindent_1595318684362.csv
1010895.8579198536
incindent_1594199100165.csv
1001939.4821839497
incindent_1596016310729.csv
0.0
incindent_1595426066398.csv
1046258.7056955319
incindent_1596703352501.csv
1020912.6622509054
incindent_1594966726307.csv
987858.8834283426
incindent_1596709607702.csv
978908.3335405232
incindent_1596644388217.csv
1052747.5447649297
incindent_1595316376508.csv
996189.7158378512
incindent_1595494452020.csv
972671.1306354642
incindent_1596007811340.csv
0.0
incindent_1595935076399.csv
976752.7425768115
incindent_1595938362536.csv
1177327.5153623137
incindent_1596709564502.csv
0.0
incindent_1595264816409.csv
0.0
incindent_1597054080305.csv
0.0
incindent_1595425793658.csv
0.0
incindent_1596627094932.csv
938566.6308025948
incindent_1595485413107.csv
862593.6903779459
incindent_1594115184437.csv
984421.9167830046
incindent_1595434796269.csv
1039074.111063811
incindent_1597039101978.csv
1057892.1338450662
incindent_1595405454115.csv
1034493.1624781506
incindent_1597048213357.csv
940674.6507233394
incindent_1596524823319.csv
1003348.3302993743
incindent_1595493466078.csv
1071309.4335629311
incindent_1596008091179.csv
1071806.8220150254
incindent_1596721425481.csv
1107561.7264586862
incindent_1596523773819.csv
1006039.8907878336
incindent_1595518274850.csv
971116.0125818031
incindent_1595938760277.csv
1013748.8576954408
incindent_1595317877024.csv
0.0
incindent_1594115092339.csv
0.0
incindent_1596708267185.csv
1054629.9253229257
incindent_1595941231556.csv
1049205.3081903225
incindent_1595494910416.csv
1034266.3411129717
incindent_1595322979315.csv
0.0
incindent_1595488983907.csv
1100718.9935449706
incindent_1595393741577.csv
1051699.2836628824
incindent_1595317633982.csv
1006690.583872778
incindent_1595316254267.csv
986994.9961746002
incindent_1595434965390.csv
0.0
incindent_1594123411933.csv
1019033.3318072187
incindent_1595405803402.csv
0.0
incindent_1596803370938.csv
985319.6956284206
incindent_1597049860002.csv
0.0
incindent_1594646692065.csv
974547.5013985791
incindent_1594975537925.csv
974530.9332103408
incindent_1594977376720.csv
1018879.8084558429
incindent_1596727373963.csv
957632.9088185866
incindent_1596523461461.csv
0.0
incindent_1595935560580.csv
0.0
incindent_1594112334078.csv
966248.2878987644
incindent_1595425410629.csv
0.0
incindent_1594647076646.csv
0.0
incindent_1595508687906.csv
1020212.1332266856
incindent_1595402376709.csv
956302.0742436643
incindent_1595492806117.csv
0.0
incindent_1595392592858.csv
967128.9842391384
incindent_1594976027505.csv
1018818.9614053462
incindent_1596720587800.csv
1057857.4052747027
incindent_1595938760271.csv
960003.1605494318
incindent_1596614074503.csv
1007059.2018794264
incindent_1596642659547.csv
997453.9010689855
incindent_1596719683224.csv
0.0
incindent_1595255655617.csv
1190716.8247177198
incindent_1594199281476.csv
1042053.7908176312
incindent_1595319526862.csv
0.0
incindent_1594646175406.csv
0.0
incindent_1596523936600.csv
0.0
incindent_1596709271743.csv
1044259.4510372968
incindent_1596526294263.csv
967128.9842391384
incindent_1594975955745.csv
1045173.1672376981
incindent_1594899080220.csv
919613.4670651586
incindent_1596814947722.csv
980868.0370779096
incindent_1595571651419.csv
972667.163072132
incindent_1595487665668.csv
985924.0018305653
incindent_1594197553928.csv
1093383.757139843
incindent_1595494970308.csv
1024338.033951805
incindent_1594195778309.csv
944354.7304334273
incindent_1597077977849.csv
946811.6188817406
incindent_1596612781482.csv
0.0
incindent_1595580755180.csv
971486.0321145566
incindent_1594104404360.csv
0.0
incindent_1594358692765.csv
1051681.616748438
incindent_1596437786122.csv
1027221.6357238883
incindent_1595318289902.csv
1053393.9448794685
incindent_1594904369167.csv
1115694.7972645403
incindent_1595335577815.csv
1056270.0500819397
incindent_1596697339043.csv
570740.4085676013
incindent_1594124309478.csv
0.0
incindent_1595315837588.csv
1041160.4342142373
incindent_1595323001032.csv
992377.0400575519
incindent_1595406338921.csv
1024509.748750015
incindent_1595519177227.csv
980504.2183027851
incindent_1593448527263.csv
1039164.8594865883
incindent_1594193623711.csv
1049151.6200719413
incindent_1595405201303.csv
1109765.2292425565
incindent_1595434881835.csv
956547.9032336303
incindent_1596012135859.csv
0.0
incindent_1594358503167.csv
1094510.9872098388
incindent_1594196977687.csv
0.0
incindent_1596803309858.csv
496000679.87015295
incindent_1596705566341.csv
991603.689861571
incindent_1596446752981.csv
0.0
incindent_1594198287253.csv
995193.2566270549
incindent_1595572316745.csv
0.0
incindent_1595523014367.csv
1049830.8656989394
incindent_1595410275591.csv
977122.9547567833
incindent_1596437951143.csv
1055555.6371579836
incindent_1593533735754.csv
938334.815721646
incindent_1596815890099.csv
893729.9547829408
incindent_1596008518637.csv
919985.0759197411
incindent_1595334985102.csv
1140748.9228340548
incindent_1595321862611.csv
987538.6119783041
incindent_1595402441843.csv
0.0
incindent_1595941320161.csv
981289.6321797813
incindent_1595402606164.csv
1124792.831309287
incindent_1596720446920.csv
1001696.1793728918
incindent_1596720896580.csv
1006007.1433691399
incindent_1595574833534.csv
1047991.7368055504
incindent_1595316521028.csv
780787.1606362872
incindent_1593686588554.csv
0.0
incindent_1595517905948.csv
957345.3130739899
incindent_1595577730146.csv
1188089.7357624853
incindent_1594191267046.csv
1003536.029117962
incindent_1595937261938.csv
985866.6156613635
incindent_1597048391241.csv
952194.2566083127
incindent_1594110338577.csv
918449.964527969
incindent_1595578701784.csv
991782.6719241618
incindent_1596709271783.csv
973050.7912182283
incindent_1596717889359.csv
985319.6956284206
incindent_1597049859942.csv
1153535.1030112037
incindent_1593697758889.csv
0.0
incindent_1595425540663.csv
983793.4143670156
incindent_1595493398942.csv
0.0
incindent_1595517904669.csv
921305.8948254104
incindent_1597066428138.csv
1016098.390665962
incindent_1594018401590.csv
1006167.4003238514
incindent_1596719532743.csv
973628.8359217509
incindent_1595580885061.csv
0.0
incindent_1595508320283.csv
1164211.684209354
incindent_1594098967139.csv
971995.9588970914
incindent_1596646030848.csv
1075376.17058004
incindent_1594805196590.csv
1053139.4461641414
incindent_1595265447127.csv
0.0
incindent_1595421230221.csv
0.0
incindent_1595422333796.csv
1031751.9321297389
incindent_1597124088756.csv
1036268.9971467442
incindent_1594191267014.csv
0.0
incindent_1595508319475.csv
983924.018818761
incindent_1596625962294.csv
1028741.6349697174
incindent_1597050028256.csv
974047.5277338449
incindent_1595579931549.csv
1267353.0812801959
incindent_1594207974784.csv
957524.9530875252
incindent_1595590003170.csv
950117.3461385046
incindent_1595266178029.csv
1007634.1092128203
incindent_1595319198270.csv
1124792.831309287
incindent_1596720389420.csv
907428.8961694196
incindent_1597051309420.csv
954733.8705415169
incindent_1595432603029.csv
1140748.9228340548
incindent_1595321812989.csv
0.0
incindent_1595335542883.csv
0.0
incindent_1595392700456.csv
>>> featL[mL].describe()
         modem0_rtt    modem1_rtt    modem2_rtt  ...     modem1_tx     modem2_tx     modem3_tx
count  21242.000000  21242.000000  21242.000000  ...  2.031400e+04  2.124000e+04  2.083300e+04
mean     422.597048    522.024715    123.942873  ...  5.360303e+05  5.784121e+05  5.775939e+05
std     1366.504516   1131.702299    303.102994  ...  5.069563e+05  6.829616e+06  6.867014e+06
min        0.000000      0.000000      0.000000  ... -3.542121e+06 -3.542123e+06 -3.542130e+06
25%       59.000000     52.000000     51.000000  ...  0.000000e+00  0.000000e+00  0.000000e+00
50%       72.000000     70.000000     57.000000  ...  8.446034e+05  8.380546e+05  8.374693e+05
75%      191.000000    682.000000     64.000000  ...  9.239658e+05  9.216360e+05  9.229548e+05
max    31581.000000  19829.000000   4856.000000  ...  1.939308e+06  9.931981e+08  9.890070e+08

[8 rows x 12 columns]
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 72, in <module>
    feat[tL].boxplot()
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: "['camera_latency', 'e2e_latency', 'joystick_latency'] not in index"
>>> incindent_1596459206367.csv
incindent_1596456976630.csv
incindent_1596458402349.csv
incindent_1596444402887.csv
incindent_1596524823319.csv
incindent_1596523773819.csv
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'joystick_latency'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 31, in <module>
    if feat['joystick_latency'].sum() <= 0.: print(f)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'joystick_latency'
>>> feat['joystick_latency']
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'joystick_latency'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'joystick_latency'
>>> .~lock.incindent_1596720446920.csv#
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 27, in <module>
    feat = pd.read_csv(projDir + f)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 448, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 880, in __init__
    self._make_engine(self.engine)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 1114, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 1891, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 374, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File /home/sabeiro/lav//rem/raw/incident/incident_sec/.~lock.incindent_1595432472889.csv# does not exist: '/home/sabeiro/lav//rem/raw/incident/incident_sec/.~lock.incindent_1595432472889.csv#'
>>> .~lock.incindent_1596720446920.csv#
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 27, in <module>
    feat = pd.read_csv(projDir + f)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 448, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 880, in __init__
    self._make_engine(self.engine)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 1114, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 1891, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 374, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File /home/sabeiro/lav//rem/raw/incident/incident_sec/.~lock.incindent_1595432472889.csv# does not exist: '/home/sabeiro/lav//rem/raw/incident/incident_sec/.~lock.incindent_1595432472889.csv#'
>>> .~lock.incindent_1596720446920.csv#
>>> incindent_1596459206367.csv
incindent_1596456976630.csv
incindent_1596458402349.csv
incindent_1596444402887.csv
incindent_1596524823319.csv
incindent_1596523773819.csv
incindent_1596523461461.csv
incindent_1596523936600.csv
incindent_1596526294263.csv
incindent_1596437786122.csv
incindent_1596446752981.csv
incindent_1596437951143.csv
>>> feat['joystick_latency']
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: string indices must be integers
>>> feat['joystick_latency']
1      33
2      51
3      34
4      32
5      35
       ..
116    32
117    32
118    51
119    33
120    32
Name: joystick_latency, Length: 120, dtype: int64
>>> f = 'incindent_1596437951143.csv'
>>> feat['joystick_latency']
1     NaN
2     NaN
3     NaN
4     NaN
5     NaN
       ..
116   NaN
117   NaN
118   NaN
119   NaN
120   NaN
Name: joystick_latency, Length: 120, dtype: float64
>>> 
>>> 
>>> 
>>> /home/sabeiro/lav//src/lernia/lernia/train_viz.py:339: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
  plt.subplot(len(groups), 1, i)
/home/sabeiro/lav//src/lernia/lernia/train_viz.py:339: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
  plt.subplot(len(groups), 1, i)
/home/sabeiro/lav//src/lernia/lernia/train_viz.py:339: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
  plt.subplot(len(groups), 1, i)
>>> /home/sabeiro/lav//src/lernia/lernia/train_viz.py:339: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
  plt.subplot(len(groups), 1, i)
/home/sabeiro/lav//src/lernia/lernia/train_viz.py:339: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
  plt.subplot(len(groups), 1, i)
/home/sabeiro/lav//src/lernia/lernia/train_viz.py:339: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
  plt.subplot(len(groups), 1, i)
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 106, in <module>
    sns.pairplot(featL[sL+['b_latency']],hue='b_latency',diag_kind="kde",markers="+",plot_kws=dict(s=50,edgecolor="b", linewidth=1),diag_kws=dict(shade=True))
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: "['b_latency'] not in index"
>>> /usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
>>> /usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
>>> /usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
>>> /usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:369: UserWarning: Default bandwidth for data is 0; skipping density estimation.
  warnings.warn(msg, UserWarning)
>>> i
'e2e_latency'
>>> threshold
13.001382485229687
>>> featL[i].min()
163.0
>>> threshold
182.0193547932156
>>> --------------------histograms-and-outliers------------------
>>> --------------------histograms-and-outliers------------------
>>> fL
['telemetry_2020-08-04T05.csv.cF5C1d1d', 'telemetry_2020-08-04T18.csv.18BfEb0B']
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_telemetry.py", line 26
    if len(down) == 0: return
                       ^
SyntaxError: 'return' outside function
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_telemetry.py", line 25, in <module>
    down = pd.read_csv()
TypeError: parser_f() missing 1 required positional argument: 'filepath_or_buffer'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_telemetry.py", line 25, in <module>
    down = pd.read_csv()
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 448, in _read
    parser = TextFileReader(fp_or_buf, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 880, in __init__
    self._make_engine(self.engine)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 1114, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 1891, in __init__
    self._reader = parsers.TextReader(src, **kwds)
  File "pandas/_libs/parsers.pyx", line 374, in pandas._libs.parsers.TextReader.__cinit__
  File "pandas/_libs/parsers.pyx", line 674, in pandas._libs.parsers.TextReader._setup_parser_source
FileNotFoundError: [Errno 2] File /home/sabeiro/lav/rem/raw/telemetrytelemetry_2020-08-04T05.csv.cF5C1d1d does not exist: '/home/sabeiro/lav/rem/raw/telemetrytelemetry_2020-08-04T05.csv.cF5C1d1d'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_telemetry.py", line 25, in <module>
    down = pd.read_csv(fileN)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 676, in parser_f
    return _read(filepath_or_buffer, kwds)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 454, in _read
    data = parser.read(nrows)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 1133, in read
    ret = self._engine.read(nrows)
  File "/usr/local/lib/python3.8/dist-packages/pandas/io/parsers.py", line 2037, in read
    data = self._reader.read(nrows)
  File "pandas/_libs/parsers.pyx", line 860, in pandas._libs.parsers.TextReader.read
  File "pandas/_libs/parsers.pyx", line 875, in pandas._libs.parsers.TextReader._read_low_memory
  File "pandas/_libs/parsers.pyx", line 929, in pandas._libs.parsers.TextReader._read_rows
  File "pandas/_libs/parsers.pyx", line 916, in pandas._libs.parsers.TextReader._tokenize_rows
  File "pandas/_libs/parsers.pyx", line 2071, in pandas._libs.parsers.raise_parser_error
pandas.errors.ParserError: Error tokenizing data. C error: Expected 50 fields in line 733612, saw 91

>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 102, in <module>
    
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 239, in corM
    cM[i, j] = corS(webH[colL[i]], webH[colL[j]])
NameError: name 'webH' is not defined
>>> cor
>>> array([[ 1., nan, nan, nan, nan, nan, nan, nan],
       [nan,  1., nan, nan, nan, nan, nan, nan],
       [nan, nan,  1., nan, nan, nan, nan, nan],
       [nan, nan, nan,  1., nan, nan, nan, nan],
       [nan, nan, nan, nan,  1., nan, nan, nan],
       [nan, nan, nan, nan, nan,  1., nan, nan],
       [nan, nan, nan, nan, nan, nan,  1., nan],
       [nan, nan, nan, nan, nan, nan, nan,  1.]])
>>> 
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: ('camera_latency', 'e2e_latency')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 122, in <module>
    # Sine sample with some noise and copy to y1 and y2 with a 1-second lag
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: ('camera_latency', 'e2e_latency')
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 122, in <module>
    lag_finder(featL['camera_latency','e2e_latency'])
TypeError: lag_finder() missing 1 required positional argument: 'sr'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 122, in <module>
    lag_finder(featL['camera_latency'],featL['e2e_latency'],sr = 1024)
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 111, in lag_finder
    corr = signal.correlate(y2, y1, mode='same') / np.sqrt(signal.correlate(y1, y1, mode='same')[int(n/2)] * signal.correlate(y2, y2, mode='same')[int(n/2)])
NameError: name 'signal' is not defined
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 107, in <module>
    corr = sp.signal.correlate(y2, y1, mode='same') / np.sqrt(sp.signal.correlate(y1, y1, mode='same')[int(n/2)] * sp.signal.correlate(y2, y2, mode='same')[int(n/2)])
NameError: name 'sp' is not defined
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 108, in <module>
    corr = sp.signal.correlate(y2, y1, mode='same') / np.sqrt(sp.signal.correlate(y1, y1, mode='same')[int(n/2)] * sp.signal.correlate(y2, y2, mode='same')[int(n/2)])
NameError: name 'sp' is not defined
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 109, in <module>
    delay_arr = np.linspace(-0.5*n/sr, 0.5*n/sr, n)
NameError: name 'sr' is not defined
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 122, in <module>
    y = np.linspace(0, 2*np.pi, sr)
NameError: name 'sr' is not defined
>>> len(ya)
1024
>>> y2 is -0.5 behind y1
>>> corr
array([nan, nan, nan, ..., nan, nan, nan])
>>> np.argmax(corr)
0
>>> y1
timebucket
2020-08-03 14:52:27    5.093750
2020-08-03 14:52:28    5.010635
2020-08-03 14:52:29    5.056246
2020-08-03 14:52:30    5.497168
2020-08-03 14:52:31    5.017280
                         ...   
2020-07-21 10:57:48    5.225747
2020-07-21 10:57:49    5.313206
2020-07-21 10:57:50    5.497168
2020-07-21 10:57:51    5.278115
2020-07-21 10:57:52    5.170484
Name: camera_latency, Length: 21884, dtype: float64
>>> sp.signal.correlate(y2, y1, mode='same')
array([nan, nan, nan, ..., nan, nan, nan])
>>> sp.signal.correlate(y2, y1, mode='same')
array([310692.90638712, 310719.23830788, 310746.9913201 , ...,
       311094.0614791 , 311070.55513442, 311043.48786712])
>>> y2 is 2.2848786729423587e-05 behind y1
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 108, in <module>
    n = len(y1)
TypeError: phaseLag() got an unexpected keyword argument 'isPlot'
>>> s_s.phaseLag(y1,y2)
array([5.72575931e-07, 7.05772571e-07, 6.32678980e-07, ...,
       1.25342424e-07, 1.53448137e-07, 1.40473753e-07])
>>> norm
624204.7836208993
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 110, in <module>
    
AttributeError: module 'albio.series_stat' has no attribute 'maxLag'
>>> 2.2848786729423587e-05
>>> 2.2848786729423587e-05
>>> 2.2848786729423587e-05
>>> 2.2848786729423587e-05
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/markers.py", line 305, in set_marker
    Path(marker)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/path.py", line 131, in __init__
    vertices = _to_unmasked_float_array(vertices)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/__init__.py", line 1289, in _to_unmasked_float_array
    return np.asarray(x, float)
  File "/usr/local/lib/python3.8/dist-packages/numpy/core/_asarray.py", line 83, in asarray
    return array(a, dtype, copy=False, order=order)
ValueError: could not convert string to float: '--'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 110, in <module>
    
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 160, in maxLag
    plt.axvline(x=0.,color="red",marker="--")
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py", line 2456, in axvline
    return gca().axvline(x=x, ymin=ymin, ymax=ymax, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py", line 916, in axvline
    l = mlines.Line2D([x, x], [ymin, ymax], transform=trans, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/lines.py", line 368, in __init__
    self._marker = MarkerStyle(marker, fillstyle)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/markers.py", line 228, in __init__
    self.set_marker(marker)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/markers.py", line 308, in set_marker
    raise ValueError('Unrecognized marker style {!r}'
ValueError: Unrecognized marker style '--'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 110, in <module>
    
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 160, in maxLag
    plt.axvline(x=0.,color="red",style="--")
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py", line 2456, in axvline
    return gca().axvline(x=x, ymin=ymin, ymax=ymax, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py", line 916, in axvline
    l = mlines.Line2D([x, x], [ymin, ymax], transform=trans, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/lines.py", line 390, in __init__
    self.update(kwargs)
  File "/usr/local/lib/python3.8/dist-packages/matplotlib/artist.py", line 996, in update
    raise AttributeError(f"{type(self).__name__!r} object "
AttributeError: 'Line2D' object has no property 'style'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 109, in <module>
    
  File "/usr/lib/python3.8/importlib/__init__.py", line 169, in reload
    _bootstrap._exec(spec, module)
  File "<frozen importlib._bootstrap>", line 604, in _exec
  File "<frozen importlib._bootstrap_external>", line 779, in exec_module
  File "<frozen importlib._bootstrap_external>", line 916, in get_code
  File "<frozen importlib._bootstrap_external>", line 846, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 160
    plt.axvline(x=0.,"--",color="red")
                     ^
SyntaxError: positional argument follows keyword argument
>>> 2.2848786729423587e-05
>>> 2.2848786729423587e-05
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/albio/albio/series_stat.py", line 170
    def interpMissing(y, isPlot=False):
    ^
IndentationError: expected an indented block
>>> np.zeros(n,n)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: Cannot interpret '21884' as a data type
>>> n = 3
>>> np.zeros((n,n))
array([], shape=(0, 0), dtype=float64)
>>> np.zeros((n,n))
array([], shape=(0, 0), dtype=float64)
>>> n = 3
>>> np.zeros((n,n))
array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]])
>>> 
>>> lagM
array([[ 2.28487867e-05,  1.39377599e-03,  4.28871727e-02,
        -8.91102682e-04, -1.20938628e-01,  1.19019330e-01,
        -4.44568843e-01, -6.85463602e-05,  1.51487456e-02,
        -1.28638669e-02, -1.46460723e-02,  1.67938582e-02,
        -8.60256820e-02, -8.60256820e-02,  2.28487867e-05,
         2.28487867e-05,  2.28487867e-05,  2.28487867e-05,
         5.50655760e-03],
       [-1.34807842e-03,  2.28487867e-05,  7.86226751e-02,
         4.34126948e-04,  2.12493717e-03,  1.11959055e-03,
         2.07923959e-03,  7.06027510e-03, -1.33208427e-02,
         2.28487867e-05,  2.28487867e-05, -8.45405109e-04,
         1.11524928e-01,  1.11524928e-01, -3.63295709e-03,
        -6.23771878e-03, -3.49586437e-03, -3.35877165e-03,
        -6.16917242e-04],
       [-4.28414751e-02, -7.85769776e-02,  2.28487867e-05,
         2.48846136e-01, -1.16528812e-03, -1.41411141e-01,
         6.75638624e-02,  6.16917242e-04, -2.05639081e-04,
         1.33094183e-01,  1.33048485e-01,  3.88429374e-04,
        -3.62610245e-02, -3.62610245e-02, -1.56514189e-02,
        -7.13567610e-02, -1.56971165e-02,  2.28487867e-05,
        -1.09445688e-02],
       [ 9.36800256e-04, -3.88429374e-04, -2.48800439e-01,
         2.28487867e-05, -1.89439291e-01, -2.92235982e-02,
        -2.70598181e-01,  3.42731801e-04, -4.85056893e-01,
        -3.77004981e-03, -3.86144496e-03, -4.90860485e-01,
        -7.49211717e-02, -7.49211717e-02, -4.99588722e-01,
        -4.94105013e-01, -4.94150711e-01, -4.99862907e-01,
        -4.99680117e-01],
       [ 1.20984326e-01, -2.07923959e-03,  1.21098570e-03,
         1.89484988e-01,  2.28487867e-05,  2.28487867e-05,
         2.28487867e-05,  9.33829914e-02,  8.45405109e-04,
         1.00237627e-01,  1.00237627e-01,  1.27884659e-01,
         3.74948590e-02,  3.74948590e-02,  3.95969474e-02,
         3.69007906e-02,  5.50427272e-02,  3.90942741e-02,
         3.60782342e-02],
       [-1.18973633e-01, -1.07389298e-03,  1.41456839e-01,
         2.92692958e-02,  2.28487867e-05,  2.28487867e-05,
         6.85463602e-05, -8.79678289e-03,  9.36800256e-04,
         6.11205045e-02,  4.38719554e-01,  1.94214687e-03,
        -8.22327834e-02, -8.22327834e-02,  7.53781474e-02,
         1.46163689e-01,  5.15697116e-02,  8.99099758e-02,
         7.31846639e-02],
       [ 4.44614541e-01, -2.03354202e-03, -6.75181648e-02,
         2.70643879e-01,  2.28487867e-05, -2.28487867e-05,
         2.28487867e-05,  2.84718731e-01,  9.36800256e-04,
         6.08920166e-02,  6.08920166e-02,  1.27930357e-01,
         1.63391674e-01,  1.63391674e-01,  4.63579034e-01,
         4.94287803e-01,  4.58597998e-01,  4.70250880e-01,
         4.50509528e-01],
       [ 1.14243934e-04, -7.01457753e-03, -5.71219668e-04,
        -2.97034227e-04, -9.33372938e-02,  8.84248046e-03,
        -2.84673034e-01,  2.28487867e-05,  1.00763149e-02,
         2.10048896e-01,  3.34734726e-02,  1.89873418e-02,
         8.47689988e-03,  8.43120230e-03,  2.28487867e-05,
         2.26202989e-03,  1.14243934e-04,  2.07923959e-03,
         2.28487867e-05],
       [-1.51030480e-02,  1.33665402e-02,  2.51336654e-04,
         4.85102591e-01, -7.99707536e-04, -8.91102682e-04,
        -8.91102682e-04, -1.00306174e-02,  2.28487867e-05,
         1.21098570e-03,  1.21098570e-03,  6.85463602e-05,
        -6.90718823e-02, -6.90718823e-02, -6.85463602e-05,
        -5.46086003e-03,  1.59941507e-04,  2.28487867e-05,
        -2.21633231e-03],
       [ 1.29095645e-02,  2.28487867e-05, -1.33048485e-01,
         3.81574738e-03, -1.00191930e-01, -6.10748069e-02,
        -6.08463191e-02, -2.10003199e-01, -1.16528812e-03,
         2.28487867e-05,  2.28487867e-05, -4.34126948e-04,
        -7.06712974e-02, -7.09911804e-02, -6.68783988e-02,
        -4.13791528e-02, -1.59233195e-01, -4.22474067e-02,
        -8.76707947e-02],
       [ 1.46917699e-02,  2.28487867e-05, -1.33002788e-01,
         3.90714253e-03, -1.00191930e-01, -4.38673856e-01,
        -6.08463191e-02, -3.34277750e-02, -1.16528812e-03,
         2.28487867e-05,  2.28487867e-05, -5.71219668e-04,
        -7.06712974e-02, -7.06712974e-02, -6.57816570e-02,
        -4.13791528e-02, -1.57770872e-01, -4.22474067e-02,
        -8.76707947e-02],
       [-1.67481607e-02,  8.91102682e-04, -3.42731801e-04,
         4.90906183e-01, -1.27838962e-01, -1.89644930e-03,
        -1.27884659e-01, -1.89416442e-02, -2.28487867e-05,
         4.79824521e-04,  6.16917242e-04,  2.28487867e-05,
        -6.97116483e-02, -6.97116483e-02,  2.28487867e-05,
         2.17063474e-03,  2.30772746e-03,  2.28487867e-05,
        -1.08988713e-02],
       [ 8.60713796e-02, -1.11479230e-01,  3.63067221e-02,
         7.49668693e-02, -3.74491614e-02,  8.22784810e-02,
        -1.63345976e-01, -8.43120230e-03,  6.91175799e-02,
         7.07169949e-02,  7.07169949e-02,  6.97573459e-02,
         2.28487867e-05,  2.28487867e-05,  1.59941507e-04,
        -5.78074304e-03,  6.60329936e-03,  3.22167893e-03,
        -2.28487867e-05],
       [ 8.60713796e-02, -1.11479230e-01,  3.63067221e-02,
         7.49668693e-02, -3.74491614e-02,  8.22784810e-02,
        -1.63345976e-01, -8.38550473e-03,  6.91175799e-02,
         7.10368779e-02,  7.07169949e-02,  6.97573459e-02,
         2.28487867e-05,  2.28487867e-05,  1.59941507e-04,
        -5.78074304e-03,  6.60329936e-03, -2.28487867e-05,
        -2.28487867e-05],
       [ 2.28487867e-05,  3.67865466e-03,  1.56971165e-02,
         4.99634419e-01, -3.95512498e-02, -7.53324498e-02,
        -4.63533336e-01,  2.28487867e-05,  1.14243934e-04,
         6.69240963e-02,  6.58273546e-02,  2.28487867e-05,
        -1.14243934e-04, -1.14243934e-04,  2.28487867e-05,
         2.28487867e-05,  2.28487867e-05,  2.28487867e-05,
         6.85463602e-05],
       [ 2.28487867e-05,  6.28341635e-03,  7.14024585e-02,
         4.94150711e-01, -3.68550930e-02, -1.46117991e-01,
        -4.94242106e-01, -2.21633231e-03,  5.50655760e-03,
         4.14248503e-02,  4.14248503e-02, -2.12493717e-03,
         5.82644062e-03,  5.82644062e-03,  2.28487867e-05,
         2.28487867e-05,  2.28487867e-05,  2.28487867e-05,
        -5.46086003e-03],
       [ 2.28487867e-05,  3.54156194e-03,  1.57428141e-02,
         4.94196408e-01, -5.49970297e-02, -5.15240141e-02,
        -4.58552301e-01, -6.85463602e-05, -1.14243934e-04,
         1.59278892e-01,  1.57816570e-01, -2.26202989e-03,
        -6.55760179e-03, -6.55760179e-03,  2.28487867e-05,
         2.28487867e-05,  2.28487867e-05,  2.28487867e-05,
        -5.46086003e-03],
       [ 2.28487867e-05,  3.40446922e-03,  2.28487867e-05,
         4.99908605e-01, -3.90485765e-02, -8.98642782e-02,
        -4.70205182e-01, -2.03354202e-03,  2.28487867e-05,
         4.22931042e-02,  4.22931042e-02,  2.28487867e-05,
        -3.17598136e-03,  6.85463602e-05,  2.28487867e-05,
         2.28487867e-05,  2.28487867e-05,  2.28487867e-05,
         2.28487867e-05],
       [-5.46086003e-03,  6.62614815e-04,  1.09902664e-02,
         4.99725815e-01, -3.60325367e-02, -7.31389663e-02,
        -4.50463830e-01,  2.28487867e-05,  2.26202989e-03,
         8.77164923e-02,  8.77164923e-02,  1.09445688e-02,
         6.85463602e-05,  6.85463602e-05, -2.28487867e-05,
         5.50655760e-03,  5.50655760e-03,  2.28487867e-05,
         2.28487867e-05]])
>>> 
>>> lagM
array([[ 0.00000000e+00,  1.39377599e-03,  4.28871727e-02,
        -8.91102682e-04, -1.20938628e-01,  1.19019330e-01,
        -4.44568843e-01, -6.85463602e-05,  1.51487456e-02,
        -1.28638669e-02, -1.46460723e-02,  1.67938582e-02,
        -8.60256820e-02, -8.60256820e-02,  2.28487867e-05,
         2.28487867e-05,  2.28487867e-05,  2.28487867e-05,
         5.50655760e-03],
       [ 1.39377599e-03,  0.00000000e+00,  7.86226751e-02,
         4.34126948e-04,  2.12493717e-03,  1.11959055e-03,
         2.07923959e-03,  7.06027510e-03, -1.33208427e-02,
         2.28487867e-05,  2.28487867e-05, -8.45405109e-04,
         1.11524928e-01,  1.11524928e-01, -3.63295709e-03,
        -6.23771878e-03, -3.49586437e-03, -3.35877165e-03,
        -6.16917242e-04],
       [ 4.28871727e-02,  7.86226751e-02,  0.00000000e+00,
         2.48846136e-01, -1.16528812e-03, -1.41411141e-01,
         6.75638624e-02,  6.16917242e-04, -2.05639081e-04,
         1.33094183e-01,  1.33048485e-01,  3.88429374e-04,
        -3.62610245e-02, -3.62610245e-02, -1.56514189e-02,
        -7.13567610e-02, -1.56971165e-02,  2.28487867e-05,
        -1.09445688e-02],
       [-8.91102682e-04,  4.34126948e-04,  2.48846136e-01,
         0.00000000e+00, -1.89439291e-01, -2.92235982e-02,
        -2.70598181e-01,  3.42731801e-04, -4.85056893e-01,
        -3.77004981e-03, -3.86144496e-03, -4.90860485e-01,
        -7.49211717e-02, -7.49211717e-02, -4.99588722e-01,
        -4.94105013e-01, -4.94150711e-01, -4.99862907e-01,
        -4.99680117e-01],
       [-1.20938628e-01,  2.12493717e-03, -1.16528812e-03,
        -1.89439291e-01,  0.00000000e+00,  2.28487867e-05,
         2.28487867e-05,  9.33829914e-02,  8.45405109e-04,
         1.00237627e-01,  1.00237627e-01,  1.27884659e-01,
         3.74948590e-02,  3.74948590e-02,  3.95969474e-02,
         3.69007906e-02,  5.50427272e-02,  3.90942741e-02,
         3.60782342e-02],
       [ 1.19019330e-01,  1.11959055e-03, -1.41411141e-01,
        -2.92235982e-02,  2.28487867e-05,  0.00000000e+00,
         6.85463602e-05, -8.79678289e-03,  9.36800256e-04,
         6.11205045e-02,  4.38719554e-01,  1.94214687e-03,
        -8.22327834e-02, -8.22327834e-02,  7.53781474e-02,
         1.46163689e-01,  5.15697116e-02,  8.99099758e-02,
         7.31846639e-02],
       [-4.44568843e-01,  2.07923959e-03,  6.75638624e-02,
        -2.70598181e-01,  2.28487867e-05,  6.85463602e-05,
         0.00000000e+00,  2.84718731e-01,  9.36800256e-04,
         6.08920166e-02,  6.08920166e-02,  1.27930357e-01,
         1.63391674e-01,  1.63391674e-01,  4.63579034e-01,
         4.94287803e-01,  4.58597998e-01,  4.70250880e-01,
         4.50509528e-01],
       [-6.85463602e-05,  7.06027510e-03,  6.16917242e-04,
         3.42731801e-04,  9.33829914e-02, -8.79678289e-03,
         2.84718731e-01,  0.00000000e+00,  1.00763149e-02,
         2.10048896e-01,  3.34734726e-02,  1.89873418e-02,
         8.47689988e-03,  8.43120230e-03,  2.28487867e-05,
         2.26202989e-03,  1.14243934e-04,  2.07923959e-03,
         2.28487867e-05],
       [ 1.51487456e-02, -1.33208427e-02, -2.05639081e-04,
        -4.85056893e-01,  8.45405109e-04,  9.36800256e-04,
         9.36800256e-04,  1.00763149e-02,  0.00000000e+00,
         1.21098570e-03,  1.21098570e-03,  6.85463602e-05,
        -6.90718823e-02, -6.90718823e-02, -6.85463602e-05,
        -5.46086003e-03,  1.59941507e-04,  2.28487867e-05,
        -2.21633231e-03],
       [-1.28638669e-02,  2.28487867e-05,  1.33094183e-01,
        -3.77004981e-03,  1.00237627e-01,  6.11205045e-02,
         6.08920166e-02,  2.10048896e-01,  1.21098570e-03,
         0.00000000e+00,  2.28487867e-05, -4.34126948e-04,
        -7.06712974e-02, -7.09911804e-02, -6.68783988e-02,
        -4.13791528e-02, -1.59233195e-01, -4.22474067e-02,
        -8.76707947e-02],
       [-1.46460723e-02,  2.28487867e-05,  1.33048485e-01,
        -3.86144496e-03,  1.00237627e-01,  4.38719554e-01,
         6.08920166e-02,  3.34734726e-02,  1.21098570e-03,
         2.28487867e-05,  0.00000000e+00, -5.71219668e-04,
        -7.06712974e-02, -7.06712974e-02, -6.57816570e-02,
        -4.13791528e-02, -1.57770872e-01, -4.22474067e-02,
        -8.76707947e-02],
       [ 1.67938582e-02, -8.45405109e-04,  3.88429374e-04,
        -4.90860485e-01,  1.27884659e-01,  1.94214687e-03,
         1.27930357e-01,  1.89873418e-02,  6.85463602e-05,
        -4.34126948e-04, -5.71219668e-04,  0.00000000e+00,
        -6.97116483e-02, -6.97116483e-02,  2.28487867e-05,
         2.17063474e-03,  2.30772746e-03,  2.28487867e-05,
        -1.08988713e-02],
       [-8.60256820e-02,  1.11524928e-01, -3.62610245e-02,
        -7.49211717e-02,  3.74948590e-02, -8.22327834e-02,
         1.63391674e-01,  8.47689988e-03, -6.90718823e-02,
        -7.06712974e-02, -7.06712974e-02, -6.97116483e-02,
         0.00000000e+00,  2.28487867e-05,  1.59941507e-04,
        -5.78074304e-03,  6.60329936e-03,  3.22167893e-03,
        -2.28487867e-05],
       [-8.60256820e-02,  1.11524928e-01, -3.62610245e-02,
        -7.49211717e-02,  3.74948590e-02, -8.22327834e-02,
         1.63391674e-01,  8.43120230e-03, -6.90718823e-02,
        -7.09911804e-02, -7.06712974e-02, -6.97116483e-02,
         2.28487867e-05,  0.00000000e+00,  1.59941507e-04,
        -5.78074304e-03,  6.60329936e-03, -2.28487867e-05,
        -2.28487867e-05],
       [ 2.28487867e-05, -3.63295709e-03, -1.56514189e-02,
        -4.99588722e-01,  3.95969474e-02,  7.53781474e-02,
         4.63579034e-01,  2.28487867e-05, -6.85463602e-05,
        -6.68783988e-02, -6.57816570e-02,  2.28487867e-05,
         1.59941507e-04,  1.59941507e-04,  0.00000000e+00,
         2.28487867e-05,  2.28487867e-05,  2.28487867e-05,
         6.85463602e-05],
       [ 2.28487867e-05, -6.23771878e-03, -7.13567610e-02,
        -4.94105013e-01,  3.69007906e-02,  1.46163689e-01,
         4.94287803e-01,  2.26202989e-03, -5.46086003e-03,
        -4.13791528e-02, -4.13791528e-02,  2.17063474e-03,
        -5.78074304e-03, -5.78074304e-03,  2.28487867e-05,
         0.00000000e+00,  2.28487867e-05,  2.28487867e-05,
        -5.46086003e-03],
       [ 2.28487867e-05, -3.49586437e-03, -1.56971165e-02,
        -4.94150711e-01,  5.50427272e-02,  5.15697116e-02,
         4.58597998e-01,  1.14243934e-04,  1.59941507e-04,
        -1.59233195e-01, -1.57770872e-01,  2.30772746e-03,
         6.60329936e-03,  6.60329936e-03,  2.28487867e-05,
         2.28487867e-05,  0.00000000e+00,  2.28487867e-05,
        -5.46086003e-03],
       [ 2.28487867e-05, -3.35877165e-03,  2.28487867e-05,
        -4.99862907e-01,  3.90942741e-02,  8.99099758e-02,
         4.70250880e-01,  2.07923959e-03,  2.28487867e-05,
        -4.22474067e-02, -4.22474067e-02,  2.28487867e-05,
         3.22167893e-03, -2.28487867e-05,  2.28487867e-05,
         2.28487867e-05,  2.28487867e-05,  0.00000000e+00,
         2.28487867e-05],
       [ 5.50655760e-03, -6.16917242e-04, -1.09445688e-02,
        -4.99680117e-01,  3.60782342e-02,  7.31846639e-02,
         4.50509528e-01,  2.28487867e-05, -2.21633231e-03,
        -8.76707947e-02, -8.76707947e-02, -1.08988713e-02,
        -2.28487867e-05, -2.28487867e-05,  6.85463602e-05,
        -5.46086003e-03, -5.46086003e-03,  2.28487867e-05,
         0.00000000e+00]])
>>> 
>>> 
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 113, in <module>
    y = np.tile(np.sin(y), 5)
  File "/home/sabeiro/lav//src/lernia/lernia/train_viz.py", line 388, in plotHeatmap
    if not labV:
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2149, in __nonzero__
    raise ValueError(
ValueError: The truth value of a Index is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
>>> [[ 2.28487867e-05  1.39377599e-03  4.28871727e-02 -8.91102682e-04
  -1.20938628e-01  1.19019330e-01 -4.44568843e-01 -6.85463602e-05
   1.51487456e-02 -1.28638669e-02 -1.46460723e-02  1.67938582e-02
  -8.60256820e-02 -8.60256820e-02  2.28487867e-05  2.28487867e-05
   2.28487867e-05  2.28487867e-05  5.50655760e-03]
 [-1.34807842e-03  2.28487867e-05  7.86226751e-02  4.34126948e-04
   2.12493717e-03  1.11959055e-03  2.07923959e-03  7.06027510e-03
  -1.33208427e-02  2.28487867e-05  2.28487867e-05 -8.45405109e-04
   1.11524928e-01  1.11524928e-01 -3.63295709e-03 -6.23771878e-03
  -3.49586437e-03 -3.35877165e-03 -6.16917242e-04]
 [-4.28414751e-02 -7.85769776e-02  2.28487867e-05  2.48846136e-01
  -1.16528812e-03 -1.41411141e-01  6.75638624e-02  6.16917242e-04
  -2.05639081e-04  1.33094183e-01  1.33048485e-01  3.88429374e-04
  -3.62610245e-02 -3.62610245e-02 -1.56514189e-02 -7.13567610e-02
  -1.56971165e-02  2.28487867e-05 -1.09445688e-02]
 [ 9.36800256e-04 -3.88429374e-04 -2.48800439e-01  2.28487867e-05
  -1.89439291e-01 -2.92235982e-02 -2.70598181e-01  3.42731801e-04
  -4.85056893e-01 -3.77004981e-03 -3.86144496e-03 -4.90860485e-01
  -7.49211717e-02 -7.49211717e-02 -4.99588722e-01 -4.94105013e-01
  -4.94150711e-01 -4.99862907e-01 -4.99680117e-01]
 [ 1.20984326e-01 -2.07923959e-03  1.21098570e-03  1.89484988e-01
   2.28487867e-05  2.28487867e-05  2.28487867e-05  9.33829914e-02
   8.45405109e-04  1.00237627e-01  1.00237627e-01  1.27884659e-01
   3.74948590e-02  3.74948590e-02  3.95969474e-02  3.69007906e-02
   5.50427272e-02  3.90942741e-02  3.60782342e-02]
 [-1.18973633e-01 -1.07389298e-03  1.41456839e-01  2.92692958e-02
   2.28487867e-05  2.28487867e-05  6.85463602e-05 -8.79678289e-03
   9.36800256e-04  6.11205045e-02  4.38719554e-01  1.94214687e-03
  -8.22327834e-02 -8.22327834e-02  7.53781474e-02  1.46163689e-01
   5.15697116e-02  8.99099758e-02  7.31846639e-02]
 [ 4.44614541e-01 -2.03354202e-03 -6.75181648e-02  2.70643879e-01
   2.28487867e-05 -2.28487867e-05  2.28487867e-05  2.84718731e-01
   9.36800256e-04  6.08920166e-02  6.08920166e-02  1.27930357e-01
   1.63391674e-01  1.63391674e-01  4.63579034e-01  4.94287803e-01
   4.58597998e-01  4.70250880e-01  4.50509528e-01]
 [ 1.14243934e-04 -7.01457753e-03 -5.71219668e-04 -2.97034227e-04
  -9.33372938e-02  8.84248046e-03 -2.84673034e-01  2.28487867e-05
   1.00763149e-02  2.10048896e-01  3.34734726e-02  1.89873418e-02
   8.47689988e-03  8.43120230e-03  2.28487867e-05  2.26202989e-03
   1.14243934e-04  2.07923959e-03  2.28487867e-05]
 [-1.51030480e-02  1.33665402e-02  2.51336654e-04  4.85102591e-01
  -7.99707536e-04 -8.91102682e-04 -8.91102682e-04 -1.00306174e-02
   2.28487867e-05  1.21098570e-03  1.21098570e-03  6.85463602e-05
  -6.90718823e-02 -6.90718823e-02 -6.85463602e-05 -5.46086003e-03
   1.59941507e-04  2.28487867e-05 -2.21633231e-03]
 [ 1.29095645e-02  2.28487867e-05 -1.33048485e-01  3.81574738e-03
  -1.00191930e-01 -6.10748069e-02 -6.08463191e-02 -2.10003199e-01
  -1.16528812e-03  2.28487867e-05  2.28487867e-05 -4.34126948e-04
  -7.06712974e-02 -7.09911804e-02 -6.68783988e-02 -4.13791528e-02
  -1.59233195e-01 -4.22474067e-02 -8.76707947e-02]
 [ 1.46917699e-02  2.28487867e-05 -1.33002788e-01  3.90714253e-03
  -1.00191930e-01 -4.38673856e-01 -6.08463191e-02 -3.34277750e-02
  -1.16528812e-03  2.28487867e-05  2.28487867e-05 -5.71219668e-04
  -7.06712974e-02 -7.06712974e-02 -6.57816570e-02 -4.13791528e-02
  -1.57770872e-01 -4.22474067e-02 -8.76707947e-02]
 [-1.67481607e-02  8.91102682e-04 -3.42731801e-04  4.90906183e-01
  -1.27838962e-01 -1.89644930e-03 -1.27884659e-01 -1.89416442e-02
  -2.28487867e-05  4.79824521e-04  6.16917242e-04  2.28487867e-05
  -6.97116483e-02 -6.97116483e-02  2.28487867e-05  2.17063474e-03
   2.30772746e-03  2.28487867e-05 -1.08988713e-02]
 [ 8.60713796e-02 -1.11479230e-01  3.63067221e-02  7.49668693e-02
  -3.74491614e-02  8.22784810e-02 -1.63345976e-01 -8.43120230e-03
   6.91175799e-02  7.07169949e-02  7.07169949e-02  6.97573459e-02
   2.28487867e-05  2.28487867e-05  1.59941507e-04 -5.78074304e-03
   6.60329936e-03  3.22167893e-03 -2.28487867e-05]
 [ 8.60713796e-02 -1.11479230e-01  3.63067221e-02  7.49668693e-02
  -3.74491614e-02  8.22784810e-02 -1.63345976e-01 -8.38550473e-03
   6.91175799e-02  7.10368779e-02  7.07169949e-02  6.97573459e-02
   2.28487867e-05  2.28487867e-05  1.59941507e-04 -5.78074304e-03
   6.60329936e-03 -2.28487867e-05 -2.28487867e-05]
 [ 2.28487867e-05  3.67865466e-03  1.56971165e-02  4.99634419e-01
  -3.95512498e-02 -7.53324498e-02 -4.63533336e-01  2.28487867e-05
   1.14243934e-04  6.69240963e-02  6.58273546e-02  2.28487867e-05
  -1.14243934e-04 -1.14243934e-04  2.28487867e-05  2.28487867e-05
   2.28487867e-05  2.28487867e-05  6.85463602e-05]
 [ 2.28487867e-05  6.28341635e-03  7.14024585e-02  4.94150711e-01
  -3.68550930e-02 -1.46117991e-01 -4.94242106e-01 -2.21633231e-03
   5.50655760e-03  4.14248503e-02  4.14248503e-02 -2.12493717e-03
   5.82644062e-03  5.82644062e-03  2.28487867e-05  2.28487867e-05
   2.28487867e-05  2.28487867e-05 -5.46086003e-03]
 [ 2.28487867e-05  3.54156194e-03  1.57428141e-02  4.94196408e-01
  -5.49970297e-02 -5.15240141e-02 -4.58552301e-01 -6.85463602e-05
  -1.14243934e-04  1.59278892e-01  1.57816570e-01 -2.26202989e-03
  -6.55760179e-03 -6.55760179e-03  2.28487867e-05  2.28487867e-05
   2.28487867e-05  2.28487867e-05 -5.46086003e-03]
 [ 2.28487867e-05  3.40446922e-03  2.28487867e-05  4.99908605e-01
  -3.90485765e-02 -8.98642782e-02 -4.70205182e-01 -2.03354202e-03
   2.28487867e-05  4.22931042e-02  4.22931042e-02  2.28487867e-05
  -3.17598136e-03  6.85463602e-05  2.28487867e-05  2.28487867e-05
   2.28487867e-05  2.28487867e-05  2.28487867e-05]
 [-5.46086003e-03  6.62614815e-04  1.09902664e-02  4.99725815e-01
  -3.60325367e-02 -7.31389663e-02 -4.50463830e-01  2.28487867e-05
   2.26202989e-03  8.77164923e-02  8.77164923e-02  1.09445688e-02
   6.85463602e-05  6.85463602e-05 -2.28487867e-05  5.50655760e-03
   5.50655760e-03  2.28487867e-05  2.28487867e-05]]
>>>     object_distance  brake_pressure  force_lat  ...  room_cpu  vehicle_ram  vehicle_cpu
0          0.000023        0.001394   0.042887  ...  0.000023     0.000023     0.005507
1         -0.001348        0.000023   0.078623  ... -0.003496    -0.003359    -0.000617
2         -0.042841       -0.078577   0.000023  ... -0.015697     0.000023    -0.010945
3          0.000937       -0.000388  -0.248800  ... -0.494151    -0.499863    -0.499680
4          0.120984       -0.002079   0.001211  ...  0.055043     0.039094     0.036078
5         -0.118974       -0.001074   0.141457  ...  0.051570     0.089910     0.073185
6          0.444615       -0.002034  -0.067518  ...  0.458598     0.470251     0.450510
7          0.000114       -0.007015  -0.000571  ...  0.000114     0.002079     0.000023
8         -0.015103        0.013367   0.000251  ...  0.000160     0.000023    -0.002216
9          0.012910        0.000023  -0.133048  ... -0.159233    -0.042247    -0.087671
10         0.014692        0.000023  -0.133003  ... -0.157771    -0.042247    -0.087671
11        -0.016748        0.000891  -0.000343  ...  0.002308     0.000023    -0.010899
12         0.086071       -0.111479   0.036307  ...  0.006603     0.003222    -0.000023
13         0.086071       -0.111479   0.036307  ...  0.006603    -0.000023    -0.000023
14         0.000023        0.003679   0.015697  ...  0.000023     0.000023     0.000069
15         0.000023        0.006283   0.071402  ...  0.000023     0.000023    -0.005461
16         0.000023        0.003542   0.015743  ...  0.000023     0.000023    -0.005461
17         0.000023        0.003404   0.000023  ...  0.000023     0.000023     0.000023
18        -0.005461        0.000663   0.010990  ...  0.005507     0.000023     0.000023

[19 rows x 19 columns]
>>> incindent_1596459206367.csv
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:39: RuntimeWarning: Mean of empty slice
  # feat.drop(columns=mL,inplace=True)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:40: RuntimeWarning: Mean of empty slice
  featL.append(feat)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:41: RuntimeWarning: Mean of empty slice
  featL = pd.concat(featL)
934073.2562465966
0.0
871021.8830547202
0.0
0.0
0.0
955509.7077305102
970169.6587846288
919627.4197127365
1049144.6667342456
0.0
1048254.6273275902
969643.5970196116
1030865.6269059775
0.0
1021964.6973737266
955522.6048351778
1141526.206241183
0.0
964515.3540791736
999530.7501712615
0.0
0.0
953122.7960696402
939985.7313286143
917897.3252094175
incindent_1596456976630.csv
933888.5531575667
0.0
0.0
955104.9766705253
953946.798418911
incindent_1596458402349.csv
985678.8075523505
1007582.8063308317
0.0
0.0
414799.99345049844
1000147.1183974723
0.0
1028646.4203475708
incindent_1596444402887.csv
983350.6852628856
0.0
0.0
977050.6683751917
0.0
981116.965835578
912395.0688241635
844684.8764786351
0.0
0.0
0.0
885400.1801227091
0.0
0.0
790076.2478891868
0.0
0.0
711373.4901865672
1024350.3438938418
1036381.0720161663
939351.7746111955
1062779.5080925357
993663.9964814269
1105750.3399795885
954455.4925574258
757693.8640230941
0.0
988003.8561285471
0.0
1046023.4189205316
0.0
998997.169084074
925639.1088123583
0.0
1034154.2667693475
0.0
966791.0848261964
961784.9536174583
1049464.5531675064
921706.3277270806
1051153.122256797
995195.0079886022
989811.4097915763
913203.3423934167
0.0
1046258.7056955319
1016085.4419769231
936147.4820721716
978908.3335405232
0.0
926423.0112732964
0.0
0.0
975472.9617871011
1177327.5153623137
0.0
0.0
0.0
0.0
938566.6308025948
0.0
939400.7321906625
966274.2822930205
1047214.2225034838
1034493.1624781506
incindent_1596524823319.csv
0.0
1003348.3302993743
1016348.7282705924
946369.7646628178
incindent_1596523773819.csv
999150.0080566219
1006039.8907878336
918598.244147273
1021579.256891274
0.0
0.0
937016.6064016939
999983.2233182159
1017522.9595746621
0.0
986093.192682757
1017714.2807732853
0.0
961179.0631878884
0.0
997948.8515305712
0.0
985319.6956284206
0.0
974547.5013985791
974530.9332103408
916363.4325774512
incindent_1596523461461.csv
0.0
0.0
0.0
945863.0754848022
0.0
0.0
970652.912228557
0.0
0.0
961533.7479263622
1018818.9614053462
1070675.7470368869
950680.4524827473
1007059.2018794264
952303.4183547093
0.0
1120287.1302775645
975815.264158343
0.0
incindent_1596523936600.csv
0.0
0.0
incindent_1596526294263.csv
1044259.4510372968
0.0
1025525.3965137197
919613.4670651586
942622.0474936693
972667.163072132
985924.0018305653
1093383.757139843
1000895.0333847429
816555.8062495367
920524.235574884
0.0
887710.1959201718
0.0
incindent_1596437786122.csv
1051681.616748438
1107436.065348706
959260.6084610238
1146349.6828043016
1010613.718486006
570740.4085676013
0.0
927464.8708310868
783125.4929026692
994487.8840977728
967988.5520408696
1001295.436003089
1000495.1312843441
1109765.2292425565
915733.9756168963
0.0
984394.0016659218
0.0
496000679.87015295
incindent_1596446752981.csv
991603.689861571
0.0
909993.410423344
0.0
0.0
incindent_1596437951143.csv
924837.2891777215
1036274.2430259297
938334.815721646
0.0
0.0
975120.216751044
987538.6119783041
0.0
26.246242788840558
1052907.7018128065
1001696.1793728918
428249.9087264786
1047991.7368055504
755588.2068627491
0.0
806670.1537727725
1183928.597485004
927808.1666663897
958612.266355618
894681.323195555
0.0
991782.6719241618
973050.7912182283
985319.6956284206
1147116.2267400657
0.0
892615.0043650093
0.0
0.0
1004236.6785605068
1065169.395587174
453425.06798352173
0.0
1147448.2685900282
951743.9694874594
973784.0174754497
964211.2759760597
0.0
0.0
1031751.9321297389
1108834.1315108147
0.0
956070.8022360213
992701.71164903
0.0
1111325.6907070987
0.0
927265.6963095441
997388.3093727932
1124792.831309287
907428.8961694196
666500.8354162646
1140748.9228340548
0.0
0.0
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'steering_wheel'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 52, in <module>
    threshold = np.nanmean(featL[i])*7.
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'steering_wheel'
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'steering_angle'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 52, in <module>
    threshold = np.nanmean(featL[i])*7.
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'steering_angle'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 66, in <module>
    X = s_s.interpMissingMatrix(featL[xL])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2806, in __getitem__
    indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1552, in _get_listlike_indexer
    self._validate_read_indexer(
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py", line 1646, in _validate_read_indexer
    raise KeyError(f"{not_found} not in index")
KeyError: "['steering_angle'] not in index"
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'steering_angle'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 53, in <module>
    threshold = np.nanmean(featL[i])*7.
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'steering_angle'
>>> feat.columns
Index(['timebucket', 'ts', 'object_distance', 'brake_pressure',
       'brake_partition', 'steering_partition', 'force_lat', 'force_lon',
       'yaw_rate', 'steering_wheel_deg', 'steering_angle_deg', 'td_brake',
       'td_throttle', 'wheel_speed', 'vehicle_ping', 'rtp_lost', 'rtp_late',
       'steering_interval', 'modem0_rtt', 'modem1_rtt', 'modem2_rtt',
       'modem3_rtt', 'modem0_rx', 'modem1_rx', 'modem2_rx', 'modem3_rx',
       'modem0_tx', 'modem1_tx', 'modem2_tx', 'modem3_tx', 'camera_jitter',
       'room_ram', 'room_cpu', 'vehicle_ram', 'vehicle_cpu', 'camera_latency',
       'joystick_latency', 'e2e_latency_ms', 'modem_rtt', 'modem_tx',
       'modem_rx'],
      dtype='object')
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'e2e_latency'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 53, in <module>
    threshold = np.nanmean(featL[i])*7.
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'e2e_latency'
>>> lagM
array([[ 2.28487867e-05,  1.39377599e-03,  4.28871727e-02,
        -8.91102682e-04, -1.20938628e-01,  1.19019330e-01,
        -4.44568843e-01, -6.85463602e-05,  1.51487456e-02,
        -1.28638669e-02, -1.46460723e-02,  1.67938582e-02,
        -8.60256820e-02, -8.60256820e-02,  2.28487867e-05,
         2.28487867e-05,  2.28487867e-05,  2.28487867e-05,
         5.50655760e-03],
       [-1.34807842e-03,  2.28487867e-05,  7.86226751e-02,
         4.34126948e-04,  2.12493717e-03,  1.11959055e-03,
         2.07923959e-03,  7.06027510e-03, -1.33208427e-02,
         2.28487867e-05,  2.28487867e-05, -8.45405109e-04,
         1.11524928e-01,  1.11524928e-01, -3.63295709e-03,
        -6.23771878e-03, -3.49586437e-03, -3.35877165e-03,
        -6.16917242e-04],
       [-4.28414751e-02, -7.85769776e-02,  2.28487867e-05,
         2.48846136e-01, -1.16528812e-03, -1.41411141e-01,
         6.75638624e-02,  6.16917242e-04, -2.05639081e-04,
         1.33094183e-01,  1.33048485e-01,  3.88429374e-04,
        -3.62610245e-02, -3.62610245e-02, -1.56514189e-02,
        -7.13567610e-02, -1.56971165e-02,  2.28487867e-05,
        -1.09445688e-02],
       [ 9.36800256e-04, -3.88429374e-04, -2.48800439e-01,
         2.28487867e-05, -1.89439291e-01, -2.92235982e-02,
        -2.70598181e-01,  3.42731801e-04, -4.85056893e-01,
        -3.77004981e-03, -3.86144496e-03, -4.90860485e-01,
        -7.49211717e-02, -7.49211717e-02, -4.99588722e-01,
        -4.94105013e-01, -4.94150711e-01, -4.99862907e-01,
        -4.99680117e-01],
       [ 1.20984326e-01, -2.07923959e-03,  1.21098570e-03,
         1.89484988e-01,  2.28487867e-05,  2.28487867e-05,
         2.28487867e-05,  9.33829914e-02,  8.45405109e-04,
         1.00237627e-01,  1.00237627e-01,  1.27884659e-01,
         3.74948590e-02,  3.74948590e-02,  3.95969474e-02,
         3.69007906e-02,  5.50427272e-02,  3.90942741e-02,
         3.60782342e-02],
       [-1.18973633e-01, -1.07389298e-03,  1.41456839e-01,
         2.92692958e-02,  2.28487867e-05,  2.28487867e-05,
         6.85463602e-05, -8.79678289e-03,  9.36800256e-04,
         6.11205045e-02,  4.38719554e-01,  1.94214687e-03,
        -8.22327834e-02, -8.22327834e-02,  7.53781474e-02,
         1.46163689e-01,  5.15697116e-02,  8.99099758e-02,
         7.31846639e-02],
       [ 4.44614541e-01, -2.03354202e-03, -6.75181648e-02,
         2.70643879e-01,  2.28487867e-05, -2.28487867e-05,
         2.28487867e-05,  2.84718731e-01,  9.36800256e-04,
         6.08920166e-02,  6.08920166e-02,  1.27930357e-01,
         1.63391674e-01,  1.63391674e-01,  4.63579034e-01,
         4.94287803e-01,  4.58597998e-01,  4.70250880e-01,
         4.50509528e-01],
       [ 1.14243934e-04, -7.01457753e-03, -5.71219668e-04,
        -2.97034227e-04, -9.33372938e-02,  8.84248046e-03,
        -2.84673034e-01,  2.28487867e-05,  1.00763149e-02,
         2.10048896e-01,  3.34734726e-02,  1.89873418e-02,
         8.47689988e-03,  8.43120230e-03,  2.28487867e-05,
         2.26202989e-03,  1.14243934e-04,  2.07923959e-03,
         2.28487867e-05],
       [-1.51030480e-02,  1.33665402e-02,  2.51336654e-04,
         4.85102591e-01, -7.99707536e-04, -8.91102682e-04,
        -8.91102682e-04, -1.00306174e-02,  2.28487867e-05,
         1.21098570e-03,  1.21098570e-03,  6.85463602e-05,
        -6.90718823e-02, -6.90718823e-02, -6.85463602e-05,
        -5.46086003e-03,  1.59941507e-04,  2.28487867e-05,
        -2.21633231e-03],
       [ 1.29095645e-02,  2.28487867e-05, -1.33048485e-01,
         3.81574738e-03, -1.00191930e-01, -6.10748069e-02,
        -6.08463191e-02, -2.10003199e-01, -1.16528812e-03,
         2.28487867e-05,  2.28487867e-05, -4.34126948e-04,
        -7.06712974e-02, -7.09911804e-02, -6.68783988e-02,
        -4.13791528e-02, -1.59233195e-01, -4.22474067e-02,
        -8.76707947e-02],
       [ 1.46917699e-02,  2.28487867e-05, -1.33002788e-01,
         3.90714253e-03, -1.00191930e-01, -4.38673856e-01,
        -6.08463191e-02, -3.34277750e-02, -1.16528812e-03,
         2.28487867e-05,  2.28487867e-05, -5.71219668e-04,
        -7.06712974e-02, -7.06712974e-02, -6.57816570e-02,
        -4.13791528e-02, -1.57770872e-01, -4.22474067e-02,
        -8.76707947e-02],
       [-1.67481607e-02,  8.91102682e-04, -3.42731801e-04,
         4.90906183e-01, -1.27838962e-01, -1.89644930e-03,
        -1.27884659e-01, -1.89416442e-02, -2.28487867e-05,
         4.79824521e-04,  6.16917242e-04,  2.28487867e-05,
        -6.97116483e-02, -6.97116483e-02,  2.28487867e-05,
         2.17063474e-03,  2.30772746e-03,  2.28487867e-05,
        -1.08988713e-02],
       [ 8.60713796e-02, -1.11479230e-01,  3.63067221e-02,
         7.49668693e-02, -3.74491614e-02,  8.22784810e-02,
        -1.63345976e-01, -8.43120230e-03,  6.91175799e-02,
         7.07169949e-02,  7.07169949e-02,  6.97573459e-02,
         2.28487867e-05,  2.28487867e-05,  1.59941507e-04,
        -5.78074304e-03,  6.60329936e-03,  3.22167893e-03,
        -2.28487867e-05],
       [ 8.60713796e-02, -1.11479230e-01,  3.63067221e-02,
         7.49668693e-02, -3.74491614e-02,  8.22784810e-02,
        -1.63345976e-01, -8.38550473e-03,  6.91175799e-02,
         7.10368779e-02,  7.07169949e-02,  6.97573459e-02,
         2.28487867e-05,  2.28487867e-05,  1.59941507e-04,
        -5.78074304e-03,  6.60329936e-03, -2.28487867e-05,
        -2.28487867e-05],
       [ 2.28487867e-05,  3.67865466e-03,  1.56971165e-02,
         4.99634419e-01, -3.95512498e-02, -7.53324498e-02,
        -4.63533336e-01,  2.28487867e-05,  1.14243934e-04,
         6.69240963e-02,  6.58273546e-02,  2.28487867e-05,
        -1.14243934e-04, -1.14243934e-04,  2.28487867e-05,
         2.28487867e-05,  2.28487867e-05,  2.28487867e-05,
         6.85463602e-05],
       [ 2.28487867e-05,  6.28341635e-03,  7.14024585e-02,
         4.94150711e-01, -3.68550930e-02, -1.46117991e-01,
        -4.94242106e-01, -2.21633231e-03,  5.50655760e-03,
         4.14248503e-02,  4.14248503e-02, -2.12493717e-03,
         5.82644062e-03,  5.82644062e-03,  2.28487867e-05,
         2.28487867e-05,  2.28487867e-05,  2.28487867e-05,
        -5.46086003e-03],
       [ 2.28487867e-05,  3.54156194e-03,  1.57428141e-02,
         4.94196408e-01, -5.49970297e-02, -5.15240141e-02,
        -4.58552301e-01, -6.85463602e-05, -1.14243934e-04,
         1.59278892e-01,  1.57816570e-01, -2.26202989e-03,
        -6.55760179e-03, -6.55760179e-03,  2.28487867e-05,
         2.28487867e-05,  2.28487867e-05,  2.28487867e-05,
        -5.46086003e-03],
       [ 2.28487867e-05,  3.40446922e-03,  2.28487867e-05,
         4.99908605e-01, -3.90485765e-02, -8.98642782e-02,
        -4.70205182e-01, -2.03354202e-03,  2.28487867e-05,
         4.22931042e-02,  4.22931042e-02,  2.28487867e-05,
        -3.17598136e-03,  6.85463602e-05,  2.28487867e-05,
         2.28487867e-05,  2.28487867e-05,  2.28487867e-05,
         2.28487867e-05],
       [-5.46086003e-03,  6.62614815e-04,  1.09902664e-02,
         4.99725815e-01, -3.60325367e-02, -7.31389663e-02,
        -4.50463830e-01,  2.28487867e-05,  2.26202989e-03,
         8.77164923e-02,  8.77164923e-02,  1.09445688e-02,
         6.85463602e-05,  6.85463602e-05, -2.28487867e-05,
         5.50655760e-03,  5.50655760e-03,  2.28487867e-05,
         2.28487867e-05]])
>>> lagM
array([[0.        , 0.        , 0.        , 0.        , 0.        ,
        0.11901933, 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.11152493, 0.11152493, 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.24884614, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.13309418,
        0.13304849, 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.12098433, 0.        , 0.        , 0.18948499, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.10023763,
        0.10023763, 0.12788466, 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.14145684, 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.43871955, 0.        , 0.        , 0.        , 0.        ,
        0.14616369, 0.        , 0.        , 0.        ],
       [0.44461454, 0.        , 0.        , 0.27064388, 0.        ,
        0.        , 0.        , 0.28471873, 0.        , 0.        ,
        0.        , 0.12793036, 0.16339167, 0.16339167, 0.46357903,
        0.4942878 , 0.458598  , 0.47025088, 0.45050953],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.2100489 ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.48510259, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.49090618, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.49963442, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.49415071, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.49419641, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.15927889,
        0.15781657, 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.4999086 , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.49972581, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ]])
>>> lagM.abs()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'numpy.ndarray' object has no attribute 'abs'
>>> np.abs(lagM)
array([[0.        , 0.        , 0.        , 0.        , 0.        ,
        0.11901933, 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.11152493, 0.11152493, 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.24884614, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.13309418,
        0.13304849, 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.12098433, 0.        , 0.        , 0.18948499, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.10023763,
        0.10023763, 0.12788466, 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.14145684, 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.43871955, 0.        , 0.        , 0.        , 0.        ,
        0.14616369, 0.        , 0.        , 0.        ],
       [0.44461454, 0.        , 0.        , 0.27064388, 0.        ,
        0.        , 0.        , 0.28471873, 0.        , 0.        ,
        0.        , 0.12793036, 0.16339167, 0.16339167, 0.46357903,
        0.4942878 , 0.458598  , 0.47025088, 0.45050953],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.2100489 ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.48510259, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.49090618, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.49963442, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.49415071, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.49419641, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.15927889,
        0.15781657, 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.4999086 , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.49972581, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        ]])
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py", line 1654, in create_block_manager_from_blocks
    make_block(values=blocks[0], placement=slice(0, len(axes[0])))
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/blocks.py", line 3053, in make_block
    return klass(values, ndim=ndim, placement=placement)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/blocks.py", line 124, in __init__
    raise ValueError(
ValueError: Wrong number of items passed 22, placement implies 19

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 113, in <module>
    plt.yticks(rotation=15)
  File "/home/sabeiro/lav//src/lernia/lernia/train_viz.py", line 389, in plotHeatmap
    X = pd.DataFrame(X,columns=labV,index=labV)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 464, in __init__
    mgr = init_ndarray(data, index, columns, dtype=dtype, copy=copy)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/construction.py", line 210, in init_ndarray
    return create_block_manager_from_blocks(block_values, [columns, index])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py", line 1664, in create_block_manager_from_blocks
    construction_error(tot_items, blocks[0].shape[1:], axes, e)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py", line 1694, in construction_error
    raise ValueError(f"Shape of passed values is {passed}, indices imply {implied}")
ValueError: Shape of passed values is (22, 22), indices imply (19, 19)
>>> 
>>> lagM
array([[ 0.        ,  0.        ,  0.49934524,  0.5       ,  0.5       ,
         0.5       ,  0.49819163,  0.        ,  0.        ,  0.        ,
         0.21585134,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        , -0.49485549, -0.49929848, -0.49984411,
        -0.49917376, -0.49967262,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [-0.49934524,  0.49485549,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        , -0.49265738,  0.49872167, -0.48087176,
        -0.48087176,  0.5       ,  0.49967262, -0.49643002,  0.5       ,
        -0.5       , -0.5       ,  0.5       ,  0.5       , -0.49981293,
        -0.5       , -0.5       ],
       [-0.5       ,  0.49766158,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        , -0.5       ,  0.5       , -0.5       ,
        -0.49300034,  0.5       , -0.5       , -0.5       ,  0.5       ,
        -0.5       , -0.5       ,  0.5       ,  0.5       , -0.5       ,
        -0.5       , -0.5       ],
       [-0.5       ,  0.49858136,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        , -0.5       ,  0.5       , -0.5       ,
        -0.49770835,  0.5       , -0.5       , -0.5       ,  0.5       ,
        -0.5       , -0.5       ,  0.5       ,  0.5       , -0.5       ,
        -0.5       , -0.5       ],
       [-0.5       ,  0.49717831,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        , -0.5       ,  0.5       , -0.5       ,
        -0.49805132,  0.5       , -0.5       , -0.5       ,  0.5       ,
        -0.5       , -0.5       ,  0.5       ,  0.5       , -0.5       ,
        -0.5       , -0.5       ],
       [-0.49819163,  0.49967262,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        , -0.49920494,  0.49694447, -0.5       ,
        -0.5       ,  0.48896268,  0.49878402, -0.49643002, -0.5       ,
        -0.5       , -0.49984411, -0.5       , -0.5       , -0.49533876,
        -0.49984411, -0.49532317],
       [ 0.        ,  0.        ,  0.49265738,  0.5       ,  0.5       ,
         0.5       ,  0.49920494,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        , -0.49872167, -0.5       , -0.5       ,
        -0.5       , -0.49694447,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.48087176,  0.5       ,  0.5       ,
         0.5       ,  0.5       ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        -0.19326225, -0.15272971, -0.15287002,  0.        ,  0.        ,
         0.        ,  0.        ],
       [-0.21585134,  0.        ,  0.48087176,  0.49194026,  0.4815577 ,
         0.48444174,  0.5       ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        -0.19323107, -0.15272971, -0.15287002,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        , -0.5       , -0.5       , -0.5       ,
        -0.5       , -0.48896268,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        , -0.49967262,  0.5       ,  0.5       ,
         0.5       , -0.49878402,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.49643002,  0.5       ,  0.5       ,
         0.5       ,  0.49643002,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        , -0.5       , -0.5       , -0.5       ,
        -0.5       ,  0.5       ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.5       ,  0.5       ,  0.5       ,
         0.5       ,  0.5       ,  0.        ,  0.        ,  0.19326225,
         0.19323107,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.5       ,  0.5       ,  0.5       ,
         0.5       ,  0.49984411,  0.        ,  0.        ,  0.15272971,
         0.15272971,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        , -0.5       , -0.5       , -0.5       ,
        -0.5       ,  0.5       ,  0.        ,  0.        ,  0.15287002,
         0.15287002,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        , -0.5       , -0.5       , -0.5       ,
        -0.5       ,  0.5       ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.49981293,  0.5       ,  0.5       ,
         0.5       ,  0.49533876,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.5       ,  0.5       ,  0.5       ,
         0.5       ,  0.49984411,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.5       ,  0.5       ,  0.5       ,
         0.5       ,  0.49532317,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ]])
>>> 
>>> 
>>> lagM
array([[ 0.00000000e+00,  2.04221619e-03,  4.99345244e-01,
         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,
         4.98191625e-01, -2.96199295e-04,  1.91282387e-02,
        -3.55750943e-02,  2.15851339e-01, -1.17232563e-02,
        -1.93309014e-02,  1.56362049e-02,  2.18252112e-04,
         2.63149690e-02,  4.67683098e-05,  8.12209647e-03,
         1.75537056e-02, -2.80921647e-02,  2.32282605e-03,
        -6.69722196e-02],
       [-2.04221619e-03,  0.00000000e+00, -4.94855486e-01,
        -4.99298475e-01, -4.99844106e-01, -4.99173760e-01,
        -4.99672622e-01,  1.98921211e-02,  1.97050479e-02,
         0.00000000e+00,  0.00000000e+00, -1.75225267e-02,
         8.73008449e-04,  2.82168802e-03, -3.25819225e-03,
        -6.50079506e-03, -5.53424999e-03, -4.52093661e-03,
        -4.91067253e-03, -1.90658810e-02,  5.65896548e-03,
        -1.90191126e-02],
       [-4.99345244e-01,  4.94855486e-01,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         2.96199295e-04, -4.92657375e-01,  4.98721666e-01,
        -4.80871761e-01, -4.80871761e-01,  5.00000000e-01,
         4.99672622e-01, -4.96430019e-01,  5.00000000e-01,
        -5.00000000e-01, -5.00000000e-01,  5.00000000e-01,
         5.00000000e-01, -4.99812927e-01, -5.00000000e-01,
        -5.00000000e-01],
       [-5.00000000e-01,  4.97661585e-01,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00, -5.00000000e-01,  5.00000000e-01,
        -5.00000000e-01, -4.93000343e-01,  5.00000000e-01,
        -5.00000000e-01, -5.00000000e-01,  5.00000000e-01,
        -5.00000000e-01, -5.00000000e-01,  5.00000000e-01,
         5.00000000e-01, -5.00000000e-01, -5.00000000e-01,
        -5.00000000e-01],
       [-5.00000000e-01,  4.98581361e-01,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00, -5.00000000e-01,  5.00000000e-01,
        -5.00000000e-01, -4.97708353e-01,  5.00000000e-01,
        -5.00000000e-01, -5.00000000e-01,  5.00000000e-01,
        -5.00000000e-01, -5.00000000e-01,  5.00000000e-01,
         5.00000000e-01, -5.00000000e-01, -5.00000000e-01,
        -5.00000000e-01],
       [-5.00000000e-01,  4.97178312e-01,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00, -5.00000000e-01,  5.00000000e-01,
        -5.00000000e-01, -4.98051320e-01,  5.00000000e-01,
        -5.00000000e-01, -5.00000000e-01,  5.00000000e-01,
        -5.00000000e-01, -5.00000000e-01,  5.00000000e-01,
         5.00000000e-01, -5.00000000e-01, -5.00000000e-01,
        -5.00000000e-01],
       [-4.98191625e-01,  4.99672622e-01, -2.96199295e-04,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00, -4.99204939e-01,  4.96944470e-01,
        -5.00000000e-01, -5.00000000e-01,  4.88962679e-01,
         4.98784024e-01, -4.96430019e-01, -5.00000000e-01,
        -5.00000000e-01, -4.99844106e-01, -5.00000000e-01,
        -5.00000000e-01, -4.95338758e-01, -4.99844106e-01,
        -4.95323169e-01],
       [ 2.96199295e-04, -1.98921211e-02,  4.92657375e-01,
         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,
         4.99204939e-01,  0.00000000e+00,  7.01524647e-03,
         3.93477380e-02,  3.93321485e-02,  1.93153119e-02,
        -1.63065507e-02, -1.49190908e-02,  3.42967605e-04,
         3.11788732e-05,  7.48292957e-04,  7.63882393e-04,
         1.09126056e-04, -4.05325352e-03,  1.90191126e-02,
        -3.28937112e-03],
       [-1.91282387e-02, -1.97050479e-02, -4.98721666e-01,
        -5.00000000e-01, -5.00000000e-01, -5.00000000e-01,
        -4.96944470e-01, -7.01524647e-03,  0.00000000e+00,
         1.65248028e-03,  2.57225704e-03,  1.87073239e-04,
         4.48975774e-03, -8.99510492e-03,  3.11788732e-05,
         8.79244224e-03, -9.69662956e-03,  0.00000000e+00,
        -1.96426901e-03,  1.09126056e-04,  0.00000000e+00,
         2.02662676e-04],
       [ 3.55750943e-02,  0.00000000e+00,  4.80871761e-01,
         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,
         5.00000000e-01, -3.93477380e-02, -1.65248028e-03,
         0.00000000e+00,  0.00000000e+00,  5.95516478e-03,
        -6.36828485e-02, -7.53905154e-02, -5.61219718e-04,
        -1.93262246e-01, -1.52729710e-01, -1.52870015e-01,
        -8.75346865e-02, -5.46565647e-02, -3.74146478e-04,
        -5.46565647e-02],
       [-2.15851339e-01,  0.00000000e+00,  4.80871761e-01,
         4.91940261e-01,  4.81557697e-01,  4.84441742e-01,
         5.00000000e-01, -3.93321485e-02, -2.57225704e-03,
         0.00000000e+00,  0.00000000e+00,  5.97075422e-03,
        -6.36672591e-02, -7.53749260e-02, -8.61628161e-02,
        -1.93231067e-01, -1.52729710e-01, -1.52870015e-01,
        -8.73320238e-02, -5.46565647e-02, -3.74146478e-04,
        -5.46409753e-02],
       [ 1.17232563e-02,  1.75225267e-02, -5.00000000e-01,
        -5.00000000e-01, -5.00000000e-01, -5.00000000e-01,
        -4.88962679e-01, -1.93153119e-02, -1.87073239e-04,
        -5.95516478e-03, -5.97075422e-03,  0.00000000e+00,
        -7.57646619e-03, -7.62323450e-03,  0.00000000e+00,
         1.10685000e-03,  1.38745986e-03,  0.00000000e+00,
        -1.24247810e-02, -6.23577464e-05,  0.00000000e+00,
        -1.55894366e-05],
       [ 1.93309014e-02, -8.73008449e-04, -4.99672622e-01,
         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,
        -4.98784024e-01,  1.63065507e-02, -4.48975774e-03,
         6.36828485e-02,  6.36672591e-02,  7.57646619e-03,
         0.00000000e+00,  0.00000000e+00, -1.09126056e-04,
         1.24871387e-02,  1.67742338e-02,  2.18252112e-04,
        -1.55894366e-05,  9.64986125e-03, -3.41408661e-03,
         2.82168802e-03],
       [-1.56362049e-02, -2.82168802e-03,  4.96430019e-01,
         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,
         4.96430019e-01,  1.49190908e-02,  8.99510492e-03,
         7.53905154e-02,  7.53749260e-02,  7.62323450e-03,
         0.00000000e+00,  0.00000000e+00, -1.40304929e-04,
         1.24871387e-02,  1.67430549e-02,  1.71483803e-04,
        -3.11788732e-05,  9.60309294e-03, -3.41408661e-03,
         2.23396626e-02],
       [-2.18252112e-04,  3.25819225e-03, -5.00000000e-01,
        -5.00000000e-01, -5.00000000e-01, -5.00000000e-01,
         5.00000000e-01, -3.42967605e-04, -3.11788732e-05,
         5.61219718e-04,  8.61628161e-02,  0.00000000e+00,
         1.09126056e-04,  1.40304929e-04,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  1.55894366e-05, -3.11788732e-05,
         1.55894366e-05],
       [-2.63149690e-02,  6.50079506e-03,  5.00000000e-01,
         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,
         5.00000000e-01, -3.11788732e-05, -8.79244224e-03,
         1.93262246e-01,  1.93231067e-01, -1.10685000e-03,
        -1.24871387e-02, -1.24871387e-02,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
        -5.50307112e-03,  6.31372182e-03, -4.00648521e-03,
        -7.99738097e-03],
       [-4.67683098e-05,  5.53424999e-03,  5.00000000e-01,
         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,
         4.99844106e-01, -7.48292957e-04,  9.69662956e-03,
         1.52729710e-01,  1.52729710e-01, -1.38745986e-03,
        -1.67742338e-02, -1.67430549e-02,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
        -6.17341689e-03, -2.99317183e-03, -5.14451408e-04,
        -2.96199295e-03],
       [-8.12209647e-03,  4.52093661e-03, -5.00000000e-01,
        -5.00000000e-01, -5.00000000e-01, -5.00000000e-01,
         5.00000000e-01, -7.63882393e-04,  0.00000000e+00,
         1.52870015e-01,  1.52870015e-01,  0.00000000e+00,
        -2.18252112e-04, -1.71483803e-04,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  4.25591619e-03,  0.00000000e+00,
        -2.94640352e-03],
       [-1.75537056e-02,  4.91067253e-03, -5.00000000e-01,
        -5.00000000e-01, -5.00000000e-01, -5.00000000e-01,
         5.00000000e-01, -1.09126056e-04,  1.96426901e-03,
         8.75346865e-02,  8.73320238e-02,  1.24247810e-02,
         1.55894366e-05,  3.11788732e-05,  0.00000000e+00,
         5.50307112e-03,  6.17341689e-03,  0.00000000e+00,
         0.00000000e+00,  2.74374084e-03,  0.00000000e+00,
         2.74374084e-03],
       [ 2.80921647e-02,  1.90658810e-02,  4.99812927e-01,
         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,
         4.95338758e-01,  4.05325352e-03, -1.09126056e-04,
         5.46565647e-02,  5.46565647e-02,  6.23577464e-05,
        -9.64986125e-03, -9.60309294e-03, -1.55894366e-05,
        -6.31372182e-03,  2.99317183e-03, -4.25591619e-03,
        -2.74374084e-03,  0.00000000e+00, -1.09126056e-04,
         0.00000000e+00],
       [-2.32282605e-03, -5.65896548e-03,  5.00000000e-01,
         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,
         4.99844106e-01, -1.90191126e-02,  0.00000000e+00,
         3.74146478e-04,  3.74146478e-04,  0.00000000e+00,
         3.41408661e-03,  3.41408661e-03,  3.11788732e-05,
         4.00648521e-03,  5.14451408e-04,  0.00000000e+00,
         0.00000000e+00,  1.09126056e-04,  0.00000000e+00,
         0.00000000e+00],
       [ 6.69722196e-02,  1.90191126e-02,  5.00000000e-01,
         5.00000000e-01,  5.00000000e-01,  5.00000000e-01,
         4.95323169e-01,  3.28937112e-03, -2.02662676e-04,
         5.46565647e-02,  5.46409753e-02,  1.55894366e-05,
        -2.82168802e-03, -2.23396626e-02, -1.55894366e-05,
         7.99738097e-03,  2.96199295e-03,  2.94640352e-03,
        -2.74374084e-03,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00]])
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py", line 1654, in create_block_manager_from_blocks
    make_block(values=blocks[0], placement=slice(0, len(axes[0])))
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/blocks.py", line 3053, in make_block
    return klass(values, ndim=ndim, placement=placement)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/blocks.py", line 124, in __init__
    raise ValueError(
ValueError: Wrong number of items passed 22, placement implies 19

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 113, in <module>
    plt.yticks(rotation=15)
  File "/home/sabeiro/lav//src/lernia/lernia/train_viz.py", line 389, in plotHeatmap
    X = pd.DataFrame(X,columns=labV,index=labV)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 464, in __init__
    mgr = init_ndarray(data, index, columns, dtype=dtype, copy=copy)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/construction.py", line 210, in init_ndarray
    return create_block_manager_from_blocks(block_values, [columns, index])
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py", line 1664, in create_block_manager_from_blocks
    construction_error(tot_items, blocks[0].shape[1:], axes, e)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py", line 1694, in construction_error
    raise ValueError(f"Shape of passed values is {passed}, indices imply {implied}")
ValueError: Shape of passed values is (22, 22), indices imply (19, 19)
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 110, in <module>
    importlib.reload(t_v)
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 175, in lagMatrix
    return lagM
NameError: name 'lagM' is not defined
>>> X1
                     object_distance  brake_pressure  ...  joystick_latency  e2e_latency_ms
timebucket                                            ...                                  
2020-08-03 14:53:06             24.5             0.0  ...              40.0           228.0
2020-08-03 14:53:06             24.5             0.0  ...              40.0           228.0
2020-08-03 14:53:06             24.5             0.0  ...              40.0           228.0
2020-08-03 14:53:06             24.5             0.0  ...              40.0           228.0
2020-08-03 14:53:06             24.5             0.0  ...              40.0           228.0
...                              ...             ...  ...               ...             ...
2020-07-21 10:57:12              3.2             0.0  ...              49.0           194.0
2020-07-21 10:57:12              3.2             0.0  ...              49.0           212.0
2020-07-21 10:57:12              3.2             0.0  ...              29.0           176.0
2020-07-21 10:57:12              3.2             0.0  ...              29.0           182.0
2020-07-21 10:57:12              3.2             0.0  ...              31.0           195.0

[64147 rows x 22 columns]
>>> ---------------------------------phase-shift-----------------------------
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/albio/albio/series_stat.py", line 151, in <module>
    w, h = sp.signal.freqs(y1, y2)
  File "/usr/local/lib/python3.8/dist-packages/scipy/signal/filter_design.py", line 184, in freqs
    w = findfreqs(b, a, worN)
  File "/usr/local/lib/python3.8/dist-packages/scipy/signal/filter_design.py", line 95, in findfreqs
    ep = atleast_1d(roots(den)) + 0j
  File "<__array_function__ internals>", line 5, in roots
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/polynomial.py", line 241, in roots
    A = diag(NX.ones((N-2,), p.dtype), -1)
  File "<__array_function__ internals>", line 5, in diag
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/twodim_base.py", line 273, in diag
    res = zeros((n, n), v.dtype)
MemoryError: Unable to allocate 30.7 GiB for an array with shape (64146, 64146) and data type float64
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/albio/albio/series_stat.py", line 151, in <module>
    w, h = sp.signal.freqs(y1, y2)
  File "/usr/local/lib/python3.8/dist-packages/scipy/signal/filter_design.py", line 184, in freqs
    w = findfreqs(b, a, worN)
  File "/usr/local/lib/python3.8/dist-packages/scipy/signal/filter_design.py", line 95, in findfreqs
    ep = atleast_1d(roots(den)) + 0j
  File "<__array_function__ internals>", line 5, in roots
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/polynomial.py", line 241, in roots
    A = diag(NX.ones((N-2,), p.dtype), -1)
  File "<__array_function__ internals>", line 5, in diag
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/twodim_base.py", line 273, in diag
    res = zeros((n, n), v.dtype)
MemoryError: Unable to allocate 30.7 GiB for an array with shape (64146, 64146) and data type float64
>>> w
  C-c C-cTraceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/albio/albio/series_stat.py", line 154, in <module>
    w, H = signal.freqz(y1, y2, 50000)
  File "/usr/local/lib/python3.8/dist-packages/scipy/signal/filter_design.py", line 469, in freqz
    h = (npp_polyval(zm1, b, tensor=False) /
  File "/usr/local/lib/python3.8/dist-packages/numpy/polynomial/polynomial.py", line 749, in polyval
    c0 = c[-i] + c0*x
KeyboardInterrupt
>>> 
>>> 
>>> w
array([0.        , 0.06283185, 0.12566371, 0.18849556, 0.25132741,
       0.31415927, 0.37699112, 0.43982297, 0.50265482, 0.56548668,
       0.62831853, 0.69115038, 0.75398224, 0.81681409, 0.87964594,
       0.9424778 , 1.00530965, 1.0681415 , 1.13097336, 1.19380521,
       1.25663706, 1.31946891, 1.38230077, 1.44513262, 1.50796447,
       1.57079633, 1.63362818, 1.69646003, 1.75929189, 1.82212374,
       1.88495559, 1.94778745, 2.0106193 , 2.07345115, 2.136283  ,
       2.19911486, 2.26194671, 2.32477856, 2.38761042, 2.45044227,
       2.51327412, 2.57610598, 2.63893783, 2.70176968, 2.76460154,
       2.82743339, 2.89026524, 2.95309709, 3.01592895, 3.0787608 ])
>>> h
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'h' is not defined
>>> H
array([-1.56898463e-02+0.00000000e+00j,  3.19490770e-02-1.95815420e-03j,
       -1.82885028e-02+3.53069409e-02j,  4.26418411e-02+3.61716840e-02j,
       -4.35168203e-02-6.01609475e-02j,  1.15804653e-02+2.09452374e-02j,
        1.00677405e-02+1.79866557e-03j,  3.71843408e-03-3.08434948e-02j,
       -2.77032407e-02+1.07200975e-01j, -1.76017882e-02+1.77903181e-01j,
       -2.48653527e-02-6.11487912e-02j, -7.95884058e-02+2.29549091e-02j,
       -1.71002152e-01-4.39134920e-03j, -3.26808450e-02-1.00607053e-02j,
       -1.06827752e-01+9.56729293e-02j,  2.08729051e-01+4.43213599e-01j,
       -2.38518874e-02-3.89772213e-02j, -1.88098677e-01+5.17733371e-02j,
       -7.38906205e-02+2.50707599e-02j,  1.64172934e-01-1.21404935e+00j,
        1.17842538e-01+3.27010833e-02j,  9.32509170e-01+8.14771616e-01j,
       -7.24520078e-01+1.73195807e-01j,  7.13208604e-01+1.50088834e+00j,
       -4.07297561e-02+3.15643554e-02j, -1.11369405e-01-4.45855844e-02j,
        1.29589356e-01-7.61108642e-02j, -2.22036489e-02-3.51813185e-02j,
        1.74382414e-02+4.63170254e-02j, -8.60894031e-03-1.95199869e-01j,
        3.18427746e-01+8.15241013e-02j,  2.08873739e-02-6.58422332e-02j,
        7.61459267e-01-3.16748545e-01j,  4.06123935e-01-2.57255475e-01j,
        3.45823476e-01+1.77004565e-01j,  4.82166243e-02-2.80348628e-01j,
        3.26257019e-01-4.66193562e-01j, -8.85472021e-02+1.56779185e-01j,
       -3.95078031e-01-9.45685711e-03j,  1.53512095e-02-1.30641483e-01j,
       -3.10321094e-01-1.65921727e-01j,  4.36860333e-02+1.32806640e-01j,
        3.73075607e-01+3.88445324e-02j,  3.37402843e-01-5.32897019e-01j,
       -1.62664269e-01+4.19425713e-02j,  2.26933228e-02+5.06781588e-02j,
        8.62907888e-04-7.37432252e-02j,  8.55955216e-01-4.67433166e-01j,
        2.31572311e-02-8.55760672e-02j, -2.07462848e+00-5.94197195e+00j])
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/albio/albio/series_stat.py", line 153, in <module>
    group_delay = -diff(np.unwrap(np.angle(h))) / np.diff(w)
NameError: name 'diff' is not defined
>>> group_delay
array([-49.02576039, -33.58077472,  21.41018628,  46.16293789,
        48.07171037,  14.1479823 ,  25.90419161,  47.88464575,
         2.45530595, -42.28362743,  23.32226715,  -4.87768616,
        -4.34438796,  16.3771838 ,  20.38078045, -48.26486896,
        20.53472906,   0.93125351, -32.34530528, -27.16888399,
        -7.12105938, -34.83631932,  28.32567874, -21.56722797,
       -16.55355744, -35.48752887,  25.50837402,  46.77065745,
        44.97057329, -29.69048388,  24.0999084 , -13.83697118,
         2.71269867, -16.51577637,  29.81839949,  -7.00745234,
       -48.46437459, -17.19830936, -26.48074394,  19.04707525,
        37.87243337,  18.29095928,  17.66786845,  37.99958633,
        27.68440688,  43.11307879, -16.85856503,  12.83872852,
         9.55232834])
>>> w
array([0.        , 0.06283185, 0.12566371, 0.18849556, 0.25132741,
       0.31415927, 0.37699112, 0.43982297, 0.50265482, 0.56548668,
       0.62831853, 0.69115038, 0.75398224, 0.81681409, 0.87964594,
       0.9424778 , 1.00530965, 1.0681415 , 1.13097336, 1.19380521,
       1.25663706, 1.31946891, 1.38230077, 1.44513262, 1.50796447,
       1.57079633, 1.63362818, 1.69646003, 1.75929189, 1.82212374,
       1.88495559, 1.94778745, 2.0106193 , 2.07345115, 2.136283  ,
       2.19911486, 2.26194671, 2.32477856, 2.38761042, 2.45044227,
       2.51327412, 2.57610598, 2.63893783, 2.70176968, 2.76460154,
       2.82743339, 2.89026524, 2.95309709, 3.01592895, 3.0787608 ])
>>> h
array([-1.56898463e-02+0.00000000e+00j,  3.19490770e-02-1.95815420e-03j,
       -1.82885028e-02+3.53069409e-02j,  4.26418411e-02+3.61716840e-02j,
       -4.35168203e-02-6.01609475e-02j,  1.15804653e-02+2.09452374e-02j,
        1.00677405e-02+1.79866557e-03j,  3.71843408e-03-3.08434948e-02j,
       -2.77032407e-02+1.07200975e-01j, -1.76017882e-02+1.77903181e-01j,
       -2.48653527e-02-6.11487912e-02j, -7.95884058e-02+2.29549091e-02j,
       -1.71002152e-01-4.39134920e-03j, -3.26808450e-02-1.00607053e-02j,
       -1.06827752e-01+9.56729293e-02j,  2.08729051e-01+4.43213599e-01j,
       -2.38518874e-02-3.89772213e-02j, -1.88098677e-01+5.17733371e-02j,
       -7.38906205e-02+2.50707599e-02j,  1.64172934e-01-1.21404935e+00j,
        1.17842538e-01+3.27010833e-02j,  9.32509170e-01+8.14771616e-01j,
       -7.24520078e-01+1.73195807e-01j,  7.13208604e-01+1.50088834e+00j,
       -4.07297561e-02+3.15643554e-02j, -1.11369405e-01-4.45855844e-02j,
        1.29589356e-01-7.61108642e-02j, -2.22036489e-02-3.51813185e-02j,
        1.74382414e-02+4.63170254e-02j, -8.60894031e-03-1.95199869e-01j,
        3.18427746e-01+8.15241013e-02j,  2.08873739e-02-6.58422332e-02j,
        7.61459267e-01-3.16748545e-01j,  4.06123935e-01-2.57255475e-01j,
        3.45823476e-01+1.77004565e-01j,  4.82166243e-02-2.80348628e-01j,
        3.26257019e-01-4.66193562e-01j, -8.85472021e-02+1.56779185e-01j,
       -3.95078031e-01-9.45685711e-03j,  1.53512095e-02-1.30641483e-01j,
       -3.10321094e-01-1.65921727e-01j,  4.36860333e-02+1.32806640e-01j,
        3.73075607e-01+3.88445324e-02j,  3.37402843e-01-5.32897019e-01j,
       -1.62664269e-01+4.19425713e-02j,  2.26933228e-02+5.06781588e-02j,
        8.62907888e-04-7.37432252e-02j,  8.55955216e-01-4.67433166e-01j,
        2.31572311e-02-8.55760672e-02j, -2.07462848e+00-5.94197195e+00j])
>>> gd
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'gd' is not defined
>>> /usr/local/lib/python3.8/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part
  return array(a, dtype, copy=False, order=order)
>>> h
array([-1.56898463e-02+0.00000000e+00j,  3.19490770e-02-1.95815420e-03j,
       -1.82885028e-02+3.53069409e-02j,  4.26418411e-02+3.61716840e-02j,
       -4.35168203e-02-6.01609475e-02j,  1.15804653e-02+2.09452374e-02j,
        1.00677405e-02+1.79866557e-03j,  3.71843408e-03-3.08434948e-02j,
       -2.77032407e-02+1.07200975e-01j, -1.76017882e-02+1.77903181e-01j,
       -2.48653527e-02-6.11487912e-02j, -7.95884058e-02+2.29549091e-02j,
       -1.71002152e-01-4.39134920e-03j, -3.26808450e-02-1.00607053e-02j,
       -1.06827752e-01+9.56729293e-02j,  2.08729051e-01+4.43213599e-01j,
       -2.38518874e-02-3.89772213e-02j, -1.88098677e-01+5.17733371e-02j,
       -7.38906205e-02+2.50707599e-02j,  1.64172934e-01-1.21404935e+00j,
        1.17842538e-01+3.27010833e-02j,  9.32509170e-01+8.14771616e-01j,
       -7.24520078e-01+1.73195807e-01j,  7.13208604e-01+1.50088834e+00j,
       -4.07297561e-02+3.15643554e-02j, -1.11369405e-01-4.45855844e-02j,
        1.29589356e-01-7.61108642e-02j, -2.22036489e-02-3.51813185e-02j,
        1.74382414e-02+4.63170254e-02j, -8.60894031e-03-1.95199869e-01j,
        3.18427746e-01+8.15241013e-02j,  2.08873739e-02-6.58422332e-02j,
        7.61459267e-01-3.16748545e-01j,  4.06123935e-01-2.57255475e-01j,
        3.45823476e-01+1.77004565e-01j,  4.82166243e-02-2.80348628e-01j,
        3.26257019e-01-4.66193562e-01j, -8.85472021e-02+1.56779185e-01j,
       -3.95078031e-01-9.45685711e-03j,  1.53512095e-02-1.30641483e-01j,
       -3.10321094e-01-1.65921727e-01j,  4.36860333e-02+1.32806640e-01j,
        3.73075607e-01+3.88445324e-02j,  3.37402843e-01-5.32897019e-01j,
       -1.62664269e-01+4.19425713e-02j,  2.26933228e-02+5.06781588e-02j,
        8.62907888e-04-7.37432252e-02j,  8.55955216e-01-4.67433166e-01j,
        2.31572311e-02-8.55760672e-02j, -2.07462848e+00-5.94197195e+00j])
>>> h*h
array([ 2.46171277e-04-0.00000000e+00j,  1.01690915e-03-1.25122438e-04j,
       -9.12110739e-04-1.29142217e-03j,  5.09935892e-04+3.08485440e-03j,
       -1.72562596e-03+5.23602628e-03j, -3.04595793e-04+4.85111187e-04j,
        9.81242011e-05+3.62169963e-05j, -9.37494417e-04-2.29379004e-04j,
       -1.07245794e-02-5.93962881e-03j, -3.13397188e-02-6.26282822e-03j,
       -3.12088890e-03+3.04097252e-03j,  5.80738649e-03-3.65388924e-03j,
        2.92224520e-02+1.50186033e-03j,  9.66819841e-04+6.57584704e-04j,
        2.25885915e-03-2.04410479e-02j, -1.52870478e-01+1.85023108e-01j,
       -9.50311248e-04+1.85936059e-03j,  3.27006339e-02-1.94769925e-02j,
        4.83128080e-03-3.70498802e-03j, -1.44696308e+00-3.98628089e-01j,
        1.28175030e-02+7.70715732e-03j,  2.05720566e-01+1.51956401e+00j,
        4.94932556e-01-2.50967680e-01j, -1.74399930e+00+2.14089296e+00j,
        6.62604499e-04-2.57121700e-03j,  1.04152699e-02+9.93093998e-03j,
        1.10005375e-02-1.97263157e-02j, -7.44723146e-04+1.56230728e-03j,
       -1.84117458e-03+1.61537494e-03j, -3.80288749e-02+3.36092804e-03j,
        9.47500505e-02+5.19190717e-02j, -3.89891729e-03-2.75054269e-03j,
        4.79490575e-01-4.82382230e-01j,  9.87562711e-02-2.08955212e-01j,
        8.82632605e-02+1.22424668e-01j, -7.62705101e-02-2.70349289e-02j,
       -1.10892795e-01-3.04197844e-01j, -1.67391059e-02-2.77647164e-02j,
        1.55997219e-01+7.47239298e-03j, -1.68315374e-02-4.01100956e-03j,
        6.87691615e-02+1.02978024e-01j, -1.57291340e-02+1.16035906e-02j,
        1.37676511e-01+2.89838950e-02j, -1.70138555e-01-3.59601938e-01j,
        2.47004853e-02-1.36451154e-02j, -2.05328889e-03+2.30011163e-03j,
       -5.43731866e-03-1.27267222e-04j,  5.14165567e-01-8.00203712e-01j,
       -6.78700593e-03-3.96340953e-03j, -3.10029473e+01+2.46547685e+01j])
>>> h*abs(h)
array([-2.46171277e-04+0.00000000e+00j,  1.02265891e-03-6.26786129e-05j,
       -7.27194981e-04+1.40388913e-03j,  2.38440629e-03+2.02261414e-03j,
       -3.23112119e-03-4.46694661e-03j,  2.77160514e-04+5.01291842e-04j,
        1.02964292e-04+1.83952225e-05j,  1.15519959e-04-9.58209606e-04j,
       -3.06737789e-03+1.18695825e-02j, -3.14670377e-03+3.18040760e-02j,
       -1.64138836e-03-4.03649670e-03j, -6.59251581e-03+1.90141516e-03j,
       -2.92513764e-02-7.51177729e-04j, -1.11750114e-03-3.44019552e-04j,
       -1.53198140e-02+1.37201378e-02j,  1.02257236e-01+2.17132198e-01j,
       -1.08993896e-03-1.78110818e-03j, -3.66968857e-02+1.01006571e-02j,
       -5.76553640e-03+1.95622094e-03j,  2.01128166e-01-1.48733116e+00j,
        1.44116292e-02+3.99920006e-03j,  1.15474088e+00+1.00894460e+00j,
       -5.39719381e-01+1.29019383e-01j,  1.18515707e+00+2.49406474e+00j,
       -2.09875717e-03+1.62647469e-03j, -1.33601602e-02-5.34860137e-03j,
        1.94756311e-02-1.14384943e-02j, -9.23716411e-04-1.46361354e-03j,
        8.63036008e-04+2.29227591e-03j, -1.68209755e-03-3.81400277e-02j,
        1.04666580e-01+2.67968132e-02j,  1.44281447e-03-4.54811252e-03j,
        6.27984583e-01-2.61226321e-01j,  1.95242590e-01-1.23674625e-01j,
        1.34348968e-01+6.87645065e-02j,  1.37159301e-02-7.97493032e-02j,
        1.85645714e-01-2.65271953e-01j, -1.59434936e-02+2.82291012e-02j,
       -1.56131361e-01-3.73726669e-03j,  2.01930307e-03-1.71846229e-02j,
       -1.09200049e-01-5.83868164e-02j,  6.10762510e-03+1.85673338e-02j,
        1.39937824e-01+1.45702888e-02j,  2.12809982e-01-3.36113958e-01j,
       -2.73251009e-02+7.04570830e-03j,  1.26009519e-03+2.81401295e-03j,
        6.36379671e-05-5.43843556e-03j,  8.34788111e-01-4.55873908e-01j,
        2.05297972e-03-7.58665531e-03j, -1.30571642e+01-3.73972033e+01j])
>>> np.conjugate(h)
array([-1.56898463e-02-0.00000000e+00j,  3.19490770e-02+1.95815420e-03j,
       -1.82885028e-02-3.53069409e-02j,  4.26418411e-02-3.61716840e-02j,
       -4.35168203e-02+6.01609475e-02j,  1.15804653e-02-2.09452374e-02j,
        1.00677405e-02-1.79866557e-03j,  3.71843408e-03+3.08434948e-02j,
       -2.77032407e-02-1.07200975e-01j, -1.76017882e-02-1.77903181e-01j,
       -2.48653527e-02+6.11487912e-02j, -7.95884058e-02-2.29549091e-02j,
       -1.71002152e-01+4.39134920e-03j, -3.26808450e-02+1.00607053e-02j,
       -1.06827752e-01-9.56729293e-02j,  2.08729051e-01-4.43213599e-01j,
       -2.38518874e-02+3.89772213e-02j, -1.88098677e-01-5.17733371e-02j,
       -7.38906205e-02-2.50707599e-02j,  1.64172934e-01+1.21404935e+00j,
        1.17842538e-01-3.27010833e-02j,  9.32509170e-01-8.14771616e-01j,
       -7.24520078e-01-1.73195807e-01j,  7.13208604e-01-1.50088834e+00j,
       -4.07297561e-02-3.15643554e-02j, -1.11369405e-01+4.45855844e-02j,
        1.29589356e-01+7.61108642e-02j, -2.22036489e-02+3.51813185e-02j,
        1.74382414e-02-4.63170254e-02j, -8.60894031e-03+1.95199869e-01j,
        3.18427746e-01-8.15241013e-02j,  2.08873739e-02+6.58422332e-02j,
        7.61459267e-01+3.16748545e-01j,  4.06123935e-01+2.57255475e-01j,
        3.45823476e-01-1.77004565e-01j,  4.82166243e-02+2.80348628e-01j,
        3.26257019e-01+4.66193562e-01j, -8.85472021e-02-1.56779185e-01j,
       -3.95078031e-01+9.45685711e-03j,  1.53512095e-02+1.30641483e-01j,
       -3.10321094e-01+1.65921727e-01j,  4.36860333e-02-1.32806640e-01j,
        3.73075607e-01-3.88445324e-02j,  3.37402843e-01+5.32897019e-01j,
       -1.62664269e-01-4.19425713e-02j,  2.26933228e-02-5.06781588e-02j,
        8.62907888e-04+7.37432252e-02j,  8.55955216e-01+4.67433166e-01j,
        2.31572311e-02+8.55760672e-02j, -2.07462848e+00+5.94197195e+00j])
>>> H
array([2.46171277e-04+0.j, 1.02457789e-03+0.j, 1.58104941e-03+0.j,
       3.12671734e-03+0.j, 5.51305325e-03+0.j, 5.72810144e-04+0.j,
       1.04594597e-04+0.j, 9.65147921e-04+0.j, 1.22595185e-02+0.j,
       3.19593647e-02+0.j, 4.35746043e-03+0.j, 6.86124219e-03+0.j,
       2.92610199e-02+0.j, 1.16925542e-03+0.j, 2.05654779e-02+0.j,
       2.40006111e-01+0.j, 2.08813632e-03+0.j, 3.80615908e-02+0.j,
       6.08836680e-03+0.j, 1.50086858e+00+0.j, 1.49562247e-02+0.j,
       1.53342614e+00+0.j, 5.54926131e-01+0.j, 2.76133233e+00+0.j,
       2.65522157e-03+0.j, 1.43910186e-02+0.j, 2.25862648e-02+0.j,
       1.73072719e-03+0.j, 2.44935910e-03+0.j, 3.81771026e-02+0.j,
       1.08042409e-01+0.j, 4.77148207e-03+0.j, 6.80149856e-01+0.j,
       2.31117030e-01+0.j, 1.50924493e-01+0.j, 8.09201959e-02+0.j,
       3.23780080e-01+0.j, 3.24203199e-02+0.j, 1.56176083e-01+0.j,
       1.73028567e-02+0.j, 1.23829201e-01+0.j, 1.95460730e-02+0.j,
       1.40694307e-01+0.j, 3.97819911e-01+0.j, 2.82188438e-02+0.j,
       3.08326268e-03+0.j, 5.43880788e-03+0.j, 9.51153096e-01+0.j,
       7.85952063e-03+0.j, 3.96111140e+01+0.j])
>>> H
array([2.46171277e-04, 1.02457789e-03, 1.58104941e-03, 3.12671734e-03,
       5.51305325e-03, 5.72810144e-04, 1.04594597e-04, 9.65147921e-04,
       1.22595185e-02, 3.19593647e-02, 4.35746043e-03, 6.86124219e-03,
       2.92610199e-02, 1.16925542e-03, 2.05654779e-02, 2.40006111e-01,
       2.08813632e-03, 3.80615908e-02, 6.08836680e-03, 1.50086858e+00,
       1.49562247e-02, 1.53342614e+00, 5.54926131e-01, 2.76133233e+00,
       2.65522157e-03, 1.43910186e-02, 2.25862648e-02, 1.73072719e-03,
       2.44935910e-03, 3.81771026e-02, 1.08042409e-01, 4.77148207e-03,
       6.80149856e-01, 2.31117030e-01, 1.50924493e-01, 8.09201959e-02,
       3.23780080e-01, 3.24203199e-02, 1.56176083e-01, 1.73028567e-02,
       1.23829201e-01, 1.95460730e-02, 1.40694307e-01, 3.97819911e-01,
       2.82188438e-02, 3.08326268e-03, 5.43880788e-03, 9.51153096e-01,
       7.85952063e-03, 3.96111140e+01])
>>> /usr/local/lib/python3.8/dist-packages/numpy/core/_asarray.py:83: ComplexWarning: Casting complex values to real discards the imaginary part
  return array(a, dtype, copy=False, order=order)
>>> manual_t_shift
0.005
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/albio/albio/series_stat.py", line 162, in <module>
    ab_corr = np.correlate(y1, y2, "full")
IndexError: index 128292 is out of bounds for axis 0 with size 20001
>>> manual_t_shift
0.005
>>> y1
array([-0.4612443, -0.4612443, -0.4612443, ..., -0.4612443, -0.4612443,
       -0.4612443])
>>> y2
array([24.677002 , 24.677002 , 24.677002 , ..., 25.9734096, 25.9460868,
       25.918764 ])
>>> ab_corr
array([-11.95488214, -23.92236677, -35.90245388, ..., -34.14637949,
       -22.76425299, -11.3821265 ])
>>> phase_shift
-1.2566370614347744
>>> dt
array([-64.146, -64.145, -64.144, ...,  64.144,  64.145,  64.146])
>>> t_shift_alt
64.146
>>> dt
array([-64.146, -64.145, -64.144, ...,  64.144,  64.145,  64.146])
>>> phase_shift
-1.2566370614347744
>>> t_shift
64.146
>>> b_shift
1.5707963267948966
>>> ab_corr
array([-11.95488214, -23.92236677, -35.90245388, ..., -34.14637949,
       -22.76425299, -11.3821265 ])
>>> phase_shift
-0.2513274122876692
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 110, in <module>
    lagM = s_s.lagMatrix(X1)
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 189, in lagMatrix
    lagM = cross_funcM(X,timeLag)
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 330, in cross_funcM
    score = func(x, y)
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 163, in timeLag
    manual_t_shift = (b_shift / (2.0 * np.pi)) / freq_Hz
NameError: name 'b_shift' is not defined
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/albio/albio/series_stat.py", line 159, in <module>
    s = numpy.sign(p_inst)
NameError: name 'numpy' is not defined
>>> s
array([-1., -1., -1., ..., -1., -1., -1.])
>>> p_inst
array([-11.3821265 , -11.3821265 , -11.3821265 , ..., -11.98008711,
       -11.96748463, -11.95488214])
>>> phase_shift
3.141592653589793
>>> s_avg
-1.0
>>> shift_fraction
0.0
>>> trapz
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'trapz' is not defined
>>> x_avg
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'x_avg' is not defined
>>> s_avg
-1.0
>>> t
array([0.0000e+00, 1.0000e-03, 2.0000e-03, ..., 6.4144e+01, 6.4145e+01,
       6.4146e+01])
>>> s_avg
-0.3091697066068016
>>> 1.7294774635396823
1.0455323787739668
1.0028745793589344
1.0028745793589344
1.0028745793589344
1.0855454925765868
1.8080589091899901
1.819543701340196
1.881742788891389
1.9060347159979978
1.6601279297030713
1.4598664580520144
1.4619724114100476
1.5914395672695754
1.8368810847993455
1.9761188847269455
1.6704617938552784
1.2043359546674726
2.0569776985434993
1.6199434010922416
2.072356055623084
0.9631063438537572
0.8944914449094866
0.8944914449094866
0.8944914449094866
0.9144735139810507
0.8323168451880377
1.8496637319260059
2.128335234419163
2.1497865732754025
1.7691967233853643
1.4607969955823101
1.4577115290344957
1.59912874580937
1.6028264081007872
1.8002472915014758
1.4967451296473315
1.2393535512022036
2.131175822669532
1.6414437156079704
2.163891563208274
3.0414374299345224
3.0414374299345224
3.0414374299345224
2.906558463701455
1.674600237082111
1.0281949953148128
0.6404057234798751
0.6207175083652445
1.1856517405723703
1.8992760749884976
1.8986393914151396
1.5107766560909546
1.3584868429095136
1.0851536873006717
1.5199106165856928
2.1984928666373444
0.4942623555643097
1.4934392726318175
0.4562572438007413
3.141592653589793
3.141592653589793
3.0067136873567257
1.695218989727035
0.9777990417005005
0.5675789178195387
0.5466173355581905
1.1428959898383606
1.93341210965242
1.931893864208257
1.509552264603728
1.3379660415835675
1.0407327641440332
1.512907097278744
2.242326081880113
0.41438305493753896
1.4813422847380018
0.3720680851389267
3.141592653589793
3.0067136873567257
1.695218989727035
0.9777990417005005
0.5675789178195387
0.5466173355581905
1.1428959898383606
1.93341210965242
1.931893864208257
1.509552264603728
1.3379660415835675
1.0407327641440332
1.512907097278744
2.242326081880113
0.41438305493753896
1.4813422847380018
0.3720680851389267
3.0067136873567257
1.695218989727035
0.9777990417005005
0.5675789178195387
0.5466173355581905
1.1428959898383606
1.93341210965242
1.931893864208257
1.509552264603728
1.3379660415835675
1.0407327641440332
1.512907097278744
2.242326081880113
0.41438305493753896
1.4813422847380018
0.3720680851389267
1.7629033511410137
1.0150205429122379
0.6138119403772773
0.5952991410903856
1.153768586244947
1.9150952130034846
1.9033900303855815
1.515527295061397
1.3806728166580897
1.0785419732696404
1.4922393689743292
2.185808170829657
0.503616706526734
1.4971124470934973
0.46443617893542566
1.5162619299537512
1.4003855196024646
1.3908842416615734
1.560437974812947
1.5651396381239024
1.5774325286556725
1.5657028582080166
1.7017817280985756
1.6006469912535215
1.7525205113293172
1.6699720372603875
1.3988182984988144
1.5993981119365432
1.3755058845819825
2.103014818463276
2.1175116136720598
2.2140426385251377
1.2822807167444337
1.2925166295776613
1.5919293238644683
1.5247102312156298
1.7528878287754839
1.6695802319844777
1.2130536220565469
2.2458278415335795
1.9708050256723662
2.2170301537539756
3.1124031605342712
1.9218538540129864
1.2834561325721658
1.3210204634003289
1.6089728533666845
1.5745674525755597
1.9711233674590531
1.6676212056049085
1.1240158731053023
2.421160702504673
1.6734248212543696
2.459459668225179
1.905202129786682
1.303144347686796
1.3417861430237208
1.6111277823842094
1.5556628480127552
1.9517290063013568
1.6481288931282343
1.1431163803060627
2.403235611131651
1.6717596488317372
2.4396735017915696
0.8848432399901301
0.886361485434292
1.5835055104323272
1.4613847034961782
1.604050799588018
1.4942963466728736
1.2588948393383685
2.092191197716181
1.802328757029756
2.045468418563554
3.0086237380768033
1.556960702989222
1.5470431319426696
1.5054627970363959
1.6747716518903222
1.819078432575042
1.2230201687625781
1.545990155263664
1.2484875116969234
1.5545608956742574
1.5401375639547001
1.505511772695887
1.6949006479403546
1.8276491729856421
1.1859455945293056
1.5568137760107608
1.2057317609629132
1.5694984718184382
1.5756204292545906
1.570869790284124
1.5463084970503347
1.5909498106746898
1.564184612763855
1.6044670926936906
2.1000273032344454
1.906916277868802
1.157613175514842
1.825028975202977
1.447059323095608
1.7905501109226278
1.8444723120201565
1.0790072420347896
2.1212827394527305
1.674649212741604
2.17123791213164
1.3993570307531904
1.722767798189675
1.6084830967717838
1.6931864998582375
0.8774724032370118
1.5200575435641606
0.8582739447172724
1.6747961397200568
2.9323686362522214
1.7167193042427555
>>> shift_fraction
0.34541514669659923
>>> 1.0851536873006769
>>> s_avg
-0.3091697066068016
>>> shift_fraction
0.34541514669659923
>>> 0.15868113674478607
-0.5252639480209298
-0.5679217474359622
-0.5679217474359622
-0.5679217474359622
-0.48525083421830983
0.23726258239509376
0.24874737454529963
0.31094646209649224
0.33523838920310134
0.08933160290817475
-0.1109298687428821
-0.10882391538484902
0.02064324047467884
0.26608475800444875
0.40532255793204885
0.09966546706038201
-0.366460372127424
0.4861813717486028
0.04914707429734496
0.5015597288281876
-0.6076899829411394
-0.67630488188541
-0.67630488188541
-0.67630488188541
-0.6563228128138459
-0.7384794816068588
0.27886740513110914
0.5575389076242668
0.578990246480506
0.19840039659046776
-0.1099993312125864
-0.11308479776040091
0.028332419014473347
0.032030081305890426
0.22945096470657927
-0.07405119714756495
-0.33144277559269303
0.5603794958746354
0.07064738881307406
0.5930952364133771
1.4706411031396258
1.4706411031396258
1.4706411031396258
1.3357621369065587
0.10380391028721452
-0.5426013314800836
-0.9303906033150214
-0.950078818429652
-0.3851445862225263
0.328479748193601
0.3278430646202429
-0.06001967070394204
-0.21230948388538295
-0.48564263949422476
-0.05088571020920385
0.6276965398424479
-1.0765339712305868
-0.07735705416307903
-1.1145390829941553
1.5707963267948966
1.5707963267948966
1.4359173605618292
0.12442266293213829
-0.592997285094396
-1.0032174089753578
-1.024178991236706
-0.427900336956536
0.3626157828575232
0.3610975374133605
-0.06124406219116848
-0.232830285211329
-0.5300635626508634
-0.057889229516152586
0.671529755085217
-1.1564132718573576
-0.08945404205689476
-1.19872824165597
1.5707963267948966
1.4359173605618292
0.12442266293213829
-0.592997285094396
-1.0032174089753578
-1.024178991236706
-0.427900336956536
0.3626157828575232
0.3610975374133605
-0.06124406219116848
-0.232830285211329
-0.5300635626508634
-0.057889229516152586
0.671529755085217
-1.1564132718573576
-0.08945404205689476
-1.19872824165597
1.4359173605618292
0.12442266293213829
-0.592997285094396
-1.0032174089753578
-1.024178991236706
-0.427900336956536
0.3626157828575232
0.3610975374133605
-0.06124406219116848
-0.232830285211329
-0.5300635626508634
-0.057889229516152586
0.671529755085217
-1.1564132718573576
-0.08945404205689476
-1.19872824165597
0.19210702434611707
-0.5557757838826586
-0.9569843864176193
-0.9754971857045109
-0.41702774054994945
0.34429888620858795
0.3325937035906852
-0.055269031733499466
-0.19012351013680678
-0.49225435352525604
-0.07855695782056732
0.6150118440347602
-1.0671796202681627
-0.07368387970139931
-1.106360147859471
-0.05453439684114543
-0.1704108071924319
-0.1799120851333231
-0.010358351981949525
-0.005656688670993961
0.006636201860775869
-0.005093468586880086
0.13098540130367906
0.029850664458624828
0.1817241845344207
0.09917571046549085
-0.17197802829608208
0.028601785141646525
-0.19529044221291397
0.5322184916683795
0.5467152868771631
0.6432463117302408
-0.2885156100504629
-0.2782796972172353
0.02113299706957168
-0.04608609557926685
0.18209150198058752
0.09878390518958115
-0.3577427047383498
0.675031514738683
0.4000086988774694
0.6462338269590793
1.5416068337393745
0.35105752721808997
-0.2873401942227308
-0.24977586339456762
0.03817652657178802
0.0037711257806631046
0.40032704066415664
0.09682487881001185
-0.4467804536895941
0.8503643757097766
0.10262849445947297
0.8886633414302825
0.3344058029917853
-0.2676519791081003
-0.22901018377117582
0.04033145558931283
-0.01513347878214117
0.38093267950646015
0.07733256633333788
-0.42767994648883395
0.8324392843367546
0.10096332203684048
0.8688771749966732
-0.6859530868047664
-0.6844348413606045
0.012709183637430729
-0.10941162329871838
0.03325447279312148
-0.0764999801220229
-0.31190148745652807
0.5213948709212843
0.23153243023485925
0.4746720917686574
1.4378274112819065
-0.013835623805674387
-0.02375319485222679
-0.06533352975850071
0.10397532509542556
0.24828210578014567
-0.3477761580323183
-0.024806171531232723
-0.3223088150979731
-0.016235431120639087
-0.03065876284019633
-0.06528455409900961
0.124104321145458
0.25685284619074544
-0.38485073226559097
-0.013982550784135753
-0.3650645658319834
-0.0012978549764584386
0.004824102459694042
7.346348922738833e-05
-0.02448782974456185
0.020153483879793047
-0.006611714031041695
0.03367076589879391
0.529230976439549
0.33611995107390535
-0.41318315128005456
0.2542326484080805
-0.12373700369928863
0.21975378412773117
0.27367598522525993
-0.4917890847601069
0.5504864126578342
0.10385288594670748
0.6004415853367435
-0.17143929604170627
0.15197147139477846
0.037686769976887115
0.1223901730633411
-0.6933239235578849
-0.0507387832307361
-0.7125223820776243
0.10399981292516026
1.3615723094573247
0.145922977447859
>>> Plot rectangular data as a color-encoded matrix.
>>> 
>>> 0.15868113674478607
-0.5252639480209298
-0.5679217474359622
-0.5679217474359622
-0.5679217474359622
-0.48525083421830983
0.23726258239509376
0.24874737454529963
0.31094646209649224
0.33523838920310134
0.08933160290817475
-0.1109298687428821
-0.10882391538484902
0.02064324047467884
0.26608475800444875
0.40532255793204885
0.09966546706038201
-0.366460372127424
0.4861813717486028
0.04914707429734496
0.5015597288281876
-0.6076899829411394
-0.67630488188541
-0.67630488188541
-0.67630488188541
-0.6563228128138459
-0.7384794816068588
0.27886740513110914
0.5575389076242668
0.578990246480506
0.19840039659046776
-0.1099993312125864
-0.11308479776040091
0.028332419014473347
0.032030081305890426
0.22945096470657927
-0.07405119714756495
-0.33144277559269303
0.5603794958746354
0.07064738881307406
0.5930952364133771
1.4706411031396258
1.4706411031396258
1.4706411031396258
1.3357621369065587
0.10380391028721452
-0.5426013314800836
-0.9303906033150214
-0.950078818429652
-0.3851445862225263
0.328479748193601
0.3278430646202429
-0.06001967070394204
-0.21230948388538295
-0.48564263949422476
-0.05088571020920385
0.6276965398424479
-1.0765339712305868
-0.07735705416307903
-1.1145390829941553
1.5707963267948966
1.5707963267948966
1.4359173605618292
0.12442266293213829
-0.592997285094396
-1.0032174089753578
-1.024178991236706
-0.427900336956536
0.3626157828575232
0.3610975374133605
-0.06124406219116848
-0.232830285211329
-0.5300635626508634
-0.057889229516152586
0.671529755085217
-1.1564132718573576
-0.08945404205689476
-1.19872824165597
1.5707963267948966
1.4359173605618292
0.12442266293213829
-0.592997285094396
-1.0032174089753578
-1.024178991236706
-0.427900336956536
0.3626157828575232
0.3610975374133605
-0.06124406219116848
-0.232830285211329
-0.5300635626508634
-0.057889229516152586
0.671529755085217
-1.1564132718573576
-0.08945404205689476
-1.19872824165597
1.4359173605618292
0.12442266293213829
-0.592997285094396
-1.0032174089753578
-1.024178991236706
-0.427900336956536
0.3626157828575232
0.3610975374133605
-0.06124406219116848
-0.232830285211329
-0.5300635626508634
-0.057889229516152586
0.671529755085217
-1.1564132718573576
-0.08945404205689476
-1.19872824165597
0.19210702434611707
-0.5557757838826586
-0.9569843864176193
-0.9754971857045109
-0.41702774054994945
0.34429888620858795
0.3325937035906852
-0.055269031733499466
-0.19012351013680678
-0.49225435352525604
-0.07855695782056732
0.6150118440347602
-1.0671796202681627
-0.07368387970139931
-1.106360147859471
-0.05453439684114543
-0.1704108071924319
-0.1799120851333231
-0.010358351981949525
-0.005656688670993961
0.006636201860775869
-0.005093468586880086
0.13098540130367906
0.029850664458624828
0.1817241845344207
0.09917571046549085
-0.17197802829608208
0.028601785141646525
-0.19529044221291397
0.5322184916683795
0.5467152868771631
0.6432463117302408
-0.2885156100504629
-0.2782796972172353
0.02113299706957168
-0.04608609557926685
0.18209150198058752
0.09878390518958115
-0.3577427047383498
0.675031514738683
0.4000086988774694
0.6462338269590793
1.5416068337393745
0.35105752721808997
-0.2873401942227308
-0.24977586339456762
0.03817652657178802
0.0037711257806631046
0.40032704066415664
0.09682487881001185
-0.4467804536895941
0.8503643757097766
0.10262849445947297
0.8886633414302825
0.3344058029917853
-0.2676519791081003
-0.22901018377117582
0.04033145558931283
-0.01513347878214117
0.38093267950646015
0.07733256633333788
-0.42767994648883395
0.8324392843367546
0.10096332203684048
0.8688771749966732
-0.6859530868047664
-0.6844348413606045
0.012709183637430729
-0.10941162329871838
0.03325447279312148
-0.0764999801220229
-0.31190148745652807
0.5213948709212843
0.23153243023485925
0.4746720917686574
1.4378274112819065
-0.013835623805674387
-0.02375319485222679
-0.06533352975850071
0.10397532509542556
0.24828210578014567
-0.3477761580323183
-0.024806171531232723
-0.3223088150979731
-0.016235431120639087
-0.03065876284019633
-0.06528455409900961
0.124104321145458
0.25685284619074544
-0.38485073226559097
-0.013982550784135753
-0.3650645658319834
-0.0012978549764584386
0.004824102459694042
7.346348922738833e-05
-0.02448782974456185
0.020153483879793047
-0.006611714031041695
0.03367076589879391
0.529230976439549
0.33611995107390535
-0.41318315128005456
0.2542326484080805
-0.12373700369928863
0.21975378412773117
0.27367598522525993
-0.4917890847601069
0.5504864126578342
0.10385288594670748
0.6004415853367435
-0.17143929604170627
0.15197147139477846
0.037686769976887115
0.1223901730633411
-0.6933239235578849
-0.0507387832307361
-0.7125223820776243
0.10399981292516026
1.3615723094573247
0.145922977447859
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/albio/albio/series_stat.py", line 329, in <module>
    def cross_funcM(M, func):
NameError: name 'M' is not defined
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/albio/albio/series_stat.py", line 331, in <module>
    M = np.array(M)
NameError: name 'itertools' is not defined
>>> M = X
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/albio/albio/series_stat.py", line 335, in <module>
    for c1, c2 in zip(comb[:, 0], comb[:, 1]):
NameError: name 'func' is not defined
>>> comb
array([[ 0,  1],
       [ 0,  2],
       [ 0,  3],
       [ 0,  4],
       [ 0,  5],
       [ 0,  6],
       [ 0,  7],
       [ 0,  8],
       [ 0,  9],
       [ 0, 10],
       [ 0, 11],
       [ 0, 12],
       [ 0, 13],
       [ 0, 14],
       [ 0, 15],
       [ 0, 16],
       [ 0, 17],
       [ 0, 18],
       [ 1,  2],
       [ 1,  3],
       [ 1,  4],
       [ 1,  5],
       [ 1,  6],
       [ 1,  7],
       [ 1,  8],
       [ 1,  9],
       [ 1, 10],
       [ 1, 11],
       [ 1, 12],
       [ 1, 13],
       [ 1, 14],
       [ 1, 15],
       [ 1, 16],
       [ 1, 17],
       [ 1, 18],
       [ 2,  3],
       [ 2,  4],
       [ 2,  5],
       [ 2,  6],
       [ 2,  7],
       [ 2,  8],
       [ 2,  9],
       [ 2, 10],
       [ 2, 11],
       [ 2, 12],
       [ 2, 13],
       [ 2, 14],
       [ 2, 15],
       [ 2, 16],
       [ 2, 17],
       [ 2, 18],
       [ 3,  4],
       [ 3,  5],
       [ 3,  6],
       [ 3,  7],
       [ 3,  8],
       [ 3,  9],
       [ 3, 10],
       [ 3, 11],
       [ 3, 12],
       [ 3, 13],
       [ 3, 14],
       [ 3, 15],
       [ 3, 16],
       [ 3, 17],
       [ 3, 18],
       [ 4,  5],
       [ 4,  6],
       [ 4,  7],
       [ 4,  8],
       [ 4,  9],
       [ 4, 10],
       [ 4, 11],
       [ 4, 12],
       [ 4, 13],
       [ 4, 14],
       [ 4, 15],
       [ 4, 16],
       [ 4, 17],
       [ 4, 18],
       [ 5,  6],
       [ 5,  7],
       [ 5,  8],
       [ 5,  9],
       [ 5, 10],
       [ 5, 11],
       [ 5, 12],
       [ 5, 13],
       [ 5, 14],
       [ 5, 15],
       [ 5, 16],
       [ 5, 17],
       [ 5, 18],
       [ 6,  7],
       [ 6,  8],
       [ 6,  9],
       [ 6, 10],
       [ 6, 11],
       [ 6, 12],
       [ 6, 13],
       [ 6, 14],
       [ 6, 15],
       [ 6, 16],
       [ 6, 17],
       [ 6, 18],
       [ 7,  8],
       [ 7,  9],
       [ 7, 10],
       [ 7, 11],
       [ 7, 12],
       [ 7, 13],
       [ 7, 14],
       [ 7, 15],
       [ 7, 16],
       [ 7, 17],
       [ 7, 18],
       [ 8,  9],
       [ 8, 10],
       [ 8, 11],
       [ 8, 12],
       [ 8, 13],
       [ 8, 14],
       [ 8, 15],
       [ 8, 16],
       [ 8, 17],
       [ 8, 18],
       [ 9, 10],
       [ 9, 11],
       [ 9, 12],
       [ 9, 13],
       [ 9, 14],
       [ 9, 15],
       [ 9, 16],
       [ 9, 17],
       [ 9, 18],
       [10, 11],
       [10, 12],
       [10, 13],
       [10, 14],
       [10, 15],
       [10, 16],
       [10, 17],
       [10, 18],
       [11, 12],
       [11, 13],
       [11, 14],
       [11, 15],
       [11, 16],
       [11, 17],
       [11, 18],
       [12, 13],
       [12, 14],
       [12, 15],
       [12, 16],
       [12, 17],
       [12, 18],
       [13, 14],
       [13, 15],
       [13, 16],
       [13, 17],
       [13, 18],
       [14, 15],
       [14, 16],
       [14, 17],
       [14, 18],
       [15, 16],
       [15, 17],
       [15, 18],
       [16, 17],
       [16, 18],
       [17, 18]])
>>> 0.15868113674478607
-0.5252639480209298
-0.5679217474359622
-0.5679217474359622
-0.5679217474359622
-0.48525083421830983
0.23726258239509376
0.24874737454529963
0.31094646209649224
0.33523838920310134
0.08933160290817475
-0.1109298687428821
-0.10882391538484902
0.02064324047467884
0.26608475800444875
0.40532255793204885
0.09966546706038201
-0.366460372127424
0.4861813717486028
0.04914707429734496
0.5015597288281876
-0.6076899829411394
-0.67630488188541
-0.67630488188541
-0.67630488188541
-0.6563228128138459
-0.7384794816068588
0.27886740513110914
0.5575389076242668
0.578990246480506
0.19840039659046776
-0.1099993312125864
-0.11308479776040091
0.028332419014473347
0.032030081305890426
0.22945096470657927
-0.07405119714756495
-0.33144277559269303
0.5603794958746354
0.07064738881307406
0.5930952364133771
1.4706411031396258
1.4706411031396258
1.4706411031396258
1.3357621369065587
0.10380391028721452
-0.5426013314800836
-0.9303906033150214
-0.950078818429652
-0.3851445862225263
0.328479748193601
0.3278430646202429
-0.06001967070394204
-0.21230948388538295
-0.48564263949422476
-0.05088571020920385
0.6276965398424479
-1.0765339712305868
-0.07735705416307903
-1.1145390829941553
1.5707963267948966
1.5707963267948966
1.4359173605618292
0.12442266293213829
-0.592997285094396
-1.0032174089753578
-1.024178991236706
-0.427900336956536
0.3626157828575232
0.3610975374133605
-0.06124406219116848
-0.232830285211329
-0.5300635626508634
-0.057889229516152586
0.671529755085217
-1.1564132718573576
-0.08945404205689476
-1.19872824165597
1.5707963267948966
1.4359173605618292
0.12442266293213829
-0.592997285094396
-1.0032174089753578
-1.024178991236706
-0.427900336956536
0.3626157828575232
0.3610975374133605
-0.06124406219116848
-0.232830285211329
-0.5300635626508634
-0.057889229516152586
0.671529755085217
-1.1564132718573576
-0.08945404205689476
-1.19872824165597
1.4359173605618292
0.12442266293213829
-0.592997285094396
-1.0032174089753578
-1.024178991236706
-0.427900336956536
0.3626157828575232
0.3610975374133605
-0.06124406219116848
-0.232830285211329
-0.5300635626508634
-0.057889229516152586
0.671529755085217
-1.1564132718573576
-0.08945404205689476
-1.19872824165597
0.19210702434611707
-0.5557757838826586
-0.9569843864176193
-0.9754971857045109
-0.41702774054994945
0.34429888620858795
0.3325937035906852
-0.055269031733499466
-0.19012351013680678
-0.49225435352525604
-0.07855695782056732
0.6150118440347602
-1.0671796202681627
-0.07368387970139931
-1.106360147859471
-0.05453439684114543
-0.1704108071924319
-0.1799120851333231
-0.010358351981949525
-0.005656688670993961
0.006636201860775869
-0.005093468586880086
0.13098540130367906
0.029850664458624828
0.1817241845344207
0.09917571046549085
-0.17197802829608208
0.028601785141646525
-0.19529044221291397
0.5322184916683795
0.5467152868771631
0.6432463117302408
-0.2885156100504629
-0.2782796972172353
0.02113299706957168
-0.04608609557926685
0.18209150198058752
0.09878390518958115
-0.3577427047383498
0.675031514738683
0.4000086988774694
0.6462338269590793
1.5416068337393745
0.35105752721808997
-0.2873401942227308
-0.24977586339456762
0.03817652657178802
0.0037711257806631046
0.40032704066415664
0.09682487881001185
-0.4467804536895941
0.8503643757097766
0.10262849445947297
0.8886633414302825
0.3344058029917853
-0.2676519791081003
-0.22901018377117582
0.04033145558931283
-0.01513347878214117
0.38093267950646015
0.07733256633333788
-0.42767994648883395
0.8324392843367546
0.10096332203684048
0.8688771749966732
-0.6859530868047664
-0.6844348413606045
0.012709183637430729
-0.10941162329871838
0.03325447279312148
-0.0764999801220229
-0.31190148745652807
0.5213948709212843
0.23153243023485925
0.4746720917686574
1.4378274112819065
-0.013835623805674387
-0.02375319485222679
-0.06533352975850071
0.10397532509542556
0.24828210578014567
-0.3477761580323183
-0.024806171531232723
-0.3223088150979731
-0.016235431120639087
-0.03065876284019633
-0.06528455409900961
0.124104321145458
0.25685284619074544
-0.38485073226559097
-0.013982550784135753
-0.3650645658319834
-0.0012978549764584386
0.004824102459694042
7.346348922738833e-05
-0.02448782974456185
0.020153483879793047
-0.006611714031041695
0.03367076589879391
0.529230976439549
0.33611995107390535
-0.41318315128005456
0.2542326484080805
-0.12373700369928863
0.21975378412773117
0.27367598522525993
-0.4917890847601069
0.5504864126578342
0.10385288594670748
0.6004415853367435
-0.17143929604170627
0.15197147139477846
0.037686769976887115
0.1223901730633411
-0.6933239235578849
-0.0507387832307361
-0.7125223820776243
0.10399981292516026
1.3615723094573247
0.145922977447859
>>> 0.15868113674478607
-0.5252639480209298
-0.5679217474359622
-0.5679217474359622
-0.5679217474359622
-0.48525083421830983
0.23726258239509376
0.24874737454529963
0.31094646209649224
0.33523838920310134
0.08933160290817475
-0.1109298687428821
-0.10882391538484902
0.02064324047467884
0.26608475800444875
0.40532255793204885
0.09966546706038201
-0.366460372127424
0.4861813717486028
0.04914707429734496
0.5015597288281876
-0.6076899829411394
-0.67630488188541
-0.67630488188541
-0.67630488188541
-0.6563228128138459
-0.7384794816068588
0.27886740513110914
0.5575389076242668
0.578990246480506
0.19840039659046776
-0.1099993312125864
-0.11308479776040091
0.028332419014473347
0.032030081305890426
0.22945096470657927
-0.07405119714756495
-0.33144277559269303
0.5603794958746354
0.07064738881307406
0.5930952364133771
1.4706411031396258
1.4706411031396258
1.4706411031396258
1.3357621369065587
0.10380391028721452
-0.5426013314800836
-0.9303906033150214
-0.950078818429652
-0.3851445862225263
0.328479748193601
0.3278430646202429
-0.06001967070394204
-0.21230948388538295
-0.48564263949422476
-0.05088571020920385
0.6276965398424479
-1.0765339712305868
-0.07735705416307903
-1.1145390829941553
1.5707963267948966
1.5707963267948966
1.4359173605618292
0.12442266293213829
-0.592997285094396
-1.0032174089753578
-1.024178991236706
-0.427900336956536
0.3626157828575232
0.3610975374133605
-0.06124406219116848
-0.232830285211329
-0.5300635626508634
-0.057889229516152586
0.671529755085217
-1.1564132718573576
-0.08945404205689476
-1.19872824165597
1.5707963267948966
1.4359173605618292
0.12442266293213829
-0.592997285094396
-1.0032174089753578
-1.024178991236706
-0.427900336956536
0.3626157828575232
0.3610975374133605
-0.06124406219116848
-0.232830285211329
-0.5300635626508634
-0.057889229516152586
0.671529755085217
-1.1564132718573576
-0.08945404205689476
-1.19872824165597
1.4359173605618292
0.12442266293213829
-0.592997285094396
-1.0032174089753578
-1.024178991236706
-0.427900336956536
0.3626157828575232
0.3610975374133605
-0.06124406219116848
-0.232830285211329
-0.5300635626508634
-0.057889229516152586
0.671529755085217
-1.1564132718573576
-0.08945404205689476
-1.19872824165597
0.19210702434611707
-0.5557757838826586
-0.9569843864176193
-0.9754971857045109
-0.41702774054994945
0.34429888620858795
0.3325937035906852
-0.055269031733499466
-0.19012351013680678
-0.49225435352525604
-0.07855695782056732
0.6150118440347602
-1.0671796202681627
-0.07368387970139931
-1.106360147859471
-0.05453439684114543
-0.1704108071924319
-0.1799120851333231
-0.010358351981949525
-0.005656688670993961
0.006636201860775869
-0.005093468586880086
0.13098540130367906
0.029850664458624828
0.1817241845344207
0.09917571046549085
-0.17197802829608208
0.028601785141646525
-0.19529044221291397
0.5322184916683795
0.5467152868771631
0.6432463117302408
-0.2885156100504629
-0.2782796972172353
0.02113299706957168
-0.04608609557926685
0.18209150198058752
0.09878390518958115
-0.3577427047383498
0.675031514738683
0.4000086988774694
0.6462338269590793
1.5416068337393745
0.35105752721808997
-0.2873401942227308
-0.24977586339456762
0.03817652657178802
0.0037711257806631046
0.40032704066415664
0.09682487881001185
-0.4467804536895941
0.8503643757097766
0.10262849445947297
0.8886633414302825
0.3344058029917853
-0.2676519791081003
-0.22901018377117582
0.04033145558931283
-0.01513347878214117
0.38093267950646015
0.07733256633333788
-0.42767994648883395
0.8324392843367546
0.10096332203684048
0.8688771749966732
-0.6859530868047664
-0.6844348413606045
0.012709183637430729
-0.10941162329871838
0.03325447279312148
-0.0764999801220229
-0.31190148745652807
0.5213948709212843
0.23153243023485925
0.4746720917686574
1.4378274112819065
-0.013835623805674387
-0.02375319485222679
-0.06533352975850071
0.10397532509542556
0.24828210578014567
-0.3477761580323183
-0.024806171531232723
-0.3223088150979731
-0.016235431120639087
-0.03065876284019633
-0.06528455409900961
0.124104321145458
0.25685284619074544
-0.38485073226559097
-0.013982550784135753
-0.3650645658319834
-0.0012978549764584386
0.004824102459694042
7.346348922738833e-05
-0.02448782974456185
0.020153483879793047
-0.006611714031041695
0.03367076589879391
0.529230976439549
0.33611995107390535
-0.41318315128005456
0.2542326484080805
-0.12373700369928863
0.21975378412773117
0.27367598522525993
-0.4917890847601069
0.5504864126578342
0.10385288594670748
0.6004415853367435
-0.17143929604170627
0.15197147139477846
0.037686769976887115
0.1223901730633411
-0.6933239235578849
-0.0507387832307361
-0.7125223820776243
0.10399981292516026
1.3615723094573247
0.145922977447859
>>> s_s.timeLag(y2,y1)
-0.48564263949422476
>>> s_s.timeLag(y2,y1)
-0.48564263949422476
>>> a
array([0., 0., 0., ..., 0., 0., 0.])
>>> b
array([ 0.       ,  0.       ,  0.       , ..., -0.0273228, -0.0273228,
       -0.0273228])
>>> y1
array([-0.4612443, -0.4612443, -0.4612443, ..., -0.4612443, -0.4612443,
       -0.4612443])
>>> s
array([0., 0., 0., ..., 0., 0., 0.])
>>> sum(s)
-24.0
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4088, in trapz
    ret = (d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0).sum(axis)
ValueError: operands could not be broadcast together with shapes (64146,) (64145,) 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/src/albio/albio/series_stat.py", line 162, in <module>
    s_avg = np.trapz(s, x=t) / (t[-1] - t[0])
  File "<__array_function__ internals>", line 5, in trapz
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4093, in trapz
    ret = add.reduce(d * (y[tuple(slice1)]+y[tuple(slice2)])/2.0, axis)
ValueError: operands could not be broadcast together with shapes (64146,) (64145,) 
>>> phase
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'phase' is not defined
>>> phase_shift
-0.0005877170760501893
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4088, in trapz
    ret = (d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0).sum(axis)
ValueError: operands could not be broadcast together with shapes (64145,) (704368,) 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 111, in <module>
    importlib.reload(t_v)
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 201, in lagMatrix
    lagM[i,j] = timeLag(X[c],X[r])
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 162, in timeLag
    s_avg = np.trapz(s, x=t) / (t[-1] - t[0])
  File "<__array_function__ internals>", line 5, in trapz
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4093, in trapz
    ret = add.reduce(d * (y[tuple(slice1)]+y[tuple(slice2)])/2.0, axis)
ValueError: operands could not be broadcast together with shapes (64145,) (704368,) 
>>> s
array([0., 0., 0., ..., 0., 0., 0.])
>>> s_Avg
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 's_Avg' is not defined
>>> a_avg
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'a_avg' is not defined
>>> s_avg
-0.000374152311171612
>>> (t[-1] - t[0])
  File "<stdin>", line 1
    (t[-1] - t[0])
    ^
IndentationError: unexpected indent
>>> (t[-1] - t[0])
64.145
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4088, in trapz
    ret = (d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0).sum(axis)
ValueError: operands could not be broadcast together with shapes (64145,) (704368,) 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 111, in <module>
    lagM = s_s.lagMatrix(X1)
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 201, in lagMatrix
    lagM[i,j] = timeLag(X[c],X[r])
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 162, in timeLag
    s_avg = np.trapz(s, x=t) / (t[-1] - t[0])
  File "<__array_function__ internals>", line 5, in trapz
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4093, in trapz
    ret = add.reduce(d * (y[tuple(slice1)]+y[tuple(slice2)])/2.0, axis)
ValueError: operands could not be broadcast together with shapes (64145,) (704368,) 
>>> 704369 64146
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4088, in trapz
    ret = (d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0).sum(axis)
ValueError: operands could not be broadcast together with shapes (64145,) (704368,) 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 111, in <module>
    lagM = s_s.lagMatrix(X1)
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 202, in lagMatrix
    lagM[i,j] = timeLag(X[c],X[r])
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 163, in timeLag
    s_avg = np.trapz(s, x=t) / (t[-1] - t[0])
  File "<__array_function__ internals>", line 5, in trapz
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4093, in trapz
    ret = add.reduce(d * (y[tuple(slice1)]+y[tuple(slice2)])/2.0, axis)
ValueError: operands could not be broadcast together with shapes (64145,) (704368,) 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 704369 64146
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4088, in trapz
    ret = (d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0).sum(axis)
ValueError: operands could not be broadcast together with shapes (64145,) (704368,) 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 111, in <module>
    lagM = s_s.lagMatrix(X1)
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 202, in lagMatrix
    lagM[i,j] = timeLag(X[c],X[r])
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 163, in timeLag
    s_avg = np.trapz(s, x=t) / (t[-1] - t[0])
  File "<__array_function__ internals>", line 5, in trapz
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4093, in trapz
    ret = add.reduce(d * (y[tuple(slice1)]+y[tuple(slice2)])/2.0, axis)
ValueError: operands could not be broadcast together with shapes (64145,) (704368,) 
>>> p_inst
array([ 0.,  0.,  0., ..., -0., -0., -0.])
>>> len(p_inst)
64146
>>> help(np.trapz)
Help on function trapz in module numpy:

trapz(y, x=None, dx=1.0, axis=-1)
    Integrate along the given axis using the composite trapezoidal rule.
    
    Integrate `y` (`x`) along given axis.
    
    Parameters
    ----------
    y : array_like
        Input array to integrate.
    x : array_like, optional
        The sample points corresponding to the `y` values. If `x` is None,
        the sample points are assumed to be evenly spaced `dx` apart. The
        default is None.
    dx : scalar, optional
        The spacing between sample points when `x` is None. The default is 1.
    axis : int, optional
        The axis along which to integrate.
    
    Returns
    -------
    trapz : float
        Definite integral as approximated by trapezoidal rule.
    
    See Also
    --------
    sum, cumsum
    
    Notes
    -----
    Image [2]_ illustrates trapezoidal rule -- y-axis locations of points
    will be taken from `y` array, by default x-axis distances between
    points will be 1.0, alternatively they can be provided with `x` array
    or with `dx` scalar.  Return value will be equal to combined area under
    the red lines.
    
    
    References
    ----------
    .. [1] Wikipedia page: https://en.wikipedia.org/wiki/Trapezoidal_rule
    
    .. [2] Illustration image:
           https://en.wikipedia.org/wiki/File:Composite_trapezoidal_rule_illustration.png
    
    Examples
    --------
    >>> np.trapz([1,2,3])
    4.0
    >>> np.trapz([1,2,3], x=[4,6,8])
    8.0
    >>> np.trapz([1,2,3], dx=2)
    8.0
    >>> a = np.arange(6).reshape(2, 3)
    >>> a
    array([[0, 1, 2],
           [3, 4, 5]])
    >>> np.trapz(a, axis=0)
    array([1.5, 2.5, 3.5])
    >>> np.trapz(a, axis=1)
    array([2.,  8.])

>>> t
array([0.0000e+00, 1.0000e-03, 2.0000e-03, ..., 6.4143e+01, 6.4144e+01,
       6.4145e+01])
>>> len(t)
64146
>>> 704369 704369 64146
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4088, in trapz
    ret = (d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0).sum(axis)
ValueError: operands could not be broadcast together with shapes (64145,) (704368,) 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 111, in <module>
    lagM = s_s.lagMatrix(X1)
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 202, in lagMatrix
    lagM[i,j] = timeLag(X[c],X[r])
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 163, in timeLag
    s_avg = np.trapz(s, x=t) / (t[-1] - t[0])
  File "<__array_function__ internals>", line 5, in trapz
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4093, in trapz
    ret = add.reduce(d * (y[tuple(slice1)]+y[tuple(slice2)])/2.0, axis)
ValueError: operands could not be broadcast together with shapes (64145,) (704368,) 
>>> 704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369

704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
704369 704369 704369
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4088, in trapz
    ret = (d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0).sum(axis)
ValueError: operands could not be broadcast together with shapes (64146,) (8424236,) 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 111, in <module>
    lagM = s_s.lagMatrix(X1)
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 203, in lagMatrix
    lagM[i,j] = timeLag(X[c],X[r])
  File "/home/sabeiro/lav//src/albio/albio/series_stat.py", line 164, in timeLag
    s_avg = np.trapz(s, x=t) / (t[-1] - t[0])
  File "<__array_function__ internals>", line 5, in trapz
  File "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py", line 4093, in trapz
    ret = add.reduce(d * (y[tuple(slice1)]+y[tuple(slice2)])/2.0, axis)
ValueError: operands could not be broadcast together with shapes (64146,) (8424236,) 
>>> a
array([0., 0., 0., ..., 0., 0., 0.])
>>> len(a)
64146
>>> len(y1)
64147
>>> len(p_inst)
64146
>>> p_inst
array([ 0.,  0.,  0., ..., -0., -0., -0.])
>>> a
array([0., 0., 0., ..., 0., 0., 0.])
>>> a
array([0.01476441, 0.01476441, 0.01476441, ..., 0.01476441, 0.01476441,
       0.01476441])
>>> b
array([-5.66164533, -5.66164533, -5.66164533, ..., -4.36523773,
       -4.39256053, -4.41988333])
>>> s
array([0., 0., 0., ..., 0., 0., 0.])
>>> s
array([-1., -1., -1., ..., -1., -1., -1.])
>>> sa
array([1., 1., 1., ..., 1., 1., 1.])
>>> np.mean(sb)
array([-1., -1., -1., ..., -1., -1., -1.])
>>> np.sign(np.mean(sa))
0.9362401982945422
>>> np.mean(sb)
-0.3374592732317957
>>> np.sing(np.mean(sa))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/lib/python3.8/dist-packages/numpy/__init__.py", line 214, in __getattr__
    raise AttributeError("module {!r} has no attribute "
AttributeError: module 'numpy' has no attribute 'sing'
>>> np.sign(np.mean(sa))
1.0
>>> s_s.timeLag(y2,y1)
3.627235293084018
>>> s_s.timeLag(y2,y1)
-3.627235293084018
>>> np.pi
3.141592653589793
>>> shift_fraction
-0.1545848533034008
>>> shift_fraction
0.34541514669659923
>>> phase_shift
2.0564389662891163
>>> a
array([0., 0., 0., ..., 0., 0., 0.])
>>> b
array([ 0.       ,  0.       ,  0.       , ..., -0.0273228, -0.0273228,
       -0.0273228])
>>> p_inst
array([ 0.,  0.,  0., ..., -0., -0., -0.])
>>> s
array([0., 0., 0., ..., 0., 0., 0.])
>>> s[s!=0]
array([ 1.,  1., -1., ...,  1.,  1.,  1.])
>>> /home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:38: RuntimeWarning: Mean of empty slice
  feat.loc[:,"modem_rx"] = feat.apply(lambda x: np.nanmean(x['modem0_rx'] + x['modem1_rx'] + x['modem2_rx'] + x['modem3_rx']),axis=1)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:39: RuntimeWarning: Mean of empty slice
  # feat.drop(columns=mL,inplace=True)
/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py:40: RuntimeWarning: Mean of empty slice
  featL.append(feat)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'steering_wheel_deg'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 53, in <module>
    threshold = np.nanmean(featL[i])*7.
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'steering_wheel_deg'
>>> --------------------histograms-and-outliers------------------
>>> Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2646, in get_loc
    return self._engine.get_loc(key)
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'class'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 156, in <module>
    b = feat['class']
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py", line 2800, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py", line 2648, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File "pandas/_libs/index.pyx", line 111, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 138, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'class'
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 157, in <module>
    b, _ = t_r.binOutlier(feat['camera_latency'],nBin=2)
NameError: name 't_l' is not defined
>>> ----------------------predictability------------------------
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 161, in <module>
    importlib.reload(tlib)
NameError: name 'tlib' is not defined
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 16, in <module>
    import lernia.train_viz as t_v
ModuleNotFoundError: No module named 'lernia.train_lib'
>>> random_forest class ravel
decision_tree   class ravel
extra_tree      class ravel
perceptron      class ravel
k_neighbors     class ravel
grad_boost      logit ravel
discriminant    logit ravel
logit_reg       logit ravel
/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
>>> b
array([1, 0, 1, ..., 1, 1, 1])
>>> set(b)
{0, 1, 2, -1}
>>> feat['camera_latency']
timebucket
2020-08-03 14:52:27    5.156213
2020-08-03 14:52:28    5.078333
2020-08-03 14:52:29    5.121020
2020-08-03 14:52:30    5.539325
2020-08-03 14:52:31    5.084544
                         ...   
2020-07-21 10:57:48    5.280694
2020-07-21 10:57:49    5.363666
2020-07-21 10:57:50    5.539325
2020-07-21 10:57:51    5.330331
2020-07-21 10:57:52    5.228465
Name: camera_latency, Length: 21884, dtype: float64
>>> feat['camera_latency']
timebucket
2020-08-03 14:52:27    0.052501
2020-08-03 14:52:28    0.019115
2020-08-03 14:52:29    0.037415
2020-08-03 14:52:30    0.216732
2020-08-03 14:52:31    0.021778
                         ...   
2020-07-21 10:57:48    0.105863
2020-07-21 10:57:49    0.141431
2020-07-21 10:57:50    0.216732
2020-07-21 10:57:51    0.127141
2020-07-21 10:57:52    0.083473
Name: camera_latency, Length: 21884, dtype: float64
>>> b
timebucket
2020-08-03 14:52:27    0.0
2020-08-03 14:52:28    0.0
2020-08-03 14:52:29    0.0
2020-08-03 14:52:30    0.0
2020-08-03 14:52:31    0.0
                      ... 
2020-07-21 10:57:48    0.0
2020-07-21 10:57:49    0.0
2020-07-21 10:57:50    0.0
2020-07-21 10:57:51    0.0
2020-07-21 10:57:52    0.0
Name: camera_latency, Length: 21884, dtype: float64
>>> np.unique(b,return_counts=True)
(array([0., 1.]), array([21537,   347]))
>>> (array([0., 1.]), array([21537,   347]))
random_forest class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 168, in <module>
    mod, trainR = tMod.loopMod(paramF=baseDir+"rem/train/weath_"+"camera"+".json",test_size=.4)
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 206, in loopMod
    mod, trainS, testS, t_diff, x_pr, y_pr, auc, fsc, acc, cv = self.perfCla(clf,trainL,testL)
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 175, in perfCla
    x_pr, y_pr, _ = sk.metrics.roc_curve(y_score1,y_score)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 775, in roc_curve
    fps, tps, thresholds = _binary_clf_curve(
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 541, in _binary_clf_curve
    check_consistent_length(y_true, y_score, sample_weight)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 256, in check_consistent_length
    raise ValueError("Found input variables with inconsistent numbers of"
ValueError: Found input variables with inconsistent numbers of samples: [8754, 17508]
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 134, in <module>
    sns.pairplot(featL[sL+['b_latency']],hue='b_latency',diag_kind="kde",markers="+",plot_kws=dict(s=50,edgecolor="b", linewidth=1),diag_kws=dict(shade=True),ax=ax[0])
TypeError: pairplot() got an unexpected keyword argument 'ax'
>>> (array([0., 1.]), array([21537,   347]))
random_forest class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 169, in <module>
    mod, trainR = tMod.loopMod(paramF=baseDir+"rem/train/weath_"+"camera"+".json",test_size=.4)
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 206, in loopMod
    mod, trainS, testS, t_diff, x_pr, y_pr, auc, fsc, acc, cv = self.perfCla(clf,trainL,testL)
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 175, in perfCla
    x_pr, y_pr, _ = sk.metrics.roc_curve(y_score1,y_score)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 775, in roc_curve
    fps, tps, thresholds = _binary_clf_curve(
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 541, in _binary_clf_curve
    check_consistent_length(y_true, y_score, sample_weight)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 256, in check_consistent_length
    raise ValueError("Found input variables with inconsistent numbers of"
ValueError: Found input variables with inconsistent numbers of samples: [8754, 17508]
>>> _
[4.783368428966969, 5.065793842039684, 5.450383948366866, 5.834974054694047, 7.783239267779369]
>>> (array([0., 1.]), array([21537,   347]))
random_forest class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 173, in <module>
    mod, trainR = tMod.loopMod(paramF=baseDir+"rem/train/weath_"+"camera"+".json",test_size=.4)
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 206, in loopMod
    mod, trainS, testS, t_diff, x_pr, y_pr, auc, fsc, acc, cv = self.perfCla(clf,trainL,testL)
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 175, in perfCla
    x_pr, y_pr, _ = sk.metrics.roc_curve(y_score1,y_score)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 775, in roc_curve
    fps, tps, thresholds = _binary_clf_curve(
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 541, in _binary_clf_curve
    check_consistent_length(y_true, y_score, sample_weight)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 256, in check_consistent_length
    raise ValueError("Found input variables with inconsistent numbers of"
ValueError: Found input variables with inconsistent numbers of samples: [8754, 17508]
>>> X.shape
(21884, 13)
>>> b.shape
(21884,)
>>> 
>>> 
>>> (array([0., 1.]), array([21537,   347]))
>>> random_forest class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 174, in <module>
    tMod.plotRoc()
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 206, in loopMod
    mod, trainS, testS, t_diff, x_pr, y_pr, auc, fsc, acc, cv = self.perfCla(clf,trainL,testL)
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 175, in perfCla
    x_pr, y_pr, _ = sk.metrics.roc_curve(y_score1,y_score)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 775, in roc_curve
    fps, tps, thresholds = _binary_clf_curve(
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 541, in _binary_clf_curve
    check_consistent_length(y_true, y_score, sample_weight)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 256, in check_consistent_length
    raise ValueError("Found input variables with inconsistent numbers of"
ValueError: Found input variables with inconsistent numbers of samples: [8754, 17508]
>>> random_forest class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
(8754, 1) (17508,) (8754,)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 174, in <module>
    tMod.plotRoc()
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 206, in loopMod
    mod, trainS, testS, t_diff, x_pr, y_pr, auc, fsc, acc, cv = self.perfCla(clf,trainL,testL)
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 175, in perfCla
    x_pr, y_pr, _ = sk.metrics.roc_curve(y_score1,y_score)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 775, in roc_curve
    fps, tps, thresholds = _binary_clf_curve(
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 541, in _binary_clf_curve
    check_consistent_length(y_true, y_score, sample_weight)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 256, in check_consistent_length
    raise ValueError("Found input variables with inconsistent numbers of"
ValueError: Found input variables with inconsistent numbers of samples: [8754, 17508]
>>> b
timebucket
2020-08-03 14:52:27    0.0
2020-08-03 14:52:28    0.0
2020-08-03 14:52:29    0.0
2020-08-03 14:52:30    0.0
2020-08-03 14:52:31    0.0
                      ... 
2020-07-21 10:57:48    0.0
2020-07-21 10:57:49    0.0
2020-07-21 10:57:50    0.0
2020-07-21 10:57:51    0.0
2020-07-21 10:57:52    0.0
Name: camera_latency, Length: 21884, dtype: float64
>>> random_forest class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
(8754, 1) (17508,) (8754,)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 174, in <module>
    tMod.plotRoc()
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 206, in loopMod
    mod, trainS, testS, t_diff, x_pr, y_pr, auc, fsc, acc, cv = self.perfCla(clf,trainL,testL)
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 175, in perfCla
    x_pr, y_pr, _ = sk.metrics.roc_curve(y_score1,y_score)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 775, in roc_curve
    fps, tps, thresholds = _binary_clf_curve(
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 541, in _binary_clf_curve
    check_consistent_length(y_true, y_score, sample_weight)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 256, in check_consistent_length
    raise ValueError("Found input variables with inconsistent numbers of"
ValueError: Found input variables with inconsistent numbers of samples: [8754, 17508]
>>> 8754*2
76632516
>>> 8754*2
17508
>>> random_forest class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
qui
(8754, 1) (17508,) (8754,)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 174, in <module>
    tMod.plotRoc()
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 207, in loopMod
    mod, trainS, testS, t_diff, x_pr, y_pr, auc, fsc, acc, cv = self.perfCla(clf,trainL,testL)
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 176, in perfCla
    x_pr, y_pr, _ = sk.metrics.roc_curve(y_score1,y_score)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 775, in roc_curve
    fps, tps, thresholds = _binary_clf_curve(
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 541, in _binary_clf_curve
    check_consistent_length(y_true, y_score, sample_weight)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 256, in check_consistent_length
    raise ValueError("Found input variables with inconsistent numbers of"
ValueError: Found input variables with inconsistent numbers of samples: [8754, 17508]
>>> random_forest class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
qui
(8754, 1) (17508,) (8754,)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 174, in <module>
    tMod.plotRoc()
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 208, in loopMod
    mod, trainS, testS, t_diff, x_pr, y_pr, auc, fsc, acc, cv = self.perfCla(clf,trainL,testL)
  File "/home/sabeiro/lav//src/lernia/lernia/train_model.py", line 177, in perfCla
    x_pr, y_pr, _ = sk.metrics.roc_curve(y_score1,y_score)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 775, in roc_curve
    fps, tps, thresholds = _binary_clf_curve(
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_ranking.py", line 541, in _binary_clf_curve
    check_consistent_length(y_true, y_score, sample_weight)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 256, in check_consistent_length
    raise ValueError("Found input variables with inconsistent numbers of"
ValueError: Found input variables with inconsistent numbers of samples: [8754, 17508]
>>> random_forest class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
qui
(8754, 1) (8754,) (8754,)
decision_tree   class ravel
qui
(8754, 1) (8754,) (8754,)
extra_tree      class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
qui
(8754, 1) (8754,) (8754,)
perceptron      class ravel
/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  return f(**kwargs)
qui
(8754, 1) (8754,) (8754,)
k_neighbors     class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
qui
(8754, 1) (8754,) (8754,)
grad_boost      logit ravel
qui
(8754,) (8754,) (8754,)
discriminant    logit ravel
qui
(8754,) (8754,) (8754,)
logit_reg       logit ravel
qui
(8754,) (8754,) (8754,)
>>> print(value, ..., sep=' ', end='\n', file=sys.stdout, flush=False)
>>> random_forest class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
(8754, 1) (8754,) (8754,)
decision_tree   class ravel
(8754, 1) (8754,) (8754,)
extra_tree      class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
(8754, 1) (8754,) (8754,)
perceptron      class ravel
/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  return f(**kwargs)
(8754, 1) (8754,) (8754,)
k_neighbors     class ravel
/home/sabeiro/lav//src/lernia/lernia/train_model.py:158: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  mod = clf['mod'].fit(X_train,y_train)
(8754, 1) (8754,) (8754,)
grad_boost      logit ravel
(8754,) (8754,) (8754,)
discriminant    logit ravel
(8754,) (8754,) (8754,)
logit_reg       logit ravel
(8754,) (8754,) (8754,)
>>> X
                     object_distance  force_lat  force_lon  ...  camera_jitter  vehicle_ram  vehicle_cpu
timebucket                                                  ...                                         
2020-08-03 14:52:27         4.344345  -1.750145  -0.212849  ...      33.320470     7.114678    63.937283
2020-08-03 14:52:28         4.344345  -1.915192  -1.513698  ...      33.373272     7.116497    65.384610
2020-08-03 14:52:29         4.344345  -2.372868  -1.513698  ...      33.340440     7.116705    67.857150
2020-08-03 14:52:30         4.344345  -1.757365  -1.513698  ...      33.463375     7.115436    69.165245
2020-08-03 14:52:31         4.344345  -1.788914  -1.513698  ...      33.241623     7.115239    62.956524
...                              ...        ...        ...  ...            ...          ...          ...
2020-07-21 10:57:48         4.323699  -1.348147  -0.384723  ...      33.238620     7.117743    64.471400
2020-07-21 10:57:49         4.323699  -1.988696  -0.150887  ...      33.377810     7.117433    66.551125
2020-07-21 10:57:50         4.323699  -2.881151  -0.083636  ...      33.453487     7.117319    68.205130
2020-07-21 10:57:51         4.323699  -3.713028  -0.093260  ...      33.366890     7.118088    66.254410
2020-07-21 10:57:52         4.323699  -4.207757  -1.513698  ...      33.133156     7.117599    64.583336

[21884 rows x 13 columns]
>>> mod
ExtraTreesClassifier()
>>> mod.predict(X)
array([0, 0, 0, ..., 0, 0, 0])
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 177, in <module>
    importlib.reload(t_k)
  File "/home/sabeiro/lav//src/lernia/lernia/train_viz.py", line 452, in plotConfMat
    cm = confusion_matrix(y,y_pred)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py", line 276, in confusion_matrix
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
  File "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py", line 90, in _check_targets
    raise ValueError("Classification metrics can't handle a mix of {0} "
ValueError: Classification metrics can't handle a mix of binary and continuous targets
>>> b
timebucket
2020-08-03 14:52:27    0.0
2020-08-03 14:52:28    0.0
2020-08-03 14:52:29    0.0
2020-08-03 14:52:30    0.0
2020-08-03 14:52:31    0.0
                      ... 
2020-07-21 10:57:48    0.0
2020-07-21 10:57:49    0.0
2020-07-21 10:57:50    0.0
2020-07-21 10:57:51    0.0
2020-07-21 10:57:52    0.0
Name: camera_latency, Length: 21884, dtype: float64
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 177, in <module>
    t_v.plotConfMat(b,y)
NameError: name 'y_pred' is not defined
>>> y_pred
array([0, 0, 0, ..., 0, 0, 0])
>>> set(y_pred)
{0, 1}
>>> set(b)
{0.0, 1.0}
>>> [[21530     7]
 [  109   238]]
>>> [[21530     7]
 [  109   238]]
>>> cm.sum()
21884
>>> y
array([5.15621345, 5.0783327 , 5.12102049, ..., 5.53932528, 5.33033054,
       5.2284646 ])
>>> 2020-08-12 18:23:19.371593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-08-12 18:23:19.984063: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-08-12 18:23:19.985563: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (malindro): /proc/driver/nvidia/version does not exist
2020-08-12 18:23:19.993182: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-08-12 18:23:20.267314: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2899500000 Hz
2020-08-12 18:23:20.270952: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f63a0000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-12 18:23:20.271009: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
16413/16413 - 19s - loss: 0.0517 - val_loss: 0.0844
16413/16413 - 19s - loss: 0.0418 - val_loss: 0.0720
16413/16413 - 19s - loss: 0.0418 - val_loss: 0.0691
16413/16413 - 19s - loss: 0.0418 - val_loss: 0.0679
16413/16413 - 19s - loss: 0.0417 - val_loss: 0.0662
16413/16413 - 24s - loss: 0.0417 - val_loss: 0.0645
16413/16413 - 24s - loss: 0.0416 - val_loss: 0.0629
16413/16413 - 24s - loss: 0.0415 - val_loss: 0.0613
16413/16413 - 24s - loss: 0.0414 - val_loss: 0.0596
16413/16413 - 25s - loss: 0.0413 - val_loss: 0.0579
16413/16413 - 24s - loss: 0.0412 - val_loss: 0.0558
16413/16413 - 25s - loss: 0.0411 - val_loss: 0.0541
16413/16413 - 26s - loss: 0.0410 - val_loss: 0.0532
16413/16413 - 26s - loss: 0.0409 - val_loss: 0.0526
16413/16413 - 24s - loss: 0.0409 - val_loss: 0.0523
16413/16413 - 21s - loss: 0.0409 - val_loss: 0.0520
16413/16413 - 22s - loss: 0.0409 - val_loss: 0.0518
16413/16413 - 21s - loss: 0.0409 - val_loss: 0.0516
16413/16413 - 21s - loss: 0.0409 - val_loss: 0.0515
16413/16413 - 24s - loss: 0.0409 - val_loss: 0.0514
  C-c C-cTraceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py", line 3715, in _get_op_def
    return self._op_def_cache[type]
KeyError: 'Const'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 195, in <module>
    plt.show()
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 167, in train
    y_pred = self.predict(X_test,batch_size=batch_size)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 177, in predict
    yhat = self.model.predict(X1, batch_size=batch_size)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 88, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 1240, in predict
    data_handler = data_adapter.DataHandler(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/data_adapter.py", line 1100, in __init__
    self._adapter = adapter_cls(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/data_adapter.py", line 328, in __init__
    indices_dataset = indices_dataset.map(permutation).prefetch(1)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py", line 1621, in map
    return MapDataset(self, map_func, preserve_cardinality=True)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py", line 3977, in __init__
    self._map_func = StructuredFunctionWrapper(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py", line 3221, in __init__
    self._function = wrapper_fn.get_concrete_function()
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2531, in get_concrete_function
    graph_function = self._get_concrete_function_garbage_collected(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2496, in _get_concrete_function_garbage_collected
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2657, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py", line 3214, in wrapper_fn
    ret = _wrapper_helper(*args)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py", line 3156, in _wrapper_helper
    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py", line 262, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py", line 492, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py", line 346, in _call_unconverted
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/data_adapter.py", line 319, in permutation
    indices = math_ops.range(num_samples, dtype=dtypes.int64)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/math_ops.py", line 1569, in range
    start = ops.convert_to_tensor(start, dtype=dtype, name="start")
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py", line 1341, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py", line 52, in _default_conversion_function
    return constant_op.constant(value, dtype, name=name)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py", line 261, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py", line 302, in _constant_impl
    const_tensor = g._create_op_internal(  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 593, in _create_op_internal
    return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py", line 3319, in _create_op_internal
    ret = Operation(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py", line 1815, in __init__
    op_def = self._graph._get_op_def(node_def.op)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py", line 3724, in _get_op_def
    op_def.ParseFromString(compat.as_bytes(data))
  File "/usr/local/lib/python3.8/dist-packages/google/protobuf/message.py", line 187, in ParseFromString
    return self.MergeFromString(serialized)
  File "/usr/local/lib/python3.8/dist-packages/google/protobuf/internal/python_message.py", line 1124, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File "/usr/local/lib/python3.8/dist-packages/google/protobuf/internal/python_message.py", line 1160, in InternalParse
    (tag_bytes, new_pos) = local_ReadTag(buffer, pos)
  File "/usr/local/lib/python3.8/dist-packages/google/protobuf/internal/decoder.py", line 192, in ReadTag
    while six.indexbytes(buffer, pos) & 0x80:
KeyboardInterrupt
>>> 

  C-c C-cTraceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py", line 3715, in _get_op_def
    return self._op_def_cache[type]
KeyError: 'Placeholder'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 196, in <module>
    tK.plotHistory()
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_keras.py", line 161, in plotPrediction
    y_pred = self.predict(X)
  File "/home/sabeiro/lav//src/deep_lernia/deep_lernia/train_longShort.py", line 177, in predict
    yhat = self.model.predict(X1, batch_size=batch_size)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 88, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py", line 1240, in predict
    data_handler = data_adapter.DataHandler(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/data_adapter.py", line 1100, in __init__
    self._adapter = adapter_cls(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/data_adapter.py", line 364, in __init__
    dataset = self.slice_inputs(indices_dataset, inputs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/data_adapter.py", line 396, in slice_inputs
    dataset = dataset.map(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py", line 1623, in map
    return ParallelMapDataset(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py", line 4016, in __init__
    self._map_func = StructuredFunctionWrapper(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py", line 3221, in __init__
    self._function = wrapper_fn.get_concrete_function()
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2531, in get_concrete_function
    graph_function = self._get_concrete_function_garbage_collected(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2496, in _get_concrete_function_garbage_collected
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2657, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 897, in func_graph_from_py_func
    func_args = _get_defun_inputs_from_args(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 1131, in _get_defun_inputs_from_args
    return _get_defun_inputs(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 1210, in _get_defun_inputs
    placeholder = graph_placeholder(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/graph_only_ops.py", line 38, in graph_placeholder
    op = g._create_op_internal(  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py", line 593, in _create_op_internal
    return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py", line 3319, in _create_op_internal
    ret = Operation(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py", line 1815, in __init__
    op_def = self._graph._get_op_def(node_def.op)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py", line 3724, in _get_op_def
    op_def.ParseFromString(compat.as_bytes(data))
  File "/usr/local/lib/python3.8/dist-packages/google/protobuf/message.py", line 187, in ParseFromString
    return self.MergeFromString(serialized)
  File "/usr/local/lib/python3.8/dist-packages/google/protobuf/internal/python_message.py", line 1124, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File "/usr/local/lib/python3.8/dist-packages/google/protobuf/internal/python_message.py", line 1189, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
  File "/usr/local/lib/python3.8/dist-packages/google/protobuf/internal/decoder.py", line 702, in DecodeRepeatedField
    if value.add()._InternalParse(buffer, pos, new_pos) != new_pos:
  File "/usr/local/lib/python3.8/dist-packages/google/protobuf/internal/python_message.py", line 1189, in InternalParse
    pos = field_decoder(buffer, new_pos, end, self, field_dict)
KeyboardInterrupt
>>> 
>>> 
>>> Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sabeiro/lav/rem/src/feature_exp/src/stat_incident.py", line 202, in <module>
    
NameError: name 'perfL' is not defined
>>> --------------------load-df-------------------------------
>>> featL
      index                            session_id  ... v_ram_usage_percent          series
0         0  fb34aceb-8d31-4296-ba7c-f50f705392f6  ...            6.905109  2020-07-07_s_0
1         1  fb34aceb-8d31-4296-ba7c-f50f705392f6  ...            6.897879  2020-07-07_s_0
2         2  fb34aceb-8d31-4296-ba7c-f50f705392f6  ...            6.876045  2020-07-07_s_0
3         3  fb34aceb-8d31-4296-ba7c-f50f705392f6  ...            6.903738  2020-07-07_s_0
4         4  fb34aceb-8d31-4296-ba7c-f50f705392f6  ...            6.898254  2020-07-07_s_0
...     ...                                   ...  ...                 ...             ...
2073    295  2a7c6612-681c-46af-9bc7-3870f99bf2dc  ...            7.187235  2020-07-07_s_6
2074    296  2a7c6612-681c-46af-9bc7-3870f99bf2dc  ...            7.186985  2020-07-07_s_6
2075    297  2a7c6612-681c-46af-9bc7-3870f99bf2dc  ...            7.187253  2020-07-07_s_6
2076    298  2a7c6612-681c-46af-9bc7-3870f99bf2dc  ...            7.188106  2020-07-07_s_6
2077    299  2a7c6612-681c-46af-9bc7-3870f99bf2dc  ...            7.187295  2020-07-07_s_6

[2078 rows x 18 columns]
>>> -------------------------visualize-series-----------------------
>>> xL
['mean_km_per_hour', 'lateral_force_m_per_sec_squared', 'longitudinal_force_m_per_sec_squared', 'v_cpu_usage_percent', 'v_ram_usage_percent']
>>> featL.columns
Index(['index', 'session_id', 'vehicle_id', 'resampled_dt', 'lat', 'lon',
       'actuator_enabled', 'e2e_latency', 'camera_latency', 'joystick_latency',
       'mean_km_per_hour', 'lateral_force_m_per_sec_squared',
       'longitudinal_force_m_per_sec_squared', 'yaw_rate_deg_per_sec',
       'c_ram_usage_percent', 'v_cpu_usage_percent', 'v_ram_usage_percent',
       'series'],
      dtype='object')
>>> 
>>> feat
       timebucket       ts  object_distance  ...  camera_latency  joystick_latency  e2e_latency
0    1.595393e+09 -60456.0        24.128572  ...             170                32          192
1    1.595393e+09 -59456.0        23.933332  ...             172                33          210
2    1.595393e+09 -58456.0        23.715385  ...             173                51          207
3    1.595393e+09 -57456.0        23.344828  ...             162                34          200
4    1.595393e+09 -56456.0        22.892591  ...             159                32          205
..            ...      ...              ...  ...             ...               ...          ...
116  1.595393e+09  55544.0         4.996429  ...             156                32          208
117  1.595393e+09  56544.0         4.700000  ...             166                32          191
118  1.595393e+09  57544.0              NaN  ...             186                51          209
119  1.595393e+09  58544.0              NaN  ...             183                33          213
120  1.595393e+09  59544.0              NaN  ...             261                32          282

[121 rows x 33 columns]
>>> "_".join(f.split("_")).split(".")[0]
''
>>> f
'incindent_1595392700456.csv'
>>> "_".join(f.split("_")[1]).split(".")[0]
'incindent_1595392700456'
>>> "_".join(f.split("_")[1]).split(".")[0]
'1_5_9_5_3_9_2_7_0_0_4_5_6_'
>>> f.split("_")[1].split(".")[0]
'1595392700456.csv'
>>> 